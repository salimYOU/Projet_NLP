{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Tokenization \n",
    "import nltk\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import collections\n",
    "\n",
    "#Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#BERT\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "#Clustering \n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la base intiale \n",
    "f = open('1377884570_tweet_global_warming.txt', 'r',newline='', encoding='ISO-8859-1')\n",
    "content = f.read().split('\\r')\n",
    "\n",
    "content_new=[]\n",
    "for x in content : \n",
    "    if len(x)>0:\n",
    "        content_new.append(x)\n",
    "\n",
    "content_new=content_new[1:len(content_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur\n",
      "erreur\n"
     ]
    }
   ],
   "source": [
    "# Création du dataframe\n",
    "\n",
    "col_tweet=[]\n",
    "col_existence=[]\n",
    "col_score=[]\n",
    "\n",
    "#Split tweet , Note\n",
    "\n",
    "for line in content_new:\n",
    "    if len(line.split('[link]'))==2:\n",
    "        (x,y)=line.split('[link]')\n",
    "        col_tweet.append(x)\n",
    "        col_existence.append(y)\n",
    "    else : \n",
    "        if len(line.split(',Yes,'))==2:\n",
    "            col_tweet.append(line.split(',Yes,')[0])\n",
    "            col_existence.append(',Yes,'+line.split(',Yes,')[1])\n",
    "        elif len(line.split(',No,'))==2:\n",
    "            col_tweet.append(line.split(',No,')[0])\n",
    "            col_existence.append(',No,'+line.split(',No,')[1])\n",
    "        elif len(line.split(',Y,'))==2:\n",
    "            col_tweet.append(line.split(',Y,')[0])\n",
    "            col_existence.append(',Yes,'+line.split(',Y,')[1])\n",
    "        elif len(line.split(',N/A,'))==2:\n",
    "            col_tweet.append(line.split(',N/A,')[0])\n",
    "            col_existence.append(',N/A,'+line.split(',N/A,')[1])\n",
    "        elif len(line.split(',NA,'))==2:\n",
    "            col_tweet.append(line.split(',NA,')[0])\n",
    "            col_existence.append(',NA,'+line.split(',NA,')[1])\n",
    "        elif len(line.split(',N,'))==2:\n",
    "            col_tweet.append(line.split(',N,')[0])\n",
    "            col_existence.append(',No,'+line.split(',N,')[1])\n",
    "        else : \n",
    "            print('erreur')\n",
    "            #print(line.split('[link]'))\n",
    "col_tweet.append('I truly  Fat ASS Gore should get the Scam Artist Award of the decade with his Global Warming and Energy Credits worth close to Billion')\n",
    "col_existence.append(' ,NA')\n",
    "col_tweet.append('Despite Climategate, LEFT investing heavily in global warming hysteria as new way 2 impose nat\\'l & international controls on human freedom.')\n",
    "col_existence.append(' ,NA')\n",
    "        \n",
    "# Split Existence/Note\n",
    "col_existence_new=[]\n",
    "\n",
    "for x in col_existence:\n",
    "    if len(x.split(','))==3:\n",
    "        col_existence_new.append(x.split(',')[1])\n",
    "        col_score.append(x.split(',')[2])\n",
    "    else:\n",
    "        col_existence_new.append('NA')\n",
    "        col_score.append('NA')\n",
    "        \n",
    "#Nettoyage existence\n",
    "for avis in range(len(col_existence_new)):\n",
    "    if col_existence_new[avis]=='NA' or col_existence_new[avis]=='N/A' or col_existence_new[avis]=='':\n",
    "        col_existence_new[avis]=np.nan\n",
    "        \n",
    "#Nettoyage score\n",
    "for score in range(len(col_score)):\n",
    "    if 'NA' not in col_score[score]:\n",
    "        col_score[score]=col_score[score].split('\\t')[0]\n",
    "        if len(col_score[score].split('\"'))>1:\n",
    "            col_score[score]=float(col_score[score].split('\"')[0])\n",
    "        else: \n",
    "            col_score[score]=float(col_score[score])\n",
    "            \n",
    "    else : \n",
    "        col_score[score]=np.nan\n",
    "\n",
    "#Creation du DataFrame\n",
    "dic={'Tweet':col_tweet,'Existence':col_existence_new,'Score':col_score}\n",
    "df=pd.DataFrame(dic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['Tweet'], inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de tweets qui croient au réchauffement climatique : \n",
      " \n",
      "Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. \n",
      " \n",
      "Fighting poverty and global warming in Africa \n",
      " \n",
      "Carbon offsets: How a Vatican forest failed to reduce global warming \n",
      " \n",
      "URUGUAY: Tools Needed for Those Most Vulnerable to Climate Change \n",
      " \n",
      "RT @sejorg: RT @JaymiHeimbuch: Ocean Saltiness Shows Global Warming Is Intensifying Our Water Cycle \n",
      " \n",
      "##################################################\n",
      "##################################################\n",
      " \n",
      "Exemples de tweets qui doutent du réchauffement climatique : \n",
      " \n",
      "Wait here's an idea: it's natural climate change, not human induced global warming. \n",
      " \n",
      "@New_federalists  i have it on good auth tht global warming also causes toe fungus.  We R all fortunate tht thr IS no global warming! #tcot\n",
      " \n",
      "Illegal war and the myth of global warming|My main campaign platform for this election will be the illegal .. \n",
      " \n",
      "the scientific community was scamed by global green  gov warming scam.\n",
      " \n",
      "40 degrees in NYC. please urinate on next liberal global warming /climate change scum you see.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Exemples de tweets\n",
    "\n",
    "print('Exemples de tweets qui croient au réchauffement climatique : ')\n",
    "print(' ')\n",
    "for k in range(5):\n",
    "    print(df[df.Existence=='Yes'].reset_index().iloc[k]['Tweet'])\n",
    "    print(' ')\n",
    "    \n",
    "print('#'*50)\n",
    "print('#'*50)\n",
    "print(' ')\n",
    "print('Exemples de tweets qui doutent du réchauffement climatique : ')\n",
    "print(' ')\n",
    "for k in range(5):\n",
    "    print(df[df.Existence=='No'].reset_index().iloc[k]['Tweet'])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On dispose de 5539 tweets.\n",
      " \n",
      "On a 1683 données manquantes sur le label de l'avis du tweet (Yes, No) .\n",
      " \n",
      "On a 2821 tweets qui croit au Changement climatique.\n",
      "Parmis ces personnes, le score est de 0.82 en moyenne, avec un écart-type de 0.18.\n",
      " \n",
      "On a 1035 tweets qui remettent en doute le Changement climatique.\n",
      "Parmis ces personnes, le score est de 0.76 en moyenne, avec un écart-type de 0.19.\n"
     ]
    }
   ],
   "source": [
    "print('On dispose de {} tweets.'.format(df.shape[0]))\n",
    "\n",
    "print(' ')\n",
    "print('On a {} données manquantes sur le label de l\\'avis du tweet (Yes, No) .'.format(str(df.isnull().sum()['Existence'])))\n",
    "print(' ')\n",
    "\n",
    "#Personnes convaincues du changement climatique \n",
    "print('On a {} tweets qui croit au Changement climatique.'.format(str(df[df.Existence=='Yes'].shape[0])))\n",
    "m=round(df[df.Existence=='Yes'].Score.describe()['mean'],2)\n",
    "std=round(df[df.Existence=='Yes'].Score.describe()['std'],2)\n",
    "print('Parmis ces personnes, le score est de {} en moyenne, avec un écart-type de {}.'.format(str(m),str(std)))\n",
    "print(' ')\n",
    "\n",
    "#Personnes qui doutent du changement climatique \n",
    "print('On a {} tweets qui remettent en doute le Changement climatique.'.format(str(df[df.Existence=='No'].shape[0])))\n",
    "m=round(df[df.Existence=='No'].Score.describe()['mean'],2)\n",
    "std=round(df[df.Existence=='No'].Score.describe()['std'],2)\n",
    "print('Parmis ces personnes, le score est de {} en moyenne, avec un écart-type de {}.'.format(str(m),str(std)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salimyoussfi/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/histograms.py:829: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "/Users/salimyoussfi/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/histograms.py:830: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=plt.hist(list(df.Score))\n",
    "x=plt.title('Histogramme des scores de la base globale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=plt.hist(list(df[df.Existence=='Yes'].Score))\n",
    "x=plt.title('Histogramme des scores pour les tweets qui croient au réchauffement climatique ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAEICAYAAAB8oq9UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de7xcVXnw8d8DAUFuAQmISSBewAK+FTACrdpSsAp4Ad+qhVYFxaIt3lq0orVVW2mxr0q1WiwWykUFI1ahiFVEkdIKGjRSLlIjIAmJEO43QYjP+8daB4Zh5pzJZWbWIb/v5zOfM3vvNXs/e+211zyz9p4zkZlIkiS1bL1xByBJkjQVExZJktQ8ExZJktQ8ExZJktQ8ExZJktQ8ExZJktS8tZKwRMSVEbHP2ljXuioiMiKeMe441kURcX1EvHDccUxXEfGCiLhm3HGsioj4w4j4xrjjmI4iYlZELIqI5wxQdl7t22YMKZYPRcQtEfHzOv2KiFgSEfdExO7D2OaoRcQpEfGh+nyk51pr7+1TJiy9OvOIODwiLp6YzsxdM/PCKdYz1IYrtWQcSVBEfCAiPjvKbQJk5n9m5jNHvd01kZmfy8wXrY11rUsfNiJiA+BU4E8y87IxxzIXOBrYJTOfXGd/BHhLZm6amT8cX3SPFhEXRsQb13Q9wzzXOhOjju1N+d4+So+b5CEiZmTmQ+OOY10xHes7IgKIzPzVuGPRcEXE+pm5ctxxPB50nuuZ+SBw4JhDmrADcGtm3tw178oxxaNhy8xJH8D1wAu75h0OXNyrDLAnsBC4C7gJ+FidfwOQwD318RuUEZ73AT8DbgZOA7boWO/r6rJbgb/s2s4HgLOAz9ZtvbFu+7vAHcBy4JPAhh3rS+BPgJ8AdwN/Azy9vuYuYMFEeWAfYCnw5zW25cDBlJP1f4HbgPd2rHs94BjgpzXeBcBWk9Tru+o6lwFvqLE9oy57AuWTwg21Dj8NbFyXbQ2cW/fxNuA/gfV6rD+A42vsdwKXA8+qyzYGPlrr9k7g4o71v5xywt8BXAjs3HWc313X9QAl4X0K8CVgBXAd8LaO8j3bQo9YJ+r6vcAtdTt/2LF8i9o2VtSY3zexz7UdfLaj7LxalzPq9IXAscB/Ab+YqON+bXyy4whsRGlvt9b6+T6wbY/1nQ78qm7vHkobOhU4ui6fXWP8kzr9jHoso06/FFhUt/HfwK93rLtnfQP7A78EHqzb/FHHuXotpb1f11mvXTFvDJwC3A5cRWmfS7vOnWd0TJ8CfKjz+E3S1hN4M+W8ux341MS+1uVvAK6uy74O7DDJup5f6+QOYAlweEc8JwDnAfcCL5yi3RzOo/uwXwPOr8fhGuDVXfv6KeCrtR4vBZ5el11U9+/eWu+/3yPmwynt7x8p59uPgf06lr++7v/d9Vi9qWPZQOd7LfvxWid3AZcBL+h1vFbhmB1Vj9l1A9RRzz6FR87Hwyj92S3AX3T1ET37bLrO5Y7z+Y31+P6Ccp7dA5xR/04ci59Odr509B1fpJzTdwP/A+wEvIfSby4BXtTVD51U47wR+BCwfmd7ovTbt9dtHVCXHQusBO6vMX5yNdp2z3ON0ne9i9In31vj2xb4Wt2nbwJbdpT/IvDzeowuAnat84+k9B2/rDH+e4++cbX7iKn6tUEfw0hYvgu8tj7fFNh7ksb3BmAx8LRa9t+A0+uyXWrFPR/YsDaEB3l0wvIgJYlYr1bmc4C9KW+k8yidwDu6KvQcYHNgV8qb7gV1+1vUg3BYR8N4CPgrYAPgjyiN/vPAZvX19wNPq+XfAVwCzKEkHP8MnNGnTvenvIE/C9ikrrMzYfmHGudWdVv/DvxdXfZ3lARmg/p4AR2df8c2XkzptGZSkpedge3qsk9RTvzZwPrAb9aYd6I0+t+t6/7zenw27DjOi4C5tb7Xq9v4q3qMnkbpcF88WVvoEetEXX+sxvHbNY5n1uWnAWfXuphHSRiP6GgHUyUsN9TjNQPYYLI2PtlxBN5Uj8UTa709B9h8kPOG0tYnOoE/oCREX+hYdnZ9vgels9yrbuOwuq4nDFDf3XWxCeXNa6Iet6N2UD3iPY7yZrhVPb5XsHYTlnMpbXF7ynm0f112MKWN7VyPz/uA/+6znu0pnfChlPb5JGC3jnjuBJ5X62mjKdrN4dQ+rNbTEkriMKMeg1t4pDM/hfImvWdd/jngzH510yPuwynt+09r3L9fY51IhF9C+eAUlLZ/H7DHqpzvtexrap3MoFwq+TmwUZ83j0GO2fm1PWw8QB3161Pm1XV9pq7n2ZR+d+f6ur59NpMkLP32gUf3o4OcL/dT+soZlPZyHfAXPNLnX9ex7q9Q+oNNgG2A71GTy3qMH6yvWR/4Y8qH0eiOezXb9mQJyyWUJGU2pe/4AbB7rf9vAe/v6oc2q8v+AVjU65zu0zeuSR/Rt1/rVyc962nKAmWl91CyoonHffRPWC4CPghs3bWeeTy28V1A/ZRZp59ZD/oMSiM7o2PZEynZX2fCctEUsb8D+HJXhT6vY/oy4N0d0x8F/qGjYfyCRzLozerr9+p6/cH1+dU8+lPTdhP70iOuk4HjOqZ3mjjYlE7rXuonuLr8N3jkU85fUzrhvh1kLbcvpYPem45PZJST+BfAs3u85i+BBV1lbwT26TjOb+hYvhdwQ9c63gP862Rtocd296F06Jt0zFtQ41mf0sHt0rHsTcCFHe1gqoTlrwdo4xPtqu9xpJzsA30y4LEJy9Mp5856lDegN1FPdsroy5/V5ycAf9O1rmsob2RT1Xd3XWxSt/l71BG0SeK9lppE1OkjWbsJy/O7ju0x9fnXqElER5u7jx6jLHVfv9xnG6cAp3VMT9VuDueRhOX3gf/sWt8/Uzv6uu5/6Vh2IPDjfnXTI7bD6XjzqvO+R03me5T/CvD2+nyg873Pem6nnuesXsKyb8d03zpi8j5lXl3XnK59P6TPdh/us1nzhGWQ8+X8jmUvo7zXdff5MykJwQN0nEeU5OLbHcd4cceyJ9bXPrk77j77PVXbnixh6RyN/hJwQsf0W4Gv9FnvzBrjFr3aSMf6J/rGNekj+vZrq9KmB/2W0MGZOXPiQbms0s8RlDfgH0fE9yPipZOUfQplCHHCzyhvDNvWZUsmFmTmfZSh+E5LOiciYqeIODcifh4RdwF/SxlS7XRTx/Nf9JjetGP61nzkOvgv+rx+ovwOwJcj4o6IuIPyxrey7ku3R+0bj66DWZTGflnHuv6jzgf4f5RPpN+IiGsj4pge6yczv0UZXv0UcFNEnBgRm1PqYyPKJ/xecf2sYx2/qnHO7ijTGfcOwFMm4qyxvrdjn1elLdyemfd2TP+sxrM15dNRdzvpjGkqS6Yu8rDJjuPplEsWZ0bEsoj4+3oT4pQy86eUznA3yqfkc4FlEfFMSjLynY7tH91Vp3MpdTFVfXdv817KG82bgeUR8dWI+LU+IU7WJteGn3c8v49Hnzcf79if2yhJe6/jO5fe7XZCZ/yr0m52APbqqtc/BJ7cUaZf/IO6MWsv3RHLUwAi4oCIuCQibqvbPpBH+q2Bzve6nqMj4uqIuLOuZwse2/+tiu5zvV8dTdanTOhZfwP22atrkPOluz+/pUefv2ld1waU82hiXf9MGWmZ8PA+1veridcOYqq2PZmB3tMiYv2IOC4iflrr+vpaZtD6XpM+YrJ+bWBr/f+wZOZPMvNQyoH8MHBWRGxCyb66LaPsyITtKZ+0b6JcJ5wzsSAiNqYMkz1qc13TJ1CuD++YmZtTGmes/t6skiWUa5YzOx4bZeaNPcoupxysCdt3PL+F0sh27VjPFpm5KUBm3p2ZR2fm0yifCP4sIvbrFVBmfiIzn0O5HLIT5ZrjLZRh0Kf3eMmjjke9SXUuZZTl4dV27fN1Xfu8WWYeWLffry30smXXsu1rPLdQRji628lETPdSErwJnW8yvWKeSt/jmJkPZuYHM3MXypD3Syn3WfXSa5vfAV5JucR2Y51+HbAl5VLbxPaP7dr+EzPzDKao717bzMyvZ+bvUkaKfkwZmu9lsjYJ5U1mqnpeHUsow+qd+7RxZv53n7K92u2Ezv2fqt10r/c7XTFsmpl/vIr7MpnZ9XzqjGVZRDyB8qn4I5T7oWZS7sMJGPx8j4gXUO4vezXlnoWZlMtOE9sc5Dzp1n2u96ujyfqUqUzWZ098gFnddjfV+bIqllBGWLbuWNfmmbnrgK+fqg+aqm2vDX8AHMQj93fNq/Mn6nuqGNekj5isXxvYWk9YIuI1ETGrfjq/o85eSblu/SvKdcQJZwB/GhFPjYhNKdn1F7LckX4W8LKI+M2I2JByaWGq5GMzyjX7e+onybXZ4Uzl08CxEbEDPPy/Cg7qU3YBcHhE7BIRT6QMqwIPj2p8Bjg+Irap65odES+uz18aEc+ond9dlLp9zLchIuK5EbFXHQG4l9KhrKzrPxn4WEQ8pWbdv1E7zgXASyJiv/q6oyknaa83DyhDu3dFxLsjYuO6rmdFxHNrDP3aQj8fjIgNa+f7UuCL9dPOglq3m9X6/TPKjXJQ3uh/KyK2j4gtKEOra6LvcYyI34mI/xMR61Pq/sFJ9ucmHt3WoSQob6FcKoMyTPxWyqWJifV8BnhzPXYREZtExEsiYjOmqO+6zXkRsV6Nd9uIeHlNBB+gjPD0i3cB8J6I2DIi5tS4Oi0C/qBuc3/KqNDa8Om63V1rzFtExKv6lP0c8MKIeHVEzIiIJ0XEbr0KDtBuOp0L7BQRr42IDerjuRGx84D70OtYd9sGeFtd96so9+ycRxkFegKlf3woIg4AHv669aDnO6Xve6iuZ0ZE/BXlXr0Ji4ADI2KriHgy5dLLquhbR1P0KVPp22dn5gpKgvmaus43sGpv6lOdLwPLzOXAN4CPRsTmEbFeRDw9IgY9D6ZqIwO37TWwGaUfuJWSWPztKsa4Jn3EZP3awIbxn273B66MiHsod60fkpn31yGyY4H/ijIktDelkZ9O6cCvo7ypvhUgM6+sz8+kZHZ3U27aeWCSbb+TkkXeTamgL6z93evr45QbZb8REXdTboTaq1fBzPwa5Yanb1GGe7/VVeTddf4lUYbuvkm5vwdgxzp9D+Wm1n/K3t+T35xSB7fzyDetPlKXvZNyR/z3KUPwH6bc53IN5ca9f6R8anoZ8LLM/GWf/VhZy+xGOX63AP9Cyd6hT1votS7KcOrtlFGVzwFvzswf12VvpSRd11LuxP88pe2QmedTjvPllHuKzu2z/kFNdhyfTEmk76JcKvoOvd8Aodws+b7a1t9Z532H0mlMJCwXUzqOiWkycyHlxr1PUupjMeX6+CD1/cX699aI+AHl/D6aUqe3UTqQfpdzP0hpJ9dROubTu5a/vW574lLAV/qsZ5Vk5pcp7e/M2tavAA7oU/YGyuWSoyn7s4hyE2c/fdtN13rvpiQJh1Dq6uc1pkHecKHcC3FqPdav7lPmUsq5ewulH3xlZt5at/02ypvB7ZT+65yO1w16vn+dcj/Q/1KO4/08evj+dOBHlMsA32AV+8YB6qhnnzLAqqfqs/+IMjJ8K2WkuN+Hp14xT3W+rKrXURLMqyjH6izKyOUgPg68MiJuj4hP9Ih1Vdv26jiN0jZupOzDJV3LTwJ2qe241/m92n3EZP3aqpi4g7l5UUZg7qAMHV437ni09kT5T4qfzcw5U5XVaHhM1p6IOJxyw+Xzxx2LtLaMo49o+reEIuJlEfHEOqT9EUoGf/14o5IkSaPWdMJCuUFoWX3sSLmkMD2GhCRJ0lozbS4JSZKkdVfrIyySJEmPnx8/XBdtvfXWOW/evHGHIUnTymWXXXZLZs6auqRaYsIyjc2bN4+FCxeOOwxJmlYiYm3/J2eNgJeEJElS80xYJElS80xYJElS80xYJElS80xYJElS80xYJElS80xYJElS80xYJElS80xYhiQiNoqI70XEjyLiyoj4YJ1/SkRcFxGL6mO3Oj8i4hMRsTgiLo+IPca7B5IktcP/dDs8DwD7ZuY9EbEBcHFEfK0ue1dmntVV/gDKL1LvCOwFnFD/SlKT5h3z1bFs9/rjXjKW7Wq8HGEZkizuqZMb1MdkP419EHBafd0lwMyI2G7YcUqSNB2YsAxRRKwfEYuAm4HzM/PSuujYetnn+Ih4Qp03G1jS8fKldV73Oo+MiIURsXDFihVDjV+SpFaYsAxRZq7MzN2AOcCeEfEs4D3ArwHPBbYC3l2LR69V9FjniZk5PzPnz5rlj41KktYNJiwjkJl3ABcC+2fm8nrZ5wHgX4E9a7GlwNyOl80Blo00UEmSGmXCMiQRMSsiZtbnGwMvBH48cV9KRARwMHBFfck5wOvqt4X2Bu7MzOVjCF2SpOb4LaHh2Q44NSLWpySGCzLz3Ij4VkTMolwCWgS8uZY/DzgQWAzcB7x+DDFLktQkE5YhyczLgd17zN+3T/kEjhp2XJIkTUdeEpIkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYRmSiNgoIr4XET+KiCsj4oN1/lMj4tKI+ElEfCEiNqzzn1CnF9fl88YZvyRJLTFhGZ4HgH0z89nAbsD+EbE38GHg+MzcEbgdOKKWPwK4PTOfARxfy0mSJExYhiaLe+rkBvWRwL7AWXX+qcDB9flBdZq6fL+IiBGFK0lS00xYhigi1o+IRcDNwPnAT4E7MvOhWmQpMLs+nw0sAajL7wSe1GOdR0bEwohYuGLFimHvgiRJTTBhGaLMXJmZuwFzgD2BnXsVq397jabkY2ZknpiZ8zNz/qxZs9ZesJIkNcyEZQQy8w7gQmBvYGZEzKiL5gDL6vOlwFyAunwL4LbRRipJUptMWIYkImZFxMz6fGPghcDVwLeBV9ZihwFn1+fn1Gnq8m9l5mNGWCRJWhfNmLqIVtN2wKkRsT4lMVyQmedGxFXAmRHxIeCHwEm1/EnA6RGxmDKycsg4gpYkqUUmLEOSmZcDu/eYfy3lfpbu+fcDrxpBaJIkTTteEpIkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc0zYZEkSc2bMe4AJA3PvGO+OpbtXn/cS8ayXUmPX46wSJKk5pmwDEFEzI2Ib0fE1RFxZUS8vc7/QETcGBGL6uPAjte8JyIWR8Q1EfHi8UUvSVJ7vCQ0HA8BR2fmDyJiM+CyiDi/Ljs+Mz/SWTgidgEOAXYFngJ8MyJ2ysyVI41akqRGOcIyBJm5PDN/UJ/fDVwNzJ7kJQcBZ2bmA5l5HbAY2HP4kUqSND2YsAxZRMwDdgcurbPeEhGXR8TJEbFlnTcbWNLxsqVMnuBIkrROMWEZoojYFPgS8I7MvAs4AXg6sBuwHPjoRNEeL88+6zwyIhZGxMIVK1YMIWpJktpjwjIkEbEBJVn5XGb+G0Bm3pSZKzPzV8BneOSyz1JgbsfL5wDLeq03M0/MzPmZOX/WrFnD2wFJkhriTbdDEBEBnARcnZkf65i/XWYur5OvAK6oz88BPh8RH6PcdLsj8L0RhiytVeP6/y/g/4CRHq9MWIbjecBrgf+JiEV13nuBQyNiN8rlnuuBNwFk5pURsQC4ivINo6P8hpAkSY8wYRmCzLyY3velnDfJa44Fjh1aUJIkTWPewyJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwiJJkppnwjIkETE3Ir4dEVdHxJUR8fY6f6uIOD8iflL/blnnR0R8IiIWR8TlEbHHePdAkqR2mLAMz0PA0Zm5M7A3cFRE7AIcA1yQmTsCF9RpgAOAHevjSOCE0YcsSVKbTFiGJDOXZ+YP6vO7gauB2cBBwKm12KnAwfX5QcBpWVwCzIyI7UYctiRJTTJhGYGImAfsDlwKbJuZy6EkNcA2tdhsYEnHy5bWed3rOjIiFkbEwhUrVgwzbEmSmmHCMmQRsSnwJeAdmXnXZEV7zMvHzMg8MTPnZ+b8WbNmra0wJUlqmgnLEEXEBpRk5XOZ+W919k0Tl3rq35vr/KXA3I6XzwGWjSpWSZJaZsIyJBERwEnA1Zn5sY5F5wCH1eeHAWd3zH9d/bbQ3sCdE5eOJEla180YdwCPY88DXgv8T0QsqvPeCxwHLIiII4AbgFfVZecBBwKLgfuA1482XEmS2mXCMiSZeTG970sB2K9H+QSOGmpQkiRNU14SkiRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhkSRJzTNhGZKIODkibo6IKzrmfSAiboyIRfVxYMey90TE4oi4JiJePJ6oJUlqkwnL8JwC7N9j/vGZuVt9nAcQEbsAhwC71tf8U0SsP7JIJUlqnAnLkGTmRcBtAxY/CDgzMx/IzOuAxcCeQwtOkqRpxoRl9N4SEZfXS0Zb1nmzgSUdZZbWeY8REUdGxMKIWLhixYphxypJUhNMWEbrBODpwG7AcuCjdX70KJu9VpCZJ2bm/MycP2vWrOFEKUlSY0xYRigzb8rMlZn5K+AzPHLZZykwt6PoHGDZqOOTJKlVJiwjFBHbdUy+Apj4BtE5wCER8YSIeCqwI/C9UccnSVKrZow7gMeriDgD2AfYOiKWAu8H9omI3SiXe64H3gSQmVdGxALgKuAh4KjMXDmOuCVJapEJy5Bk5qE9Zp80SfljgWOHF5EkSdOXl4QkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFiGJCJOjoibI+KKjnlbRcT5EfGT+nfLOj8i4hMRsTgiLo+IPcYXuSRJ7TFhGZ5TgP275h0DXJCZOwIX1GmAA4Ad6+NI4IQRxShJ0rRgwjIkmXkRcFvX7IOAU+vzU4GDO+aflsUlwMyI2G40kUqS1D4TltHaNjOXA9S/29T5s4ElHeWW1nmPERFHRsTCiFi4YsWKoQYrSVIrTFjaED3mZa+CmXliZs7PzPmzZs0acliSJLXBhGW0bpq41FP/3lznLwXmdpSbAywbcWySJDXLhGW0zgEOq88PA87umP+6+m2hvYE7Jy4dSZIkmDHuAB6vIuIMYB9g64hYCrwfOA5YEBFHADcAr6rFzwMOBBYD9wGvH3nAkiQ1zIRlSDLz0D6L9utRNoGjhhuRJEnTl5eEJElS80xYJElS80xYJElS80xYJElS87zpVuuMecd8dSzbvf64l4xlu5L0eOIIiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJap4JiyRJat6McQewLoqI64G7gZXAQ5k5PyK2Ar4AzAOuB16dmbePK0ZJklriCMv4/E5m7paZ8+v0McAFmbkjcEGdliRJmLC05CDg1Pr8VODgMcYiSVJTTFjGI4FvRMRlEXFknbdtZi4HqH+3GVt0kiQ1xntYxuN5mbksIrYBzo+IHw/6wprgHAmw/fbbDys+SZKa4gjLGGTmsvr3ZuDLwJ7ATRGxHUD9e3Of156YmfMzc/6sWbNGFbIkSWNlwjJiEbFJRGw28Rx4EXAFcA5wWC12GHD2eCKUJKk9XhIavW2BL0cElPr/fGb+R0R8H1gQEUcANwCvGmOMkiQ1xYRlxDLzWuDZPebfCuw3+ogkSWqfl4QkSVLzTFgkSVLzTFgkSVLzTFgkSVLzTFgkSVLz/JbQOmreMV8dy3avP+4lY9muJGl6c4RFkiQ1z4RFkiQ1z4RFkiQ1z4RFkiQ1z4RFkiQ1z4RFkiQ1z681a6TG9XVqSdL05giLJElqngmLJElqngmLJElqngmLJElqnjfdSkPmjcaj5e9kSY9PJiyStBaYKEnDZcIiSdOYI3haV3gPiyRJap4JS0MiYv+IuCYiFkfEMeOOR5KkVpiwNCIi1gc+BRwA7AIcGhG7jDcqSZLaYMLSjj2BxZl5bWb+EjgTOGjMMUmS1ARvum3HbGBJx/RSYK/uQhFxJHBknbwnIq4ZQWyra2vglnEHsZqma+zTNW6YvrFP17hhmsYeH17juHdYW7FodExY2hE95uVjZmSeCJw4/HDWXEQszMz5445jdUzX2Kdr3DB9Y5+uccP0jX26xq014yWhdiwF5nZMzwGWjSkWSZKaYsLSju8DO0bEUyNiQ+AQ4JwxxyRJUhO8JNSIzHwoIt4CfB1YHzg5M68cc1hralpcuupjusY+XeOG6Rv7dI0bpm/s0zVurYHIfMxtEpIkSU3xkpAkSWqeCYskSWqeCYvW2KA/KRARr4yIjIhmvo44VewRcXhErIiIRfXxxnHE2W2QOo+IV0fEVRFxZUR8ftQx9jNAnR/fUd//GxF3jCPObgPEvX1EfDsifhgRl0fEgeOIs5cBYt8hIi6ocV8YEXPGEWe3iDg5Im6OiCv6LI+I+ETdr8sjYo9Rx6gRykwfPlb7QblB+KfA04ANgR8Bu/QotxlwEXAJMH/ccQ8aO3A48Mlxx7oace8I/BDYsk5vM+64V6W9dJR/K+UG9ObjptwI+sf1+S7A9eOOexVi/yJwWH2+L3D6uOOusfwWsAdwRZ/lBwJfo/wfq72BS8cdszz74RwAAALiSURBVI/hPRxh0Zoa9CcF/gb4e+D+UQY3hen6cwiDxP1HwKcy83aAzLx5xDH2s6p1fihwxkgim9wgcSeweX2+Be38H6VBYt8FuKA+/3aP5WORmRcBt01S5CDgtCwuAWZGxHajiU6jZsKiNdXrJwVmdxaIiN2BuZl57igDG8CUsVe/V4ebz4qIuT2Wj9ogce8E7BQR/xURl0TE/iOLbnKD1jkRsQPwVOBbI4hrKoPE/QHgNRGxFDiPMjrUgkFi/xHwe/X5K4DNIuJJI4htTQ3cnjT9mbBoTU36kwIRsR5wPHD0yCIa3CA/h/DvwLzM/HXgm8CpQ49qaoPEPYNyWWgfyijFv0TEzCHHNYiBfoKiOgQ4KzNXDjGeQQ0S96HAKZk5h3Kp4vTa/sdtkNjfCfx2RPwQ+G3gRuChYQe2FqxKe9I018LJpOltqp8U2Ax4FnBhRFxPuc58TiM33k75cwiZeWtmPlAnPwM8Z0SxTWaQn3FYCpydmQ9m5nXANZQEZtxW5ScoDqGNy0EwWNxHAAsAMvO7wEaUHxcct0Ha+bLM/L+ZuTvwF3XenaMLcbX5kybrEBMWralJf1IgM+/MzK0zc15mzqPcdPvyzFw4nnAfZcqfQ+i6Hv5y4OoRxtfPID/j8BXgdwAiYmvKJaJrRxplbwP9BEVEPBPYEvjuiOPrZ5C4bwD2A4iInSkJy4qRRtnbIO18647RoPcAJ484xtV1DvC6+m2hvYE7M3P5uIPScPiv+bVGss9PCkTEXwMLM7PZ30MaMPa3RcTLKcPjt1G+NTRWA8b9deBFEXEVsBJ4V2beOr6oi1VoL4cCZ2ZmE8P7A8Z9NPCZiPhTymWJw1uIf8DY9wH+LiKS8m2+o8YWcIeIOIMS29b13qD3AxsAZOanKfcKHQgsBu4DXj+eSDUK/mt+SZLUPC8JSZKk5pmwSJKk5pmwSJKk5pmwSJKk5pmwSJKk5pmwSJKk5pmwSJKk5v1/c5F2k8FH/jgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=plt.hist(list(df[df.Existence=='No'].Score))\n",
    "x=plt.title('Histogramme des scores pour les tweets qui ne croient pas au réchauffement climatique ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les scores sont des indicateurs sur la precision de la classification du tweet. Globalement, on a la même certitude sur cette précision pour les deux types de tweets (Yes et No)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En utilisant le séparateur ' ', on obtient 19602 mots dans l'ensemble des tweets\n"
     ]
    }
   ],
   "source": [
    "#Nombre de mots différents dans l'ensemble des articles \n",
    "\n",
    "##Récupération de tout les mots de tout les tweets\n",
    "arr=df.Tweet.apply(lambda x : x.split(' ')).array\n",
    "arr = reduce(add, arr)\n",
    "\n",
    "\n",
    "##Nombre de mots différents dans l'ensemble des articles \n",
    "print('En utilisant le séparateur \\' \\', on obtient {} mots dans l\\'ensemble des tweets'.format(len(set(arr))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En utilisant le Tokenizer TweetTokenizer, on obtient un vocabulaire de 15804 mots.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization en utilisant le Tokenizer Tweeter\n",
    "\n",
    "arr_tokens = df.Tweet.apply(lambda x: TweetTokenizer().tokenize(x)).array\n",
    "arr_tokens = reduce(add, arr_tokens)\n",
    "print('En utilisant le Tokenizer TweetTokenizer, on obtient un vocabulaire de {} mots.'.format(len(set(arr_tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On prend toutes les phrases de touts les texts, et on les concatène dans une liste, en les traitant auparavant\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def remove_hashtags(tokens):\n",
    "    tokens= map(lambda x : x.replace('#',''),tokens) #map : parcours tout les tokens\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_url(tokens): #pb pour https\n",
    "    tokens= filter(lambda x: \"http\" not in x, tokens) #filter : garde là où il y a True\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_html(tokens):\n",
    "    tokens= filter(lambda x: x[0]+x[-1]!='<>',tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_www(tokens):\n",
    "    tokens= filter(lambda x: \"www\" not in x, tokens) #filter : garde là où il y a True\n",
    "    return list(tokens)\n",
    "\n",
    "'''\n",
    "def remove_x95(clean_corpus):\n",
    "    for sentence_r in range(len(clean_corpus)):\n",
    "        sentence=clean_corpus[sentence_r]\n",
    "        for x in range(len(sentence)):\n",
    "            if '\\x95' in sentence[x]:\n",
    "                y=sentence[x].split('\\x95')\n",
    "                new_x=''\n",
    "                for part_x in y:\n",
    "                    new_x=new_x+part_x\n",
    "                sentence[x]=new_x\n",
    "        clean_corpus[sentence_r]=sentence\n",
    "    return(clean_corpus)\n",
    "'''\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def clean_ponctuation(text_tokens): # Nettoyage de la ponctuation\n",
    "\n",
    "    list_word_clean_ponctuation=[]\n",
    "    for tweet in text_tokens:\n",
    "        list_tweet=[]\n",
    "        for word in tweet:\n",
    "            if len(word)<2:\n",
    "                if (word=='a') or (word=='i') or (word=='u'):\n",
    "                    list_tweet.append(word)\n",
    "                if RepresentsInt(word):\n",
    "                    list_tweet.append(word)\n",
    "            else :\n",
    "                if (word!='..') & (word!='...') & (word!='rt'):\n",
    "                    list_tweet.append(word)\n",
    "        list_word_clean_ponctuation.append(list_tweet)\n",
    "    \n",
    "    return(list_word_clean_ponctuation)\n",
    "\n",
    "def remove_arobase(text_tokens):\n",
    "    \n",
    "    list_new_tokens=[]\n",
    "    for tweet in text_tokens:\n",
    "        new_tweet=[]\n",
    "        for word in tweet: \n",
    "            if '@' not in word:\n",
    "                new_tweet.append(word)\n",
    "        list_new_tokens.append(new_tweet)\n",
    "    \n",
    "    return(list_new_tokens)\n",
    "\n",
    "###############################################################\n",
    "###############################################################\n",
    "###############################################################\n",
    "\n",
    "def clean_text_first(corpus):\n",
    "    \n",
    "    tok=TweetTokenizer()\n",
    "    tokens=[]\n",
    "    for sample in corpus:\n",
    "        token=tok.tokenize(sample) \n",
    "        token=remove_url(token)\n",
    "        token=remove_html(token)\n",
    "        token=remove_hashtags(token)\n",
    "        token=remove_www(token)\n",
    "        token=list(map(lambda x : x.lower(),token)) #.lower() : met les majuscules en minuscules\n",
    "        tokens.append(token) #ajout du token à l'ensemble des phrases\n",
    "    \n",
    "    #Nettoyage de la ponctuation\n",
    "    tokens=clean_ponctuation(tokens)\n",
    "    \n",
    "    #Nettoyage des arobase : pour la plupart, se sont des noms propres\n",
    "    tokens=remove_arobase(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def clean_text_second(corpus,threshold): #On rajoute l'association des mots qui vont ensembles\n",
    "    \n",
    "    #clean les textes\n",
    "    tokens=clean_text_first(corpus)\n",
    "    \n",
    "    #associer les mots\n",
    "    phrases=Phrases(tokens,threshold=threshold) #On fait apprendre le modèle d'association sur tout les mots\n",
    "    phraser=Phraser(phrases) #Outil pour associer\n",
    "    \n",
    "    clean_tokens=[]\n",
    "    for token in tokens: #On parcours les phrases et on associe les mots\n",
    "        new_tokens=phraser[token]\n",
    "        clean_tokens.append(new_tokens)\n",
    "        \n",
    "    #tokens = remove_x95(tokens)\n",
    "    \n",
    "    return(clean_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clean_text_first(df.Tweet) #sans association de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words=clean_text_second(df.Tweet,threshold=1000) #avec association de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 10 mots qui apparaissent le plus sont (par ordre décroissant) :\n",
      " \n",
      "climate (3348) \n",
      "change (3027) \n",
      "global (2869) \n",
      "warming (2765) \n",
      "the (2275) \n",
      "to (1674) \n",
      "of (1440) \n",
      "on (1063) \n",
      "a (1056) \n",
      "in (998) \n"
     ]
    }
   ],
   "source": [
    "#Mots les plus fréquents après nettoyage des tokens\n",
    "\n",
    "counter=collections.Counter(reduce(add, list_words))\n",
    "\n",
    "#10 mots les plus fréquents \n",
    "number_word=10\n",
    "print('Les {} mots qui apparaissent le plus sont (par ordre décroissant) :'.format(number_word))\n",
    "print(' ')\n",
    "for word in counter.most_common(number_word):\n",
    "    print(word[0]+' ('+str(word[1])+') ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En moyenne, on a retenu 14.0 mots par tweet \n"
     ]
    }
   ],
   "source": [
    "# Taille moyenne des tweet : \n",
    "list_words\n",
    "mean=0\n",
    "for tweet in list_words:\n",
    "    mean=mean+len(tweet)\n",
    "    \n",
    "print('En moyenne, on a retenu {} mots par tweet '.format(str(round(mean/len(list_words),0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Représentation des Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise tout d'abord l'algorithme Word2Vec pour représenter ces tweets. Chaque mot à une représentation vectorielle. Pour chaque tweet, on fait la moyenne des vecteurs (chaque mot) inclut dans ce tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement du modèle Word2Vec ...\n"
     ]
    }
   ],
   "source": [
    "#Cleaning des tweets\n",
    "clean_text=clean_text_second(df.Tweet,threshold=1000)\n",
    "\n",
    "print(\"Entrainement du modèle Word2Vec ...\")\n",
    "model = Word2Vec(clean_text, size=100, window=5, min_count=3, workers=4) \n",
    "\n",
    "model.train(clean_text, total_examples=len(clean_text), epochs=10) #réseau de neuronne du Word2Vec\n",
    "model_wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salimyoussfi/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/salimyoussfi/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Représentation des tweets en moyennant les mots\n",
    "\n",
    "def tokens2vectors(tokenCorpus):\n",
    "    ''' transforms our X into a list of list of vec (2D array) '''\n",
    "    new_sample = list()\n",
    "    i=0\n",
    "    for sample in tokenCorpus:\n",
    "        tweetVecs = list()\n",
    "        for token in sample:\n",
    "            try : \n",
    "                tweetVecs.append(model_wv.get_vector(token)  )\n",
    "            except: \n",
    "                i=i+1\n",
    "                tweetVecs.append( np.zeros(100) ) \n",
    "        new_sample.append(np.mean(tweetVecs, axis=0))\n",
    "    \n",
    "    return np.array(new_sample)\n",
    "\n",
    "\n",
    "X= tokens2vectors(clean_text)\n",
    "\n",
    "Y=[]\n",
    "for x in list(X):\n",
    "    try: Y.append(list(x))\n",
    "    except : pass\n",
    "    \n",
    "df_representation_W2v= pd.DataFrame(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5527</th>\n",
       "      <th>5528</th>\n",
       "      <th>5529</th>\n",
       "      <th>5530</th>\n",
       "      <th>5531</th>\n",
       "      <th>5532</th>\n",
       "      <th>5533</th>\n",
       "      <th>5534</th>\n",
       "      <th>5535</th>\n",
       "      <th>5536</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.193357</td>\n",
       "      <td>-0.149447</td>\n",
       "      <td>-0.144458</td>\n",
       "      <td>-0.117858</td>\n",
       "      <td>-0.108957</td>\n",
       "      <td>-0.024010</td>\n",
       "      <td>-0.288531</td>\n",
       "      <td>-0.115899</td>\n",
       "      <td>-0.141132</td>\n",
       "      <td>0.147953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126116</td>\n",
       "      <td>-0.096101</td>\n",
       "      <td>-0.088420</td>\n",
       "      <td>-0.044344</td>\n",
       "      <td>-0.110260</td>\n",
       "      <td>-0.211010</td>\n",
       "      <td>0.057965</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>-0.094698</td>\n",
       "      <td>-0.011287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.059302</td>\n",
       "      <td>-0.087868</td>\n",
       "      <td>-0.123348</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>-0.120210</td>\n",
       "      <td>-0.072669</td>\n",
       "      <td>-0.039426</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>-0.068202</td>\n",
       "      <td>-0.089802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093196</td>\n",
       "      <td>-0.041646</td>\n",
       "      <td>0.037461</td>\n",
       "      <td>-0.010557</td>\n",
       "      <td>-0.079030</td>\n",
       "      <td>-0.176610</td>\n",
       "      <td>-0.107298</td>\n",
       "      <td>-0.020363</td>\n",
       "      <td>-0.083380</td>\n",
       "      <td>-0.036577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.117837</td>\n",
       "      <td>0.104812</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>0.126964</td>\n",
       "      <td>0.039949</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.251654</td>\n",
       "      <td>0.111417</td>\n",
       "      <td>0.101397</td>\n",
       "      <td>-0.133405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104722</td>\n",
       "      <td>0.033976</td>\n",
       "      <td>0.084681</td>\n",
       "      <td>-0.009239</td>\n",
       "      <td>0.094522</td>\n",
       "      <td>0.160429</td>\n",
       "      <td>-0.034903</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>0.046650</td>\n",
       "      <td>0.004937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.185309</td>\n",
       "      <td>-0.224516</td>\n",
       "      <td>-0.179630</td>\n",
       "      <td>-0.258568</td>\n",
       "      <td>-0.236283</td>\n",
       "      <td>-0.284707</td>\n",
       "      <td>-0.204144</td>\n",
       "      <td>-0.203223</td>\n",
       "      <td>-0.212819</td>\n",
       "      <td>-0.365461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254983</td>\n",
       "      <td>-0.198700</td>\n",
       "      <td>-0.277118</td>\n",
       "      <td>-0.237472</td>\n",
       "      <td>-0.209459</td>\n",
       "      <td>-0.129051</td>\n",
       "      <td>-0.270134</td>\n",
       "      <td>-0.215277</td>\n",
       "      <td>-0.174167</td>\n",
       "      <td>-0.229636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.018992</td>\n",
       "      <td>-0.053855</td>\n",
       "      <td>-0.223139</td>\n",
       "      <td>-0.177284</td>\n",
       "      <td>-0.107277</td>\n",
       "      <td>-0.172527</td>\n",
       "      <td>-0.110242</td>\n",
       "      <td>-0.119537</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.412183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135456</td>\n",
       "      <td>-0.066704</td>\n",
       "      <td>-0.007727</td>\n",
       "      <td>-0.089324</td>\n",
       "      <td>-0.093500</td>\n",
       "      <td>-0.120392</td>\n",
       "      <td>-0.259552</td>\n",
       "      <td>-0.104806</td>\n",
       "      <td>-0.120526</td>\n",
       "      <td>-0.148472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>-0.523935</td>\n",
       "      <td>-0.711340</td>\n",
       "      <td>-0.517215</td>\n",
       "      <td>-0.365655</td>\n",
       "      <td>-0.672212</td>\n",
       "      <td>-0.634504</td>\n",
       "      <td>-0.224574</td>\n",
       "      <td>-0.303463</td>\n",
       "      <td>-0.510182</td>\n",
       "      <td>-0.674103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377451</td>\n",
       "      <td>-0.533407</td>\n",
       "      <td>-0.345817</td>\n",
       "      <td>-0.539590</td>\n",
       "      <td>-0.552869</td>\n",
       "      <td>-0.688724</td>\n",
       "      <td>-0.561133</td>\n",
       "      <td>-0.464320</td>\n",
       "      <td>-0.554635</td>\n",
       "      <td>-0.540232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>-0.137068</td>\n",
       "      <td>-0.284812</td>\n",
       "      <td>-0.306518</td>\n",
       "      <td>-0.062223</td>\n",
       "      <td>-0.371546</td>\n",
       "      <td>-0.358298</td>\n",
       "      <td>0.092643</td>\n",
       "      <td>-0.039871</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>-0.552148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>-0.202753</td>\n",
       "      <td>-0.058736</td>\n",
       "      <td>-0.236887</td>\n",
       "      <td>-0.205654</td>\n",
       "      <td>-0.364200</td>\n",
       "      <td>-0.321659</td>\n",
       "      <td>-0.208627</td>\n",
       "      <td>-0.232714</td>\n",
       "      <td>-0.212720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.018346</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>-0.061472</td>\n",
       "      <td>0.074547</td>\n",
       "      <td>0.062571</td>\n",
       "      <td>-0.145427</td>\n",
       "      <td>-0.052120</td>\n",
       "      <td>0.009887</td>\n",
       "      <td>0.128994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165326</td>\n",
       "      <td>0.035119</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.052710</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>0.038392</td>\n",
       "      <td>0.064423</td>\n",
       "      <td>0.055372</td>\n",
       "      <td>0.063736</td>\n",
       "      <td>0.019149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>-0.073689</td>\n",
       "      <td>-0.050296</td>\n",
       "      <td>-0.019038</td>\n",
       "      <td>-0.025331</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.049066</td>\n",
       "      <td>-0.188642</td>\n",
       "      <td>-0.019759</td>\n",
       "      <td>-0.033656</td>\n",
       "      <td>0.158397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058312</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>-0.051219</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>-0.060379</td>\n",
       "      <td>-0.043594</td>\n",
       "      <td>0.069309</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>-0.016317</td>\n",
       "      <td>0.063543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>-0.013094</td>\n",
       "      <td>-0.049977</td>\n",
       "      <td>-0.229780</td>\n",
       "      <td>-0.124082</td>\n",
       "      <td>-0.059042</td>\n",
       "      <td>-0.050357</td>\n",
       "      <td>-0.165812</td>\n",
       "      <td>-0.072948</td>\n",
       "      <td>-0.082494</td>\n",
       "      <td>-0.149557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064496</td>\n",
       "      <td>-0.044798</td>\n",
       "      <td>-0.028459</td>\n",
       "      <td>-0.010321</td>\n",
       "      <td>-0.036408</td>\n",
       "      <td>-0.095491</td>\n",
       "      <td>-0.047167</td>\n",
       "      <td>-0.018514</td>\n",
       "      <td>-0.069886</td>\n",
       "      <td>-0.049317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5537 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "0  -0.193357 -0.149447 -0.144458 -0.117858 -0.108957 -0.024010 -0.288531   \n",
       "1  -0.059302 -0.087868 -0.123348  0.020774 -0.120210 -0.072669 -0.039426   \n",
       "2   0.117837  0.104812  0.116388  0.126964  0.039949  0.012358  0.251654   \n",
       "3  -0.185309 -0.224516 -0.179630 -0.258568 -0.236283 -0.284707 -0.204144   \n",
       "4   0.018992 -0.053855 -0.223139 -0.177284 -0.107277 -0.172527 -0.110242   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.523935 -0.711340 -0.517215 -0.365655 -0.672212 -0.634504 -0.224574   \n",
       "96 -0.137068 -0.284812 -0.306518 -0.062223 -0.371546 -0.358298  0.092643   \n",
       "97  0.003860  0.018346 -0.000638 -0.061472  0.074547  0.062571 -0.145427   \n",
       "98 -0.073689 -0.050296 -0.019038 -0.025331  0.013649  0.049066 -0.188642   \n",
       "99 -0.013094 -0.049977 -0.229780 -0.124082 -0.059042 -0.050357 -0.165812   \n",
       "\n",
       "        7         8         9     ...      5527      5528      5529      5530  \\\n",
       "0  -0.115899 -0.141132  0.147953  ... -0.126116 -0.096101 -0.088420 -0.044344   \n",
       "1   0.010809 -0.068202 -0.089802  ...  0.093196 -0.041646  0.037461 -0.010557   \n",
       "2   0.111417  0.101397 -0.133405  ...  0.104722  0.033976  0.084681 -0.009239   \n",
       "3  -0.203223 -0.212819 -0.365461  ... -0.254983 -0.198700 -0.277118 -0.237472   \n",
       "4  -0.119537 -0.128758 -0.412183  ... -0.135456 -0.066704 -0.007727 -0.089324   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95 -0.303463 -0.510182 -0.674103  ... -0.377451 -0.533407 -0.345817 -0.539590   \n",
       "96 -0.039871 -0.212728 -0.552148  ...  0.008112 -0.202753 -0.058736 -0.236887   \n",
       "97 -0.052120  0.009887  0.128994  ... -0.165326  0.035119  0.006946  0.052710   \n",
       "98 -0.019759 -0.033656  0.158397  ... -0.058312  0.015185 -0.051219  0.040894   \n",
       "99 -0.072948 -0.082494 -0.149557  ... -0.064496 -0.044798 -0.028459 -0.010321   \n",
       "\n",
       "        5531      5532      5533      5534      5535      5536  \n",
       "0  -0.110260 -0.211010  0.057965  0.023700 -0.094698 -0.011287  \n",
       "1  -0.079030 -0.176610 -0.107298 -0.020363 -0.083380 -0.036577  \n",
       "2   0.094522  0.160429 -0.034903 -0.005517  0.046650  0.004937  \n",
       "3  -0.209459 -0.129051 -0.270134 -0.215277 -0.174167 -0.229636  \n",
       "4  -0.093500 -0.120392 -0.259552 -0.104806 -0.120526 -0.148472  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95 -0.552869 -0.688724 -0.561133 -0.464320 -0.554635 -0.540232  \n",
       "96 -0.205654 -0.364200 -0.321659 -0.208627 -0.232714 -0.212720  \n",
       "97  0.053380  0.038392  0.064423  0.055372  0.063736  0.019149  \n",
       "98 -0.060379 -0.043594  0.069309  0.059667 -0.016317  0.063543  \n",
       "99 -0.036408 -0.095491 -0.047167 -0.018514 -0.069886 -0.049317  \n",
       "\n",
       "[100 rows x 5537 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_representation_W2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 10 mots les plus proches de change sont : \n",
      " \n",
      "arizona's\n",
      "graham\n",
      "immigration\n",
      "reform\n",
      "climate-change\n",
      "support\n",
      "ndp\n",
      "lizards\n",
      "interactive\n",
      "align\n",
      " \n",
      "##################################################\n",
      " \n",
      "Les 10 mots les plus proches de climate sont : \n",
      " \n",
      "roadblock\n",
      "legislation\n",
      "giles\n",
      "nations\n",
      "indigenous\n",
      "predicting\n",
      "qdr\n",
      "immigration\n",
      "wars\n",
      "at\n"
     ]
    }
   ],
   "source": [
    "# Etude de cette représentation via la cosinus similarité\n",
    "\n",
    "def closest_to(word,n_top_similar):\n",
    "    print('Les {} mots les plus proches de {} sont : '.format(n_top_similar,word))\n",
    "    print(' ')\n",
    "    for word in [w[0] for w in model_wv.most_similar(word,topn=n_top_similar)]:\n",
    "        print(word)\n",
    "        \n",
    "closest_to('change',10)\n",
    "print(' ')\n",
    "print('#'*50)\n",
    "print(' ')\n",
    "\n",
    "closest_to('climate',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Une approche naïve avec TF IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque mot, on a un vecteur le représentant. Pour réprésenter un tweet, on fait la moyenne des vecteurs correspondant à chacun de ces mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Représentation des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des mots TF-IDF\n",
    "cv=CountVectorizer()\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatage de la base pour la méthode TF-IDF\n",
    "corpus=clean_text_second(df.Tweet,threshold=2000)\n",
    "corpus_new=[]\n",
    "for tweet in corpus: \n",
    "    tweet_sentence=''\n",
    "    for word in tweet:\n",
    "        tweet_sentence=tweet_sentence+' '+word\n",
    "    corpus_new.append(tweet_sentence)\n",
    "    \n",
    "# Caclul des scores TF-IDF de chaque mot\n",
    "word_count_vector=cv.fit_transform(corpus_new)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "count_vector=cv.transform(corpus_new)\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "feature_names = cv.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Tweet initial : Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. \n",
      " \n",
      "Tweet nettoyé :  global warming report urges governments to act brussels belgium ap the world faces increased hunger and\n",
      " \n",
      "Scores TF-IDF : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>brussels</td>\n",
       "      <td>0.358888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>belgium</td>\n",
       "      <td>0.358888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hunger</td>\n",
       "      <td>0.337373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>urges</td>\n",
       "      <td>0.323202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>governments</td>\n",
       "      <td>0.312617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>experts</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>expert</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>experiments</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>experiment</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>à_the</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8166 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             first_tweet\n",
       "brussels        0.358888\n",
       "belgium         0.358888\n",
       "hunger          0.337373\n",
       "urges           0.323202\n",
       "governments     0.312617\n",
       "...                  ...\n",
       "experts         0.000000\n",
       "expert          0.000000\n",
       "experiments     0.000000\n",
       "experiment      0.000000\n",
       "à_the           0.000000\n",
       "\n",
       "[8166 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de représentation du premier tweet \n",
    "\n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    "\n",
    "#print du tweet\n",
    "print(' ')\n",
    "print('Tweet initial : '+df.iloc[0]['Tweet'])\n",
    "print(' ')\n",
    "\n",
    "#print du tweet nettoyé\n",
    "print('Tweet nettoyé : '+corpus_new[0])\n",
    "print(' ')\n",
    "print('Scores TF-IDF : ')\n",
    "#print des scores \n",
    "pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"first_tweet\"]).sort_values(by=[\"first_tweet\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5529</th>\n",
       "      <th>5530</th>\n",
       "      <th>5531</th>\n",
       "      <th>5532</th>\n",
       "      <th>5533</th>\n",
       "      <th>5534</th>\n",
       "      <th>5535</th>\n",
       "      <th>5536</th>\n",
       "      <th>5537</th>\n",
       "      <th>5538</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>à_only</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>à_poisoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>à_s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>à_t</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>à_the</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8166 rows × 5539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "00            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "000           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "02            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "04            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "062           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "à_only        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "à_poisoning   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "à_s           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "à_t           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "à_the         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "             5529  5530  5531  5532  5533  5534  5535  5536  5537  5538  \n",
       "00            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "000           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "02            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "04            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "062           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "à_only        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "à_poisoning   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "à_s           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "à_t           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "à_the         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[8166 rows x 5539 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Représentation de tout les tweets \n",
    "\n",
    "dt_tfidf_tweet = pd.DataFrame(tf_idf_vector.T.todense(),index=feature_names)\n",
    "dt_tfidf_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problématique : matrice très sparse (beaucoup de zéros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avec du clustering de mots d'après Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est d'utiliser la représentation de Word2Vec des mots pour faire du clustering sur tout les mots contenus dans la base. On se place ensuite au niveau de chaque tweet et on le représente par un vecteur qui compte le nombre de mots contenu dans chaque cluster. Plus pour analyser quels sont les mots qui se rapproche le plus selon la distance euclidienne ? (plutot que la cos similarité)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation Word2Vec de chaque mot \n",
    "\n",
    "clean_text=clean_text_second(df.Tweet,threshold=1000)\n",
    "\n",
    "print(\"Entrainement du modèle Word2Vec ...\")\n",
    "model = Word2Vec(clean_text, size=100, window=5, min_count=3, workers=4) \n",
    "\n",
    "model.train(clean_text, total_examples=len(clean_text), epochs=10) #réseau de neuronne du Word2Vec\n",
    "model_wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle W2v sur chaque token du corpus\n",
    "\n",
    "clean_text = reduce(add, clean_text)\n",
    "clean_text = set(clean_text)\n",
    "\n",
    "def Representation_W2v(tokenCorpus_unique):\n",
    "    new_sample = {}\n",
    "    number_not_in_vocab=0\n",
    "    for token in tokenCorpus_unique:\n",
    "            try : \n",
    "                new_sample[token]=list(model_wv.get_vector(token))\n",
    "            except: number_not_in_vocab=number_not_in_vocab+1\n",
    "    \n",
    "    print('{} mots sur {} ne sont pas dans le vocabulaire du modèle Word2Vec entrainé'.format(str(number_not_in_vocab),str(len(tokenCorpus_unique))))\n",
    "    return new_sample\n",
    "\n",
    "Representation_for_clustering=pd.DataFrame(Representation_W2v(clean_text)).T\n",
    "Representation_for_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Representation_for_clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du clustering \n",
    "\n",
    "n_cluster = 5\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_cluster) \n",
    "kmeans.fit(Representation_for_clustering)\n",
    "clusters=kmeans.predict(Representation_for_clustering).tolist()\n",
    "\n",
    "data=[]\n",
    "for x in range(len(clusters)):\n",
    "    data.append([Representation_for_clustering.index.tolist()[x],clusters[x]])\n",
    "    \n",
    "    \n",
    "results=pd.DataFrame(data= data , columns=['token','cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_cluster):\n",
    "    print('Nombre de mots dans le cluster numéro {} : '.format(k)+str(len(results[results.cluster==k])))\n",
    "    print(' ')\n",
    "    print('#'*20)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etude de chaque cluster spécifiquement\n",
    "cluster=2\n",
    "\n",
    "list_word=''\n",
    "for word_index in range(20):\n",
    "    word=list(results[results.cluster==cluster]['token'])[word_index]\n",
    "    list_word=list_word+' , '+word\n",
    "print('Parmis les mots du cluster {}, il y a : '.format(cluster)+list_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétation des clusters ? Refaire avec des modèles pré-entrainés ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Représentation de tweet avec des modèles pré-entrainés : Fast2vec et Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast2vec pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=clean_text_second(df.Tweet,threshold=1000)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Input formating for BERT\n",
    "\n",
    "#tokenization spécifique BERT\n",
    "def tokenization_BERT(tweet,tokenizer):\n",
    "    text=''\n",
    "    for word in tweet:\n",
    "        text=text+' '+word\n",
    "    tokenized_text='[CLS]'+text+' [SEP]'\n",
    "    tokenized_text = tokenizer.tokenize(tokenized_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "    \n",
    "    return(indexed_tokens,segments_ids)\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "# Embedding du tweet\n",
    "def BERT_embedding(tweet,tokenizer):\n",
    "    \n",
    "    #tokenization et convertion des inputs en tensors\n",
    "    index, segments= tokenization_BERT(tweet,tokenizer)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([index])\n",
    "    segments_tensors = torch.tensor([segments])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    token_vecs = encoded_layers[11][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return(list(sentence_embedding.numpy())) #return le vecteur du tweet\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [13:53<00:00, 489066.62B/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import du modèle\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet initial : Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. \n",
      " \n",
      "Tweet nettoyé :  global warming report urges governments to act brussels belgium ap the world faces increased hunger and\n",
      " \n",
      "10 premières valeurs de la représentation vectorielle : [-0.20707315, -0.1527531, 0.31561086, -0.102150574, -0.0494032, -0.15978688, 0.12970391, 0.61809164, -0.43857074, -0.18857116]\n",
      " \n",
      "####################\n",
      " \n",
      "Tweet initial : Fighting poverty and global warming in Africa \n",
      " \n",
      "Tweet nettoyé :  fighting poverty and global warming in africa\n",
      " \n",
      "10 premières valeurs de la représentation vectorielle : [-0.3177802, -0.19386752, -0.51339495, 0.08279662, 0.005352762, -0.3944389, 0.124147385, 0.6389174, -0.39283025, -0.5550124]\n",
      " \n",
      "####################\n",
      " \n",
      "Tweet initial : Carbon offsets: How a Vatican forest failed to reduce global warming \n",
      " \n",
      "Tweet nettoyé :  carbon offsets how a vatican forest failed to reduce global warming\n",
      " \n",
      "10 premières valeurs de la représentation vectorielle : [-0.11185432, -0.21433498, -0.17104717, 0.31577143, 0.13757117, -0.29026532, -0.00019155655, 0.4175995, -0.15806018, -0.09747195]\n",
      " \n",
      "####################\n",
      " \n",
      "Tweet initial : URUGUAY: Tools Needed for Those Most Vulnerable to Climate Change \n",
      " \n",
      "Tweet nettoyé :  uruguay tools needed for those most vulnerable to climate change\n",
      " \n",
      "10 premières valeurs de la représentation vectorielle : [-0.27096364, 0.050121903, -0.2203478, 0.3218468, 0.3050904, -0.21759973, 0.15318978, 0.48721266, -0.17184323, -0.06042658]\n",
      " \n",
      "####################\n",
      " \n",
      "Tweet initial : RT @sejorg: RT @JaymiHeimbuch: Ocean Saltiness Shows Global Warming Is Intensifying Our Water Cycle \n",
      " \n",
      "Tweet nettoyé :  rt rt ocean saltiness shows global warming is intensifying our water cycle\n",
      " \n",
      "10 premières valeurs de la représentation vectorielle : [-0.23678532, 0.08900328, 0.50781274, 0.06978822, 0.17861946, -0.27706355, 0.34692457, 0.466961, -0.3972774, -0.4910533]\n",
      " \n",
      "####################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Représentation des 5 premiers tweets de la base\n",
    "\n",
    "for tweet_index in range(5):\n",
    "    print('Tweet initial : '+df.iloc[tweet_index]['Tweet'])\n",
    "    print(' ')\n",
    "    text=''\n",
    "    for word in clean_text[tweet_index]:\n",
    "        text=text+' '+word\n",
    "    print('Tweet nettoyé : '+text)\n",
    "    print(' ')\n",
    "    print('10 premières valeurs de la représentation vectorielle : '+str(BERT_embedding(clean_text[tweet_index],tokenizer)[0:10]))\n",
    "    print(' ')\n",
    "    print('#'*20)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5539/5539 [14:40<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Représentation de la base entière par BERT\n",
    "\n",
    "dataframe={}\n",
    "for tweet_index in tqdm(range(len(clean_text))):\n",
    "    dataframe[tweet_index]=BERT_embedding(clean_text[tweet_index],tokenizer)\n",
    "df_BERT_embedding=pd.DataFrame(dataframe)\n",
    "\n",
    "df_BERT_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_BERT_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3540d91345e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_BERT_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_BERT_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "df_BERT_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction de la postion de chaque tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dt_tfidf_tweet.reset_index(drop=True).T\n",
    "Y=list(df.Existence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " nan,\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'No',\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'No',\n",
       " 'No',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " nan,\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN pas traités !\n",
    "Y=[Y[tweet]==]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithme de classification : XGboost, RandomForest, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RandomForest\n",
    "rfc = RandomForestClassifier(n_estimators=1000)\n",
    "rfc.fit(Xtrain,ytrain)\n",
    "\n",
    "##XgBoost Classifier\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(Xtrain,ytrain)\n",
    "\n",
    "##SVM\n",
    "svm = SVC(probability=True,C=2, kernel='rbf')\n",
    "svm.fit(Xtrain, ytrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

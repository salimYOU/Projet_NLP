{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import de la base nettoyée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Tokenization \n",
    "import nltk\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import collections\n",
    "\n",
    "#Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#BERT\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "#Clustering \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "# Import de la base intiale \n",
    "f = open('1377884570_tweet_global_warming.txt', 'r',newline='', encoding='ISO-8859-1')\n",
    "content = f.read().split('\\r')\n",
    "\n",
    "content_new=[]\n",
    "for x in content : \n",
    "    if len(x)>0:\n",
    "        content_new.append(x)\n",
    "\n",
    "content_new=content_new[1:len(content_new)]\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "# Création du dataframe\n",
    "\n",
    "col_tweet=[]\n",
    "col_existence=[]\n",
    "col_score=[]\n",
    "\n",
    "#Split tweet , Note\n",
    "\n",
    "for line in content_new:\n",
    "    if len(line.split('[link]'))==2:\n",
    "        (x,y)=line.split('[link]')\n",
    "        col_tweet.append(x)\n",
    "        col_existence.append(y)\n",
    "    else : \n",
    "        if len(line.split(',Yes,'))==2:\n",
    "            col_tweet.append(line.split(',Yes,')[0])\n",
    "            col_existence.append(',Yes,'+line.split(',Yes,')[1])\n",
    "        elif len(line.split(',No,'))==2:\n",
    "            col_tweet.append(line.split(',No,')[0])\n",
    "            col_existence.append(',No,'+line.split(',No,')[1])\n",
    "        elif len(line.split(',Y,'))==2:\n",
    "            col_tweet.append(line.split(',Y,')[0])\n",
    "            col_existence.append(',Yes,'+line.split(',Y,')[1])\n",
    "        elif len(line.split(',N/A,'))==2:\n",
    "            col_tweet.append(line.split(',N/A,')[0])\n",
    "            col_existence.append(',N/A,'+line.split(',N/A,')[1])\n",
    "        elif len(line.split(',NA,'))==2:\n",
    "            col_tweet.append(line.split(',NA,')[0])\n",
    "            col_existence.append(',NA,'+line.split(',NA,')[1])\n",
    "        elif len(line.split(',N,'))==2:\n",
    "            col_tweet.append(line.split(',N,')[0])\n",
    "            col_existence.append(',No,'+line.split(',N,')[1])\n",
    "        else : \n",
    "            print('erreur')\n",
    "            #print(line.split('[link]'))\n",
    "col_tweet.append('I truly  Fat ASS Gore should get the Scam Artist Award of the decade with his Global Warming and Energy Credits worth close to Billion')\n",
    "col_existence.append(' ,NA')\n",
    "col_tweet.append('Despite Climategate, LEFT investing heavily in global warming hysteria as new way 2 impose nat\\'l & international controls on human freedom.')\n",
    "col_existence.append(' ,NA')\n",
    "        \n",
    "# Split Existence/Note\n",
    "col_existence_new=[]\n",
    "\n",
    "for x in col_existence:\n",
    "    if len(x.split(','))==3:\n",
    "        col_existence_new.append(x.split(',')[1])\n",
    "        col_score.append(x.split(',')[2])\n",
    "    else:\n",
    "        col_existence_new.append('NA')\n",
    "        col_score.append('NA')\n",
    "        \n",
    "#Nettoyage existence\n",
    "for avis in range(len(col_existence_new)):\n",
    "    if col_existence_new[avis]=='NA' or col_existence_new[avis]=='N/A' or col_existence_new[avis]=='':\n",
    "        col_existence_new[avis]=np.nan\n",
    "        \n",
    "#Nettoyage score\n",
    "for score in range(len(col_score)):\n",
    "    if 'NA' not in col_score[score]:\n",
    "        col_score[score]=col_score[score].split('\\t')[0]\n",
    "        if len(col_score[score].split('\"'))>1:\n",
    "            col_score[score]=float(col_score[score].split('\"')[0])\n",
    "        else: \n",
    "            col_score[score]=float(col_score[score])\n",
    "            \n",
    "    else : \n",
    "        col_score[score]=np.nan\n",
    "\n",
    "#Creation du DataFrame\n",
    "dic={'Tweet':col_tweet,'Existence':col_existence_new,'Score':col_score}\n",
    "df=pd.DataFrame(dic)\n",
    "\n",
    "\n",
    "df.drop_duplicates(['Tweet'], inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "\n",
    "#On prend toutes les phrases de touts les texts, et on les concatène dans une liste, en les traitant auparavant\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def remove_hashtags(tokens):\n",
    "    tokens= map(lambda x : x.replace('#',''),tokens) #map : parcours tout les tokens\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_url(tokens): #pb pour https\n",
    "    tokens= filter(lambda x: \"http\" not in x, tokens) #filter : garde là où il y a True\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_html(tokens):\n",
    "    tokens= filter(lambda x: x[0]+x[-1]!='<>',tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_www(tokens):\n",
    "    tokens= filter(lambda x: \"www\" not in x, tokens) #filter : garde là où il y a True\n",
    "    return list(tokens)\n",
    "\n",
    "'''\n",
    "def remove_x95(clean_corpus):\n",
    "    for sentence_r in range(len(clean_corpus)):\n",
    "        sentence=clean_corpus[sentence_r]\n",
    "        for x in range(len(sentence)):\n",
    "            if '\\x95' in sentence[x]:\n",
    "                y=sentence[x].split('\\x95')\n",
    "                new_x=''\n",
    "                for part_x in y:\n",
    "                    new_x=new_x+part_x\n",
    "                sentence[x]=new_x\n",
    "        clean_corpus[sentence_r]=sentence\n",
    "    return(clean_corpus)\n",
    "'''\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def clean_ponctuation(text_tokens): # Nettoyage de la ponctuation\n",
    "\n",
    "    list_word_clean_ponctuation=[]\n",
    "    for tweet in text_tokens:\n",
    "        list_tweet=[]\n",
    "        for word in tweet:\n",
    "            if len(word)<2:\n",
    "                if (word=='a') or (word=='i') or (word=='u'):\n",
    "                    list_tweet.append(word)\n",
    "                if RepresentsInt(word):\n",
    "                    list_tweet.append(word)\n",
    "            else :\n",
    "                if (word!='..') & (word!='...') & (word!='rt'):\n",
    "                    list_tweet.append(word)\n",
    "        list_word_clean_ponctuation.append(list_tweet)\n",
    "    \n",
    "    return(list_word_clean_ponctuation)\n",
    "\n",
    "def remove_arobase(text_tokens):\n",
    "    \n",
    "    list_new_tokens=[]\n",
    "    for tweet in text_tokens:\n",
    "        new_tweet=[]\n",
    "        for word in tweet: \n",
    "            if '@' not in word:\n",
    "                new_tweet.append(word)\n",
    "        list_new_tokens.append(new_tweet)\n",
    "    \n",
    "    return(list_new_tokens)\n",
    "\n",
    "###############################################################\n",
    "###############################################################\n",
    "###############################################################\n",
    "\n",
    "def clean_text_first(corpus):\n",
    "    \n",
    "    tok=TweetTokenizer()\n",
    "    tokens=[]\n",
    "    for sample in corpus:\n",
    "        token=tok.tokenize(sample) \n",
    "        token=remove_url(token)\n",
    "        token=remove_html(token)\n",
    "        token=remove_hashtags(token)\n",
    "        token=remove_www(token)\n",
    "        token=list(map(lambda x : x.lower(),token)) #.lower() : met les majuscules en minuscules\n",
    "        tokens.append(token) #ajout du token à l'ensemble des phrases\n",
    "    \n",
    "    #Nettoyage de la ponctuation\n",
    "    tokens=clean_ponctuation(tokens)\n",
    "    \n",
    "    #Nettoyage des arobase : pour la plupart, se sont des noms propres\n",
    "    tokens=remove_arobase(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def clean_text_second(corpus,threshold): #On rajoute l'association des mots qui vont ensembles\n",
    "    \n",
    "    #clean les textes\n",
    "    tokens=clean_text_first(corpus)\n",
    "    \n",
    "    #associer les mots\n",
    "    phrases=Phrases(tokens,threshold=threshold) #On fait apprendre le modèle d'association sur tout les mots\n",
    "    phraser=Phraser(phrases) #Outil pour associer\n",
    "    \n",
    "    clean_tokens=[]\n",
    "    for token in tokens: #On parcours les phrases et on associe les mots\n",
    "        new_tokens=phraser[token]\n",
    "        clean_tokens.append(new_tokens)\n",
    "        \n",
    "    #tokens = remove_x95(tokens)\n",
    "    \n",
    "    return(clean_tokens)\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('On dispose de {} tweets.'.format(df.shape[0]))\n",
    "\n",
    "print(' ')\n",
    "print('On a {} données manquantes sur le label de l\\'avis du tweet (Yes, No) .'.format(str(df.isnull().sum()['Existence'])))\n",
    "print(' ')\n",
    "\n",
    "#Personnes convaincues du changement climatique \n",
    "print('On a {} tweets qui croit au Changement climatique.'.format(str(df[df.Existence=='Yes'].shape[0])))\n",
    "m=round(df[df.Existence=='Yes'].Score.describe()['mean'],2)\n",
    "std=round(df[df.Existence=='Yes'].Score.describe()['std'],2)\n",
    "print('Parmis ces personnes, le score est de {} en moyenne, avec un écart-type de {}.'.format(str(m),str(std)))\n",
    "print(' ')\n",
    "\n",
    "#Personnes qui doutent du changement climatique \n",
    "print('On a {} tweets qui remettent en doute le Changement climatique.'.format(str(df[df.Existence=='No'].shape[0])))\n",
    "m=round(df[df.Existence=='No'].Score.describe()['mean'],2)\n",
    "std=round(df[df.Existence=='No'].Score.describe()['std'],2)\n",
    "print('Parmis ces personnes, le score est de {} en moyenne, avec un écart-type de {}.'.format(str(m),str(std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Première analyse des mots les plus importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bases des tweets Yes et des tweets No\n",
    "\n",
    "df_yes=df[df.Existence=='Yes'].reset_index()\n",
    "Tweet_clean_yes=clean_text_second(df_yes.Tweet,threshold=1000) #avec association de mots\n",
    "Tweet_clean_yes=reduce(add, Tweet_clean_yes)\n",
    "\n",
    "df_no=df[df.Existence=='No'].reset_index()\n",
    "Tweet_clean_no=clean_text_second(df_no.Tweet,threshold=1000) #avec association de mots\n",
    "Tweet_clean_no=reduce(add, Tweet_clean_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots les plus occurents dans la base No\n",
    "counter=collections.Counter(Tweet_clean_no)\n",
    "\n",
    "number_word=30\n",
    "print('Les {} mots qui apparaissent le plus dans la base No sont (par ordre décroissant) :'.format(number_word))\n",
    "print(' ')\n",
    "for word in counter.most_common(number_word):\n",
    "    print(word[0]+' ('+str(word[1])+') ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots les plus occurents dans la base Yes\n",
    "counter=collections.Counter(Tweet_clean_yes)\n",
    "\n",
    "number_word=30\n",
    "print('Les {} mots qui apparaissent le plus dans la base Yes sont (par ordre décroissant) :'.format(number_word))\n",
    "print(' ')\n",
    "for word in counter.most_common(number_word):\n",
    "    print(word[0]+' ('+str(word[1])+') ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ils sont assez similaires. On regarde ceux qui font partis das 200 mots les plus occurents chez les Yes et qui ne font pas partis des 200 mots les plus occurents chez les No. Et inversement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=collections.Counter(Tweet_clean_yes)\n",
    "list_commun_yes=[word[0] for word in counter.most_common(200)]\n",
    "\n",
    "counter=collections.Counter(Tweet_clean_no)\n",
    "list_commun_no=[word[0] for word in counter.most_common(200)]\n",
    "\n",
    "list_spe_yes=[]\n",
    "for word in list_commun_yes:\n",
    "    if word not in list_commun_no:\n",
    "        list_spe_yes.append(word)\n",
    "        \n",
    "list_spe_no=[]\n",
    "for word in list_commun_no:\n",
    "    if word not in list_commun_yes:\n",
    "        list_spe_no.append(word)\n",
    "\n",
    "list_spe_yes, list_spe_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'interesse maintenant en détails aux statistiques de présences de ces mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mots_interessants_yes=['green','us','could','world','energy','earth','fight','worse','help','carbon']\n",
    "mots_interessants_no=['gore','hoax','scam','climategate','gop','teaparty','palin','collapse','fraud','believe']\n",
    "\n",
    "counter_yes=collections.Counter(Tweet_clean_yes)\n",
    "counter_no=collections.Counter(Tweet_clean_no)\n",
    "\n",
    "#Fréquence de ces mots\n",
    "frequence_yes_in_yes={}\n",
    "frequence_yes_in_no={}\n",
    "for word in mots_interessants_yes:\n",
    "    frequence_yes_in_yes[word]=counter_yes[word]/len(Tweet_clean_yes)*100\n",
    "    frequence_yes_in_no[word]=counter_no[word]/len(Tweet_clean_no)*100\n",
    "    \n",
    "    \n",
    "frequence_no_in_no={}\n",
    "frequence_no_in_yes={}\n",
    "for word in mots_interessants_no:\n",
    "    frequence_no_in_no[word]=(counter_no[word]/len(Tweet_clean_no))*100\n",
    "    frequence_no_in_yes[word]=(counter_yes[word]/len(Tweet_clean_yes))*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Number_apparition_tweets(list_tweet,word):\n",
    "    number=0\n",
    "    for tweet in list_tweet:\n",
    "        if word in tweet:\n",
    "            number=number+1\n",
    "    return(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mots intéressants en faveur du Changement climatique :')\n",
    "print(' ')\n",
    "print('fréquence : tout mots compris de la base Yes/No')\n",
    "print(' ')\n",
    "print('Fréquences dans les tweets Yes :')\n",
    "print(' ')\n",
    "list_tweet_yes=clean_text_second(df_yes.Tweet,threshold=1000)\n",
    "for key in frequence_yes_in_yes.keys():\n",
    "    print(key+': frequence = '+str(round(frequence_yes_in_yes[key],2))+'%'+' , nombre de tweets le contenant = '+str(Number_apparition_tweets(list_tweet_yes,key))+' (sur {})'.format(len(list_tweet_yes))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_yes,key)/len(list_tweet_yes),2)))                                   \n",
    "print(' ') \n",
    "print('#'*10)\n",
    "print(' ')\n",
    "print('Fréquences dans les tweets No :')\n",
    "print(' ')\n",
    "list_tweet_no=clean_text_second(df_no.Tweet,threshold=1000)\n",
    "for key in frequence_yes_in_no.keys():\n",
    "    print(key+': frequence  = '+str(round(frequence_yes_in_no[key],2))+'%'+' , nombre de tweets le contenant = '+str(Number_apparition_tweets(list_tweet_no,key))+' (sur {})'.format(len(list_tweet_no))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_no,key)/len(list_tweet_no),2))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mots intéressants contre Changement climatique :')\n",
    "print(' ')\n",
    "print('fréquence : tout mots compris de la base Yes/No')\n",
    "print(' ')\n",
    "print('Fréquences dans les tweets No :')\n",
    "print(' ')\n",
    "list_tweet_no=clean_text_second(df_no.Tweet,threshold=1000)\n",
    "for key in frequence_no_in_no.keys():\n",
    "    print(key+': frequence = '+str(round(frequence_no_in_no[key],2))+'%'+' , nombre de tweets le contenant = '+str(Number_apparition_tweets(list_tweet_no,key))+' (sur {})'.format(len(list_tweet_no))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_no,key)/len(list_tweet_no),2)))                                   \n",
    "print(' ') \n",
    "print('#'*10)\n",
    "print(' ')\n",
    "print('Fréquences dans les tweets No :')\n",
    "print(' ')\n",
    "list_tweet_yes=clean_text_second(df_yes.Tweet,threshold=1000)\n",
    "for key in frequence_no_in_yes.keys():\n",
    "    print(key+': frequence  = '+str(round(frequence_no_in_yes[key],2))+'%'+' , nombre de tweets le contenant = '+str(Number_apparition_tweets(list_tweet_yes,key))+' (sur {})'.format(len(list_tweet_yes))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_yes,key)/len(list_tweet_yes),2))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentaires...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modèle LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_return ={\"j\":wordnet.ADJ,\n",
    "                 \"n\": wordnet.NOUN,\n",
    "                 \"v\": wordnet.VERB,\n",
    "                 \"R\": wordnet.ADV}\n",
    "    return(tag_return.get(tag, wordnet.NOUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Représentation Bag-Of-Word\n",
    "\n",
    "Tweet_clean=clean_text_second(df.Tweet,threshold=1000)\n",
    "\n",
    "#####Etape déjà faite dans la nouvelle fonction clean_text\n",
    "Tweet_clean_lemmatize=[]\n",
    "for tweet in Tweet_clean:\n",
    "    new_tweet=[]\n",
    "    for token in tweet:\n",
    "        if token!='':\n",
    "            new_token=lemmatizer.lemmatize(token,get_wordnet_pos(token))\n",
    "            if new_token=='warm': #traitement du warming\n",
    "                new_token='warming'\n",
    "            new_tweet.append(new_token)\n",
    "    Tweet_clean_lemmatize.append(new_tweet)\n",
    "Tweet_clean=Tweet_clean_lemmatize\n",
    "\n",
    "# On enlève les mots de trois lettres ou moins \n",
    "Tweet_clean_lemmatize=[]\n",
    "for tweet in Tweet_clean:\n",
    "    new_tweet=[]\n",
    "    for token in tweet:\n",
    "        if len(token)>3:\n",
    "            new_tweet.append(token)\n",
    "    Tweet_clean_lemmatize.append(new_tweet)\n",
    "Tweet_clean=Tweet_clean_lemmatize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation Bag-Of-Word \n",
    "\n",
    "def Representation_BOW(Tweet_clean):\n",
    "    \n",
    "    bow=pd.DataFrame([Tweet_clean]).T\n",
    "    bow.columns=['headline_text']\n",
    "    bow=bow.headline_text\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(bow)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in bow]\n",
    "    \n",
    "    return(bow_corpus)\n",
    "    \n",
    "# Sur la base entière\n",
    "BOW_global=Representation_BOW(clean_text_second(df[(df.Existence=='Yes') | (df.Existence=='No')].Tweet,threshold=1000))\n",
    "BOW_yes=Representation_BOW(clean_text_second(df[df.Existence=='Yes'].Tweet,threshold=1000))\n",
    "BOW_no=Representation_BOW(clean_text_second(df[df.Existence=='No'].Tweet,threshold=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics=3\n",
    "print(' ')\n",
    "# Construction du modèle LDA sur la base des Yes et No\n",
    "print('Construction du modèle sur le base entière..')\n",
    "t=time.time()\n",
    "lda_model_global = gensim.models.LdaMulticore(BOW_global, num_topics=number_topics, id2word=dictionary, passes=2, workers=2)\n",
    "print('DONE ({}s)'.format(str(round(time.time()-t,2))))\n",
    "print(' ')\n",
    "\n",
    "# Construction du modèle LDA sur la base des Yes\n",
    "print('Construction du modèle sur le base Yes..')\n",
    "t=time.time()\n",
    "lda_model_yes = gensim.models.LdaMulticore(BOW_yes, num_topics=number_topics, id2word=dictionary, passes=2, workers=2)\n",
    "print('DONE ({}s)'.format(str(round(time.time()-t,2))))\n",
    "print(' ')\n",
    "\n",
    "# Construction du modèle LDA sur la base des No\n",
    "print('Construction du modèle sur le base No..')\n",
    "t=time.time()\n",
    "lda_model_no = gensim.models.LdaMulticore(BOW_no, num_topics=number_topics, id2word=dictionary, passes=2, workers=2)\n",
    "print('DONE ({}s)'.format(str(round(time.time()-t,2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model={'global':lda_model_global, 'Yes':lda_model_yes, 'No':lda_model_no}\n",
    "\n",
    "# Pour chaque sujet remonté par le modèle, on regarde quels sont les mots qui apparaissent et avec quels poids\n",
    "print(' ')\n",
    "for model in list_model.keys():\n",
    "    print('Sujets qui apparaissent dans la base {} : '.format(model))\n",
    "    print(' ')\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Sujet numéro : {} \\nWords: {}'.format(idx, topic))\n",
    "        print(' ')\n",
    "    print('#'*30)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut encore nettoyer certains mots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Etude de la tonalité des tweets avec SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque tweet, on prend les sentiments associés aux mots du tweet. On fait ensuite la moyenne des score. A la fin, pour chaque tweet on un score associé à la positivité, négativité et neutralité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def get_sentiment(word,tag):\n",
    "    \"\"\" returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \"\"\"\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wordnet.NOUN, wordnet.ADJ, wordnet.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wordnet.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
    "\n",
    "\n",
    "# Pour chaque tweet calcul des trois scores\n",
    "def sentiments_score(Tweets,Initial_Tweet,labels):\n",
    "    \n",
    "    Tweet_sentiments=[]\n",
    "    for tweet_index in range(len(Tweets)):\n",
    "\n",
    "        list_tweet=[]\n",
    "\n",
    "        # Append le tweet initial\n",
    "        list_tweet.append(Initial_Tweet[tweet_index])\n",
    "\n",
    "        # Append les scores\n",
    "\n",
    "        ##Calcul des scores pour chaque mot du tweet\n",
    "        pos_val = nltk.pos_tag(Tweets[tweet_index])\n",
    "        senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
    "\n",
    "        ## Moyenne de ces scores\n",
    "        pos_score=0\n",
    "        neg_score=0\n",
    "        neutre_score=0\n",
    "        size_senti=0\n",
    "        for score in senti_val:\n",
    "            if len(score)==3:\n",
    "                pos_score=pos_score+score[0]\n",
    "                neg_score=neg_score+score[1]\n",
    "                neutre_score=neutre_score+score[2]\n",
    "                size_senti=size_senti+1\n",
    "        if size_senti==0:\n",
    "            list_tweet.append('pas de mots connus par SWN')\n",
    "        else:\n",
    "            pos_score=pos_score/size_senti\n",
    "            neg_score=neg_score/size_senti\n",
    "            neutre_score=neutre_score/size_senti\n",
    "        \n",
    "            list_tweet.append([pos_score,neg_score,neutre_score])\n",
    "        \n",
    "        # Append le label \n",
    "        list_tweet.append(labels[tweet_index])\n",
    "    \n",
    "        Tweet_sentiments.append(list_tweet)\n",
    "    \n",
    "    return(Tweet_sentiments)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la base des tweets et calcul des scores de sentiments\n",
    "\n",
    "Tweet_clean=clean_text_second(df.Tweet,threshold=1000)\n",
    "Labels=df.Existence\n",
    "\n",
    "print(' ')\n",
    "print('Calcul des scores de chaque mot et des scores moyens pour chaque tweet..')\n",
    "t=time.time()\n",
    "Base_SWN=sentiments_score(Tweet_clean,df.Tweet,Labels)\n",
    "print('DONE ({}s)'.format(str(round(time.time()-t,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etude des résultats\n",
    "Base_SWN=pd.DataFrame(Base_SWN,columns=['Tweet','Scores','Label'])\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "# Scores obtenus pour les Yes\n",
    "Base_SWN_yes=Base_SWN[(Base_SWN.Label=='Yes')&(Base_SWN.Scores!='pas de mots connus par SWN')]\n",
    "\n",
    "print(' Pour les Tweets qui croient au réchauffement climatique : ')\n",
    "print(' ')\n",
    "\n",
    "# Print des 10 premiers tweets \n",
    "for tweet_index in range(10):\n",
    "    print(list(Base_SWN_yes.Tweet)[tweet_index])\n",
    "    print('Scores (pos/neg/neutre) : '+str(list(Base_SWN_yes.Scores)[tweet_index]))\n",
    "    print(' ')\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "\n",
    "print('#'*100)\n",
    "print('#'*100)\n",
    "print(' ')\n",
    "\n",
    "print(' Pour les Tweets qui ne croient pas  au réchauffement climatique : ')\n",
    "print(' ')\n",
    "# Scores obtenus pour les No\n",
    "Base_SWN_no=Base_SWN[(Base_SWN.Label=='No')&(Base_SWN.Scores!='pas de mots connus par SWN')]\n",
    "\n",
    "# Print des 10 premiers tweets \n",
    "for tweet_index in range(10):\n",
    "    print(list(Base_SWN_no.Tweet)[tweet_index])\n",
    "    print('Scores (pos/neg/neutre) : '+str(list(Base_SWN_no.Scores)[tweet_index]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut scorer un Tweet en faisant le classant positif si son score négatif est plus grand\n",
    "# que son score négatif ; et inversement. Si les deux scores sont égaux on le classe comme neutre\n",
    "\n",
    "def nombre_pos_neg_neut(base):\n",
    "    \n",
    "    nombre_pos=np.sum([(list(base.Scores)[tweet][0]>list(base.Scores)[tweet][1])*1 for tweet in range(len(base))])\n",
    "    nombre_neg=np.sum([(list(base.Scores)[tweet][1]>list(base.Scores)[tweet][0])*1 for tweet in range(len(base))])\n",
    "    nombre_neut=np.sum([(list(base.Scores)[tweet][0]==list(base.Scores)[tweet][1])*1 for tweet in range(len(base))])\n",
    "\n",
    "    return([nombre_pos,nombre_neg,nombre_neut])\n",
    "\n",
    "# Sur toute la base \n",
    "data=Base_SWN[Base_SWN.Scores!='pas de mots connus par SWN']\n",
    "results=nombre_pos_neg_neut(data)\n",
    "print(' ')\n",
    "print('Sur toute la base ({} tweets) : '.format(len(data)))\n",
    "print(' ')\n",
    "print('Le nombre de tweets jugés positifs est {} --> {}%'.format(str(results[0]),str(round(100*results[0]/len(data),2))))\n",
    "print('Le nombre de tweets jugés négatifs est {} --> {}%'.format(str(results[1]),str(round(100*results[1]/len(data),2))))\n",
    "print('Le nombre de tweets jugés neutres est {} --> {}%'.format(str(results[2]),str(round(100*results[2]/len(data),2))))\n",
    "print(' ')\n",
    "print('#'*30)\n",
    "print('#'*30)\n",
    "print(' ')\n",
    "\n",
    "# Sur la bases composée des Yes  \n",
    "data=Base_SWN_yes\n",
    "results=nombre_pos_neg_neut(data)\n",
    "print(' ')\n",
    "print('Sur la base composée des Yes ({} tweets) : '.format(len(data)))\n",
    "print(' ')\n",
    "print('Le nombre de tweets jugés positifs est {} --> {}%'.format(str(results[0]),str(round(100*results[0]/len(data),2))))\n",
    "print('Le nombre de tweets jugés négatifs est {} --> {}%'.format(str(results[1]),str(round(100*results[1]/len(data),2))))\n",
    "print('Le nombre de tweets jugés neutres est {} --> {}%'.format(str(results[2]),str(round(100*results[2]/len(data),2))))\n",
    "print(' ')\n",
    "print('#'*30)\n",
    "print('#'*30)\n",
    "print(' ')\n",
    "\n",
    "# Sur la bases composée des No \n",
    "data=Base_SWN_no\n",
    "results=nombre_pos_neg_neut(data)\n",
    "print(' ')\n",
    "print('Sur la base composée des No ({} tweets) : '.format(len(data)))\n",
    "print(' ')\n",
    "print('Le nombre de tweets jugés positifs est {} --> {}%'.format(str(results[0]),str(round(100*results[0]/len(data),2))))\n",
    "print('Le nombre de tweets jugés négatifs est {} --> {}%'.format(str(results[1]),str(round(100*results[1]/len(data),2))))\n",
    "print('Le nombre de tweets jugés neutres est {} --> {}%'.format(str(results[2]),str(round(100*results[2]/len(data),2))))\n",
    "print(' ')\n",
    "print('#'*30)\n",
    "print('#'*30)\n",
    "print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculs de statistiques détaillés sur les scores \n",
    "\n",
    "def stats_scores(base):\n",
    "    \n",
    "    lis_pos=[list(base.Scores)[tweet][0] for tweet in range(len(list(base.Scores)))]\n",
    "    list_neg=[list(base.Scores)[tweet][1] for tweet in range(len(list(base.Scores)))]\n",
    "    list_neutre=[list(base.Scores)[tweet][2] for tweet in range(len(list(base.Scores)))]\n",
    "    \n",
    "    pos_score=np.mean(lis_pos)\n",
    "    pos_score_ecart_type=np.std(lis_pos)\n",
    "    pos_max=np.max(lis_pos)\n",
    "    pos_3quant=np.quantile(lis_pos,0.75)\n",
    "    dic_pos={'moyenne':round(pos_score,3),'std':round(pos_score_ecart_type,3),'max':round(pos_max,3),'quant_3':round(pos_3quant,3)}\n",
    "    \n",
    "    neg_score=np.mean(list_neg)\n",
    "    neg_score_ecart_type=np.std(list_neg)\n",
    "    neg_max=np.max(list_neg)\n",
    "    neg_3quant=np.quantile(list_neg,0.75)\n",
    "    dic_neg={'moyenne':round(neg_score,3),'std':round(neg_score_ecart_type,3),'max':round(neg_max,3),'quant_3':round(neg_3quant,3)}\n",
    "    \n",
    "    neut_score=np.mean(list_neutre)\n",
    "    neut_score_ecart_type=np.std(list_neutre)\n",
    "    neut_max=np.max(list_neutre)\n",
    "    neut_3quant=np.quantile(list_neutre,0.75)\n",
    "    dic_neut={'moyenne':round(neut_score,3),'std':round(neut_score_ecart_type,3),'max':round(neut_max,3),'quant_3':round(neut_3quant,3)}\n",
    "    \n",
    "    disc_results={'positifs':dic_pos, 'negatifs':dic_neg,'neutre':dic_neut}\n",
    "    \n",
    "    return(disc_results)\n",
    "\n",
    "#Sur la base globale \n",
    "Base_SWN_scoreconnus=Base_SWN[Base_SWN.Scores!='pas de mots connus par SWN']\n",
    "disc_results=stats_scores(Base_SWN_scoreconnus)\n",
    "\n",
    "print(' ')\n",
    "print(' Sur l\\'ensemble de la base ({} tweets) : '.format(len(Base_SWN_scoreconnus)) )\n",
    "print(' ')\n",
    "print('Pour les scores positifs : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['positifs']['moyenne'],disc_results['positifs']['std'],disc_results['positifs']['max'],disc_results['positifs']['quant_3']))\n",
    "print('Pour les scores negatifs : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['negatifs']['moyenne'],disc_results['negatifs']['std'],disc_results['negatifs']['max'],disc_results['negatifs']['quant_3']))\n",
    "print('Pour les scores neutres : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['neutre']['moyenne'],disc_results['neutre']['std'],disc_results['neutre']['max'],disc_results['neutre']['quant_3']))\n",
    "print(' ')\n",
    "print('#'*30)\n",
    "print(' ')\n",
    "\n",
    "#Sur la base Yes\n",
    "disc_results=stats_scores(Base_SWN_yes)\n",
    "\n",
    "print(' ')\n",
    "print(' Sur la bases des tweets Yes ({} tweets) : '.format(len(Base_SWN_yes)))\n",
    "print(' ')\n",
    "print('Pour les scores positifs : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['positifs']['moyenne'],disc_results['positifs']['std'],disc_results['positifs']['max'],disc_results['positifs']['quant_3']))\n",
    "print('Pour les scores negatifs : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['negatifs']['moyenne'],disc_results['negatifs']['std'],disc_results['negatifs']['max'],disc_results['negatifs']['quant_3']))\n",
    "print('Pour les scores neutres : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['neutre']['moyenne'],disc_results['neutre']['std'],disc_results['neutre']['max'],disc_results['neutre']['quant_3']))\n",
    "print(' ')\n",
    "print('#'*30)\n",
    "print(' ')\n",
    "\n",
    "#Sur la base No\n",
    "disc_results=stats_scores(Base_SWN_no)\n",
    "\n",
    "print(' ')\n",
    "print(' Sur la bases des tweets No ({} tweets) : '.format(len(Base_SWN_no)))\n",
    "print(' ')\n",
    "print('Pour les scores positifs : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['positifs']['moyenne'],disc_results['positifs']['std'],disc_results['positifs']['max'],disc_results['positifs']['quant_3']))\n",
    "print('Pour les scores negatifs : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['negatifs']['moyenne'],disc_results['negatifs']['std'],disc_results['negatifs']['max'],disc_results['negatifs']['quant_3']))\n",
    "print('Pour les scores neutres : moyenne={}, ecart-type={}, max={}, quartile 3={}'.format(disc_results['neutre']['moyenne'],disc_results['neutre']['std'],disc_results['neutre']['max'],disc_results['neutre']['quant_3']))\n",
    "print(' ')\n",
    "print('#'*30)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot des distributions des scores positifs \n",
    "\n",
    "# Bases regroupant les scores positifs des deux bases \n",
    "lis_pos_no=[list(Base_SWN_no.Scores)[tweet][0] for tweet in range(len(list(Base_SWN_no.Scores)))]\n",
    "lis_pos_yes=[list(Base_SWN_yes.Scores)[tweet][0] for tweet in range(len(list(Base_SWN_yes.Scores)))]\n",
    "\n",
    "lis_neg_no=[list(Base_SWN_no.Scores)[tweet][1] for tweet in range(len(list(Base_SWN_no.Scores)))]\n",
    "lis_neg_yes=[list(Base_SWN_yes.Scores)[tweet][1] for tweet in range(len(list(Base_SWN_yes.Scores)))]\n",
    "\n",
    "\n",
    "Base_pos=pd.DataFrame([[x,y] for (x,y) in zip(lis_pos_yes,lis_pos_no)],columns=['Positifs Yes','Positifs No'])\n",
    "Base_neg=pd.DataFrame([[x,y] for (x,y) in zip(lis_neg_yes,lis_neg_no)],columns=['Negatifs Yes','Negatifs No']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Base_pos.plot.box(grid='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Base_neg.plot.box(grid='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etudier quels types de mots apparaissent des les plus forts quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering à partir des représentations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

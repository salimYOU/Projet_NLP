{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Notebook_projet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL5SJh2JoV3Q",
        "colab_type": "text"
      },
      "source": [
        "This Notebook aims to study the arguments developed by climate sceptics. This study is based on the database available at this address: https://www.figure-eight.com/data-for-everyone/.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkqyiO_36AnW",
        "colab_type": "text"
      },
      "source": [
        "**This Notebook needs approximately 2.5 minutes to run**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETbu9vW2oV3R",
        "colab_type": "text"
      },
      "source": [
        "# 0. Import of constants and databases\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt4B1_HsoV3S",
        "colab_type": "code",
        "outputId": "707ff3c5-cadc-4c3f-9557-d03357202435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "#Import of packages\n",
        "%matplotlib inline\n",
        "import time \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import marshal\n",
        "import io\n",
        "import ast\n",
        "import time \n",
        "\n",
        "#Tokenization \n",
        "import nltk\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "#Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "#TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Modèles de classifications\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#BERT\n",
        "import torch\n",
        "!pip install pytorch_pretrained_bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "#Clustering \n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "# Keras \n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "#LDA\n",
        "import gensim\n",
        "import nltk\n",
        "import time\n",
        "from nltk.corpus import wordnet\n",
        "import warnings\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#SentiWordNet\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('sentiwordnet')\n",
        "\n",
        "#Import of results and pre-run models (our repo github)\n",
        "\n",
        "!pip install gitpython\n",
        "import git\n",
        "\n",
        "\n",
        "git.Git(\"sample_data\").clone(\"git://github.com/salimYOU/Projet_NLP.git\")\n",
        "dic_representations=['tfidf','word2vec','fast2vec_cluster','fast2vec_mean', 'bert'] \n",
        "for name_representation in dic_representations:    \n",
        "  df = pd.read_csv('sample_data/Projet_NLP/representations/{}.csv.gz'.format(name_representation), compression='gzip',index_col=0)\n",
        "  df.to_csv('sample_data/Projet_NLP/representations/{}.csv'.format(name_representation))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.35)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.35 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.35)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.35->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 2.8MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.2 gitpython-3.1.0 smmap-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4pw9cmoV3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import of intial database\n",
        "\n",
        "f = open('sample_data/Projet_NLP/tweet_global_warming.txt', 'r',newline='', encoding='ISO-8859-1')\n",
        "content = f.read().split('\\r')\n",
        "\n",
        "content_new=[]\n",
        "for x in content : \n",
        "    if len(x)>0:\n",
        "        content_new.append(x)\n",
        "\n",
        "content_new=content_new[1:len(content_new)]\n",
        "\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "\n",
        "# generation of the dataframe\n",
        "\n",
        "col_tweet=[]\n",
        "col_existence=[]\n",
        "col_score=[]\n",
        "\n",
        "#Split tweet/Score\n",
        "\n",
        "for line in content_new:\n",
        "    if len(line.split('[link]'))==2:\n",
        "        (x,y)=line.split('[link]')\n",
        "        col_tweet.append(x)\n",
        "        col_existence.append(y)\n",
        "    else : \n",
        "        if len(line.split(',Yes,'))==2:\n",
        "            col_tweet.append(line.split(',Yes,')[0])\n",
        "            col_existence.append(',Yes,'+line.split(',Yes,')[1])\n",
        "        elif len(line.split(',No,'))==2:\n",
        "            col_tweet.append(line.split(',No,')[0])\n",
        "            col_existence.append(',No,'+line.split(',No,')[1])\n",
        "        elif len(line.split(',Y,'))==2:\n",
        "            col_tweet.append(line.split(',Y,')[0])\n",
        "            col_existence.append(',Yes,'+line.split(',Y,')[1])\n",
        "        elif len(line.split(',N/A,'))==2:\n",
        "            col_tweet.append(line.split(',N/A,')[0])\n",
        "            col_existence.append(',N/A,'+line.split(',N/A,')[1])\n",
        "        elif len(line.split(',NA,'))==2:\n",
        "            col_tweet.append(line.split(',NA,')[0])\n",
        "            col_existence.append(',NA,'+line.split(',NA,')[1])\n",
        "        elif len(line.split(',N,'))==2:\n",
        "            col_tweet.append(line.split(',N,')[0])\n",
        "            col_existence.append(',No,'+line.split(',N,')[1])\n",
        "\n",
        "col_tweet.append('I truly  Fat ASS Gore should get the Scam Artist Award of the decade with his Global Warming and Energy Credits worth close to Billion')\n",
        "col_existence.append(' ,NA')\n",
        "col_tweet.append('Despite Climategate, LEFT investing heavily in global warming hysteria as new way 2 impose nat\\'l & international controls on human freedom.')\n",
        "col_existence.append(' ,NA')\n",
        "        \n",
        "# Split Existence/Note\n",
        "col_existence_new=[]\n",
        "\n",
        "for x in col_existence:\n",
        "    if len(x.split(','))==3:\n",
        "        col_existence_new.append(x.split(',')[1])\n",
        "        col_score.append(x.split(',')[2])\n",
        "    else:\n",
        "        col_existence_new.append('NA')\n",
        "        col_score.append('NA')\n",
        "        \n",
        "#Cleaning 'existence'\n",
        "for avis in range(len(col_existence_new)):\n",
        "    if col_existence_new[avis]=='NA' or col_existence_new[avis]=='N/A' or col_existence_new[avis]=='':\n",
        "        col_existence_new[avis]=np.nan\n",
        "        \n",
        "#Cleaning 'score'\n",
        "for score in range(len(col_score)):\n",
        "    if 'NA' not in col_score[score]:\n",
        "        col_score[score]=col_score[score].split('\\t')[0]\n",
        "        if len(col_score[score].split('\"'))>1:\n",
        "            col_score[score]=float(col_score[score].split('\"')[0])\n",
        "        else: \n",
        "            col_score[score]=float(col_score[score])\n",
        "            \n",
        "    else : \n",
        "        col_score[score]=np.nan\n",
        "\n",
        "#Cleaning of the DataFrame\n",
        "dic={'Tweet':col_tweet,'Existence':col_existence_new,'Score':col_score}\n",
        "df=pd.DataFrame(dic)\n",
        "\n",
        "df.drop_duplicates(['Tweet'], inplace=True)\n",
        "df.reset_index(inplace=True,drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fstg5E4oV3i",
        "colab_type": "text"
      },
      "source": [
        "Some examples of tweets : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcJr7xbToV3j",
        "colab_type": "code",
        "outputId": "b4872143-830f-442d-fe51-7016f995ece9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Examples of tweets\n",
        "\n",
        "print('Examples of tweets from people who believe in climate change : ')\n",
        "print(' ')\n",
        "for k in range(5):\n",
        "    print(df[df.Existence=='Yes'].reset_index().iloc[k]['Tweet'])\n",
        "    print(' ')\n",
        "    \n",
        "print('#'*50)\n",
        "print('#'*50)\n",
        "print(' ')\n",
        "print('Examples of tweets from people who not believe in climate change : ')\n",
        "print(' ')\n",
        "for k in range(5):\n",
        "    print(df[df.Existence=='No'].reset_index().iloc[k]['Tweet'])\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Examples of tweets from people who believe in climate change : \n",
            " \n",
            "Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. \n",
            " \n",
            "Fighting poverty and global warming in Africa \n",
            " \n",
            "Carbon offsets: How a Vatican forest failed to reduce global warming \n",
            " \n",
            "URUGUAY: Tools Needed for Those Most Vulnerable to Climate Change \n",
            " \n",
            "RT @sejorg: RT @JaymiHeimbuch: Ocean Saltiness Shows Global Warming Is Intensifying Our Water Cycle \n",
            " \n",
            "##################################################\n",
            "##################################################\n",
            " \n",
            "Examples of tweets from people who not believe in climate change : \n",
            " \n",
            "Wait here's an idea: it's natural climate change, not human induced global warming. \n",
            " \n",
            "@New_federalists  i have it on good auth tht global warming also causes toe fungus.  We R all fortunate tht thr IS no global warming! #tcot\n",
            " \n",
            "Illegal war and the myth of global warming|My main campaign platform for this election will be the illegal .. \n",
            " \n",
            "the scientific community was scamed by global green  gov warming scam.\n",
            " \n",
            "40 degrees in NYC. please urinate on next liberal global warming /climate change scum you see.\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjwRskYCoV3p",
        "colab_type": "text"
      },
      "source": [
        " A little description about the database : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V6rHK-6oV3p",
        "colab_type": "code",
        "outputId": "de975f81-851a-40da-d4cd-4645318005e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print('We have {} tweets.'.format(df.shape[0]))\n",
        "print(' ')\n",
        "print('We have {} missing data on the label from the tweet advisory (Yes, No) .'.format(str(df.isnull().sum()['Existence'])))\n",
        "print(' ')\n",
        "print('We have {} tweets from people who believe in climate change.'.format(str(df[df.Existence=='Yes'].shape[0])))\n",
        "print('We have {} tweets from people who not believe in climate change.'.format(str(df[df.Existence=='No'].shape[0])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 5539 tweets.\n",
            " \n",
            "We have 1683 missing data on the label from the tweet advisory (Yes, No) .\n",
            " \n",
            "We have 2821 tweets from people who believe in climate change.\n",
            "We have 1035 tweets from people who not believe in climate change.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf-_bvJboV3y",
        "colab_type": "text"
      },
      "source": [
        "# I. Cleaning of the data and tweet representation (word-embedding) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3iE1M-LoV3y",
        "colab_type": "text"
      },
      "source": [
        "### I.a Tokenization of the tweets and cleaning of the tokens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKGT1cxDoV3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
        "    tag_return = {\"j\": wordnet.ADJ,\n",
        "                \"n\": wordnet.NOUN,\n",
        "                \"v\": wordnet.VERB,\n",
        "                \"r\": wordnet.ADV}\n",
        "    return tag_return.get(tag, wordnet.NOUN)\n",
        "\n",
        "# We take all sentences of all texts, and we concatenate them into a list\n",
        "# We process them before\n",
        "\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def remove_hashtags(tokens):\n",
        "    tokens= map(lambda x : x.replace('#',''),tokens) #map : go through all tokens\n",
        "    return list(tokens)\n",
        "\n",
        "def remove_url(tokens): \n",
        "    tokens= filter(lambda x: \"http\" not in x, tokens) \n",
        "    return list(tokens)\n",
        "\n",
        "def remove_html(tokens):\n",
        "    tokens= filter(lambda x: x[0]+x[-1]!='<>',tokens)\n",
        "    return list(tokens)\n",
        "\n",
        "def remove_www(tokens):\n",
        "    tokens= filter(lambda x: \"www\" not in x, tokens) \n",
        "    return list(tokens)\n",
        "\n",
        "def remove_contraction(sample):\n",
        "    sample = sample.lower()\n",
        "    sample = sample.replace('won\\'t', ' will not')    \n",
        "    sample = sample.replace('n\\'t',' not')\n",
        "    sample = sample.replace('\\'ll',' will') \n",
        "    sample = sample.replace('it\\'s','it is')    \n",
        "    sample = sample.replace('he\\'s','he is')    \n",
        "    sample = sample.replace('she\\'s','she is')    \n",
        "    sample = sample.replace('\\'re', ' are')\n",
        "    sample = sample.replace('that\\'s', 'that is')\n",
        "    return sample\n",
        "\n",
        "\n",
        "def RepresentsInt(s):\n",
        "    try: \n",
        "        int(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def clean_ponctuation(text_tokens): # Cleaning of the ponctuation\n",
        "\n",
        "    list_word_clean_ponctuation=[]\n",
        "    for tweet in text_tokens:\n",
        "        list_tweet=[]\n",
        "        for word in tweet:\n",
        "            if len(word)<2:\n",
        "                if (word=='a') or (word=='i') or (word=='u'):\n",
        "                    list_tweet.append(word)\n",
        "                if RepresentsInt(word):\n",
        "                    list_tweet.append(word)\n",
        "            else :\n",
        "                if (word!='..') & (word!='...') & (word!='rt'):\n",
        "                    list_tweet.append(word)\n",
        "        list_word_clean_ponctuation.append(list_tweet)\n",
        "    \n",
        "    return(list_word_clean_ponctuation)\n",
        "\n",
        "def remove_arobase(text_tokens):\n",
        "    \n",
        "    list_new_tokens=[]\n",
        "    for tweet in text_tokens:\n",
        "        new_tweet=[]\n",
        "        for word in tweet: \n",
        "            if '@' not in word:\n",
        "                new_tweet.append(word)\n",
        "        list_new_tokens.append(new_tweet)\n",
        "    \n",
        "    return(list_new_tokens)\n",
        "\n",
        "###############################################################\n",
        "###############################################################\n",
        "###############################################################\n",
        "\n",
        "def clean_text_first(corpus):\n",
        "    \n",
        "    tok=TweetTokenizer()\n",
        "    tokens=[]\n",
        "    for sample in corpus:\n",
        "        sample = remove_contraction(sample)\n",
        "        token=tok.tokenize(sample)\n",
        "        token=remove_url(token)\n",
        "        token=remove_html(token)\n",
        "        token=remove_hashtags(token)\n",
        "        token=remove_www(token)\n",
        "        token2 = []\n",
        "        for elt in token:\n",
        "            if elt != '':\n",
        "                if elt != \"warming\":\n",
        "                    token2.append(lemmatizer.lemmatize(elt, get_wordnet_pos(elt)))\n",
        "                else:\n",
        "                    token2.append(\"warming\")\n",
        "        token=list(map(lambda x : x.lower(),token2)) #.lower() : convert upper on lower case\n",
        "        tokens.append(token) #append token in all sentences \n",
        "    \n",
        "    #Cleaning of ponctuation\n",
        "    tokens=clean_ponctuation(tokens)\n",
        "    \n",
        "    #Cleaning of @ : mostly, they are proper names\n",
        "    tokens=remove_arobase(tokens)\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "def clean_text_second(corpus,threshold=10, stopwords_out = False): #We add association of pairs words\n",
        "    \n",
        "    #Celaing of texts \n",
        "    tokens=clean_text_first(corpus)\n",
        "    \n",
        "    #Association of words \n",
        "    phrases=Phrases(tokens,threshold=threshold) \n",
        "    phraser=Phraser(phrases) \n",
        "    \n",
        "    clean_tokens=[]\n",
        "    for token in tokens: #We go through all sentences and associate words \n",
        "        new_tokens=phraser[token]\n",
        "        if stopwords_out :\n",
        "            new_tokens2 = []\n",
        "            for elt in new_tokens:\n",
        "                if elt not in stopWords:\n",
        "                    new_tokens2.append(elt)\n",
        "            clean_tokens.append(new_tokens2)\n",
        "        else:\n",
        "            clean_tokens.append(new_tokens)\n",
        "    \n",
        "    return(clean_tokens)\n",
        "\n",
        "\"\"\"\n",
        "# Save clean tokens \n",
        "clean_text=clean_text_second(df.Tweet)\n",
        "data=pd.DataFrame([clean_text]).T\n",
        "data.to_csv('sample_data/Projet_NLP/clean_text.csv')\n",
        "\n",
        "df_yes=df[df.Existence=='Yes'].reset_index()\n",
        "clean_text_yes=clean_text_second(df_yes.Tweet)\n",
        "data=pd.DataFrame([clean_text_yes]).T\n",
        "data.to_csv('sample_data/Projet_NLP/clean_text_yes.csv')\n",
        "\n",
        "df_no=df[df.Existence=='No'].reset_index()\n",
        "clean_text_no=clean_text_second(df_no.Tweet)\n",
        "data=pd.DataFrame([clean_text_no]).T\n",
        "data.to_csv('sample_data/Projet_NLP/clean_text_no.csv')\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Fonction to import clean tokens \n",
        "def import_clean_text(option='all'):\n",
        "\n",
        "    if option=='all':\n",
        "        clean_new=[]\n",
        "        clean_text=pd.read_csv('sample_data/Projet_NLP/clean_text.csv',index_col=0)\n",
        "        for tweet in list(clean_text['0']):\n",
        "            clean_new.append(ast.literal_eval(tweet))\n",
        "        clean_text=clean_new\n",
        "\n",
        "    if option=='yes':\n",
        "        clean_new=[]\n",
        "        clean_text=pd.read_csv('sample_data/Projet_NLP/clean_text_yes.csv',index_col=0)\n",
        "        for tweet in list(clean_text['0']):\n",
        "            clean_new.append(ast.literal_eval(tweet))\n",
        "        clean_text=clean_new\n",
        "        \n",
        "    if option=='no':\n",
        "        clean_new=[]\n",
        "        clean_text=pd.read_csv('sample_data/Projet_NLP/clean_text_no.csv',index_col=0)\n",
        "        for tweet in list(clean_text['0']):\n",
        "            clean_new.append(ast.literal_eval(tweet))\n",
        "        clean_text=clean_new\n",
        "    \n",
        "    return(clean_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tedDpxLFoV35",
        "colab_type": "text"
      },
      "source": [
        "Most frequent words after cleaning up tweets : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCl91u1SoV36",
        "colab_type": "code",
        "outputId": "d21b3db2-aa50-458f-82dd-7edb02fd6a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#Most frequent words after cleaning up tweets : \n",
        "\n",
        "list_words=import_clean_text()\n",
        "\n",
        "counter=collections.Counter(reduce(add, list_words))\n",
        "\n",
        "#10 most common words  \n",
        "number_word=10\n",
        "print('{} words that appear the most are (in descending order) :'.format(number_word))\n",
        "print(' ')\n",
        "for word in counter.most_common(number_word):\n",
        "    print(word[0]+' ('+str(word[1])+') ') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 words that appear the most are (in descending order) :\n",
            " \n",
            "climate_change (2936) \n",
            "global_warming (2709) \n",
            "the (2275) \n",
            "be (1801) \n",
            "to (1443) \n",
            "of (1328) \n",
            "a (1193) \n",
            "on (962) \n",
            "and (869) \n",
            "in (845) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGcgCN7DoV4C",
        "colab_type": "code",
        "outputId": "22ff3e59-60a4-4726-df49-762db9622f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mean size of the tweets : \n",
        "list_words\n",
        "mean=0\n",
        "for tweet in list_words:\n",
        "    mean=mean+len(tweet)\n",
        "    \n",
        "print('On average, {} words were retained per tweet '.format(str(round(mean/len(list_words),0))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On average, 12.0 words were retained per tweet \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flroHI1CoV4I",
        "colab_type": "text"
      },
      "source": [
        "### I.b TF-IDF representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnLsCcIyoV4J",
        "colab_type": "text"
      },
      "source": [
        "For each word, we have a vector representing it. To represent a tweet, we average the vectors corresponding to each of these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obWtyQ6_oV4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialization of TF-IDF words\n",
        "cv = CountVectorizer()\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "\n",
        "# Formating database for TF-IDF method\n",
        "corpus=clean_text_second(df.Tweet,threshold=20, stopwords_out = True)\n",
        "corpus_new=[]\n",
        "for tweet in corpus: \n",
        "    tweet_sentence=''\n",
        "    for word in tweet:\n",
        "        tweet_sentence=tweet_sentence+' '+word\n",
        "    corpus_new.append(tweet_sentence)\n",
        "    \n",
        "# Compute TF-IDF score for each word\n",
        "word_count_vector=cv.fit_transform(corpus_new)\n",
        "tfidf_transformer.fit(word_count_vector)\n",
        "\n",
        "count_vector=cv.transform(corpus_new)\n",
        "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
        "feature_names = cv.get_feature_names()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjVhmb0toV4P",
        "colab_type": "code",
        "outputId": "2f416a34-20e0-4c76-9822-f88cec005e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "# Example of representation for the first tweet\n",
        "\n",
        "#get tfidf vector for first document\n",
        "first_document_vector=tf_idf_vector[0]\n",
        "\n",
        "#print of tweet\n",
        "print(' ')\n",
        "print('Initial tweet: '+df.iloc[0]['Tweet'])\n",
        "print(' ')\n",
        "\n",
        "#print of cleaning up tweet\n",
        "print('Cleaning tweet : '+corpus_new[0])\n",
        "print(' ')\n",
        "print('TF-IDF scores : ')\n",
        "#print of the scores \n",
        "pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"first_tweet\"]).sort_values(by=[\"first_tweet\"],ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Initial tweet: Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. \n",
            " \n",
            "Cleaning tweet :  global warming report urge government act brussels belgium ap world face increase hunger\n",
            " \n",
            "TF-IDF scores : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>belgium</th>\n",
              "      <td>0.382666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brussels</th>\n",
              "      <td>0.382666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hunger</th>\n",
              "      <td>0.359726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>urge</th>\n",
              "      <td>0.320411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>face</th>\n",
              "      <td>0.287450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exclusive</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excite</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exchange</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excess</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>à_the</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7191 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           first_tweet\n",
              "belgium       0.382666\n",
              "brussels      0.382666\n",
              "hunger        0.359726\n",
              "urge          0.320411\n",
              "face          0.287450\n",
              "...                ...\n",
              "exclusive     0.000000\n",
              "excite        0.000000\n",
              "exchange      0.000000\n",
              "excess        0.000000\n",
              "à_the         0.000000\n",
              "\n",
              "[7191 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7PdnpLCoV4T",
        "colab_type": "code",
        "outputId": "cc27c208-d9a8-4801-ddc4-e38edd166604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Representation of all tweets \n",
        "\n",
        "\"\"\"\n",
        "df_tfidf = pd.DataFrame(tf_idf_vector.T.todense(),index=feature_names)\n",
        "df_tfidf=df_tfidf.T\n",
        "df_tfidf.columns=[x for x in range(df_tfidf.shape[1])]\n",
        "df_tfidf['Labels']=df.Existence\n",
        "df_tfidf.to_csv('sample_data/Projet_NLP/representations/tfidf.csv')\n",
        "df_tfidf\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf_tfidf = pd.DataFrame(tf_idf_vector.T.todense(),index=feature_names)\\ndf_tfidf=df_tfidf.T\\ndf_tfidf.columns=[x for x in range(df_tfidf.shape[1])]\\ndf_tfidf['Labels']=df.Existence\\ndf_tfidf.to_csv('sample_data/Projet_NLP/representations/tfidf.csv')\\ndf_tfidf\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZBTnSmoV4X",
        "colab_type": "code",
        "outputId": "da2bfb11-b199-4224-f956-7f5d0654c5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df_tfidf=pd.read_csv('sample_data/Projet_NLP/representations/tfidf.csv',index_col=0)\n",
        "df_tfidf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>7152</th>\n",
              "      <th>7153</th>\n",
              "      <th>7154</th>\n",
              "      <th>7155</th>\n",
              "      <th>7156</th>\n",
              "      <th>7157</th>\n",
              "      <th>7158</th>\n",
              "      <th>7159</th>\n",
              "      <th>7160</th>\n",
              "      <th>7161</th>\n",
              "      <th>7162</th>\n",
              "      <th>7163</th>\n",
              "      <th>7164</th>\n",
              "      <th>7165</th>\n",
              "      <th>7166</th>\n",
              "      <th>7167</th>\n",
              "      <th>7168</th>\n",
              "      <th>7169</th>\n",
              "      <th>7170</th>\n",
              "      <th>7171</th>\n",
              "      <th>7172</th>\n",
              "      <th>7173</th>\n",
              "      <th>7174</th>\n",
              "      <th>7175</th>\n",
              "      <th>7176</th>\n",
              "      <th>7177</th>\n",
              "      <th>7178</th>\n",
              "      <th>7179</th>\n",
              "      <th>7180</th>\n",
              "      <th>7181</th>\n",
              "      <th>7182</th>\n",
              "      <th>7183</th>\n",
              "      <th>7184</th>\n",
              "      <th>7185</th>\n",
              "      <th>7186</th>\n",
              "      <th>7187</th>\n",
              "      <th>7188</th>\n",
              "      <th>7189</th>\n",
              "      <th>7190</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5534</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5535</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5536</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5537</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5538</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5539 rows × 7192 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1    2    3    4    5  ...  7186  7187  7188  7189  7190  Labels\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     Yes\n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     Yes\n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     Yes\n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     Yes\n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     Yes\n",
              "...   ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...     ...\n",
              "5534  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     NaN\n",
              "5535  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      No\n",
              "5536  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      No\n",
              "5537  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      No\n",
              "5538  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0     NaN\n",
              "\n",
              "[5539 rows x 7192 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLL-Oz7XoV4e",
        "colab_type": "text"
      },
      "source": [
        "### I.c Word2Vec representation with training on the database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEkK5ophoV4f",
        "colab_type": "text"
      },
      "source": [
        "First of all, the Word2Vec algorithm is used to represent these tweets. Each word has a vector representation. For each tweet, we average the vectors (each word) included in this tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AUX_AJIoV4f",
        "colab_type": "code",
        "outputId": "f6c39093-5e10-4919-c48f-65d9853a2d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Cleaning of tweets\n",
        "clean_text=import_clean_text()\n",
        "\n",
        "print(\"Word2Vec training...\")\n",
        "model = Word2Vec(clean_text, size=100, window=5, min_count=3, workers=4) \n",
        "\n",
        "model.train(clean_text, total_examples=len(clean_text), epochs=10) #neural network Word2Vec\n",
        "model_wv = model.wv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAUNel1uoV4j",
        "colab_type": "code",
        "outputId": "23bc6181-e8ca-4071-828e-47ffbfb76e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Tweet representation by meaning vectors of each word on a tweet \n",
        "\n",
        "def tokens2vectors(tokenCorpus):\n",
        "    #transforms our X into a list of list of vec (2D array) \n",
        "    new_sample = list()\n",
        "    i=0\n",
        "    labels=[]\n",
        "    for sample in tokenCorpus:\n",
        "        tweetVecs = list()\n",
        "        for token in sample:\n",
        "            try : \n",
        "                tweetVecs.append(model_wv.get_vector(token)  )\n",
        "            except: \n",
        "                i=i+1\n",
        "                tweetVecs.append( np.zeros(100) ) \n",
        "        new_sample.append(np.mean(tweetVecs, axis=0))\n",
        "    \n",
        "    return np.array(new_sample)\n",
        "\n",
        "\n",
        "X= tokens2vectors(clean_text)\n",
        "\n",
        "Y=[]\n",
        "labels=[]\n",
        "index_label=0\n",
        "for x in list(X):\n",
        "    try: \n",
        "        Y.append(list(x))\n",
        "        labels.append(df.Existence[index_label])\n",
        "        index_label=index_label+1\n",
        "    except : \n",
        "        index_label=index_label+1\n",
        "        pass\n",
        "    \n",
        "df_word2vec= pd.DataFrame(Y,columns=[str(x) for x in range(len(Y[0]))])\n",
        "df_word2vec['Labels']=labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Ji9THsoV4m",
        "colab_type": "code",
        "outputId": "e94d6db4-2bfd-4d0f-f957-5da06fb171a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df_word2vec.to_csv('sample_data/Projet_NLP/representations/word2vec.csv')\n",
        "df_word2vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038422</td>\n",
              "      <td>-0.054940</td>\n",
              "      <td>-0.026879</td>\n",
              "      <td>0.212916</td>\n",
              "      <td>0.133102</td>\n",
              "      <td>0.016153</td>\n",
              "      <td>0.153705</td>\n",
              "      <td>0.039869</td>\n",
              "      <td>-0.019208</td>\n",
              "      <td>0.319491</td>\n",
              "      <td>-0.038438</td>\n",
              "      <td>-0.020885</td>\n",
              "      <td>0.377016</td>\n",
              "      <td>-0.421053</td>\n",
              "      <td>-0.136682</td>\n",
              "      <td>-0.284889</td>\n",
              "      <td>-0.001499</td>\n",
              "      <td>0.320178</td>\n",
              "      <td>-0.308142</td>\n",
              "      <td>-0.160106</td>\n",
              "      <td>-0.107245</td>\n",
              "      <td>0.449948</td>\n",
              "      <td>0.088689</td>\n",
              "      <td>0.208480</td>\n",
              "      <td>0.127086</td>\n",
              "      <td>0.005575</td>\n",
              "      <td>-0.106528</td>\n",
              "      <td>-0.056126</td>\n",
              "      <td>-0.313498</td>\n",
              "      <td>-0.168791</td>\n",
              "      <td>0.196229</td>\n",
              "      <td>-0.411824</td>\n",
              "      <td>0.397756</td>\n",
              "      <td>0.315542</td>\n",
              "      <td>-0.105504</td>\n",
              "      <td>0.135827</td>\n",
              "      <td>-0.284630</td>\n",
              "      <td>0.223583</td>\n",
              "      <td>0.044572</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.150093</td>\n",
              "      <td>-0.019681</td>\n",
              "      <td>-0.326594</td>\n",
              "      <td>0.022410</td>\n",
              "      <td>-0.017792</td>\n",
              "      <td>0.049503</td>\n",
              "      <td>0.127317</td>\n",
              "      <td>-0.297339</td>\n",
              "      <td>0.163908</td>\n",
              "      <td>0.181536</td>\n",
              "      <td>0.240021</td>\n",
              "      <td>0.027537</td>\n",
              "      <td>-0.236166</td>\n",
              "      <td>0.115694</td>\n",
              "      <td>0.214974</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>-0.314100</td>\n",
              "      <td>-0.191791</td>\n",
              "      <td>0.129458</td>\n",
              "      <td>-0.053207</td>\n",
              "      <td>0.033156</td>\n",
              "      <td>0.160460</td>\n",
              "      <td>-0.266733</td>\n",
              "      <td>0.125641</td>\n",
              "      <td>0.188067</td>\n",
              "      <td>-0.185768</td>\n",
              "      <td>-0.171468</td>\n",
              "      <td>-0.061191</td>\n",
              "      <td>0.212438</td>\n",
              "      <td>-0.351616</td>\n",
              "      <td>0.412419</td>\n",
              "      <td>-0.149385</td>\n",
              "      <td>0.095579</td>\n",
              "      <td>0.079496</td>\n",
              "      <td>-0.233274</td>\n",
              "      <td>-0.330428</td>\n",
              "      <td>-0.416172</td>\n",
              "      <td>0.418054</td>\n",
              "      <td>0.274729</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.041158</td>\n",
              "      <td>-0.022320</td>\n",
              "      <td>-0.000589</td>\n",
              "      <td>0.220677</td>\n",
              "      <td>0.213429</td>\n",
              "      <td>0.024704</td>\n",
              "      <td>0.166882</td>\n",
              "      <td>0.027566</td>\n",
              "      <td>-0.020027</td>\n",
              "      <td>0.438649</td>\n",
              "      <td>-0.048930</td>\n",
              "      <td>-0.015182</td>\n",
              "      <td>0.412347</td>\n",
              "      <td>-0.503576</td>\n",
              "      <td>-0.147170</td>\n",
              "      <td>-0.293896</td>\n",
              "      <td>-0.040550</td>\n",
              "      <td>0.359880</td>\n",
              "      <td>-0.409436</td>\n",
              "      <td>-0.230064</td>\n",
              "      <td>-0.146785</td>\n",
              "      <td>0.545163</td>\n",
              "      <td>0.123432</td>\n",
              "      <td>0.240808</td>\n",
              "      <td>0.156222</td>\n",
              "      <td>0.008517</td>\n",
              "      <td>-0.153286</td>\n",
              "      <td>-0.011497</td>\n",
              "      <td>-0.351413</td>\n",
              "      <td>-0.230550</td>\n",
              "      <td>0.233498</td>\n",
              "      <td>-0.507971</td>\n",
              "      <td>0.454806</td>\n",
              "      <td>0.343359</td>\n",
              "      <td>-0.125951</td>\n",
              "      <td>0.185396</td>\n",
              "      <td>-0.319252</td>\n",
              "      <td>0.267833</td>\n",
              "      <td>0.097448</td>\n",
              "      <td>0.078768</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.176307</td>\n",
              "      <td>-0.048122</td>\n",
              "      <td>-0.394991</td>\n",
              "      <td>0.029752</td>\n",
              "      <td>0.016772</td>\n",
              "      <td>0.014019</td>\n",
              "      <td>0.113139</td>\n",
              "      <td>-0.335540</td>\n",
              "      <td>0.163189</td>\n",
              "      <td>0.181605</td>\n",
              "      <td>0.276959</td>\n",
              "      <td>0.054697</td>\n",
              "      <td>-0.253796</td>\n",
              "      <td>0.101026</td>\n",
              "      <td>0.232291</td>\n",
              "      <td>-0.062892</td>\n",
              "      <td>-0.347325</td>\n",
              "      <td>-0.225779</td>\n",
              "      <td>0.154493</td>\n",
              "      <td>-0.056581</td>\n",
              "      <td>0.038373</td>\n",
              "      <td>0.219760</td>\n",
              "      <td>-0.315699</td>\n",
              "      <td>0.137913</td>\n",
              "      <td>0.200152</td>\n",
              "      <td>-0.228742</td>\n",
              "      <td>-0.190907</td>\n",
              "      <td>-0.070726</td>\n",
              "      <td>0.226788</td>\n",
              "      <td>-0.422094</td>\n",
              "      <td>0.462810</td>\n",
              "      <td>-0.179357</td>\n",
              "      <td>0.137859</td>\n",
              "      <td>0.090172</td>\n",
              "      <td>-0.250406</td>\n",
              "      <td>-0.397136</td>\n",
              "      <td>-0.452317</td>\n",
              "      <td>0.475543</td>\n",
              "      <td>0.311070</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013122</td>\n",
              "      <td>-0.032088</td>\n",
              "      <td>-0.061312</td>\n",
              "      <td>-0.035500</td>\n",
              "      <td>0.291356</td>\n",
              "      <td>0.018973</td>\n",
              "      <td>0.276936</td>\n",
              "      <td>0.118045</td>\n",
              "      <td>0.124539</td>\n",
              "      <td>0.512917</td>\n",
              "      <td>-0.065020</td>\n",
              "      <td>-0.072198</td>\n",
              "      <td>0.385163</td>\n",
              "      <td>-0.424894</td>\n",
              "      <td>-0.166537</td>\n",
              "      <td>-0.268705</td>\n",
              "      <td>-0.124530</td>\n",
              "      <td>0.401905</td>\n",
              "      <td>-0.409392</td>\n",
              "      <td>-0.242874</td>\n",
              "      <td>-0.032991</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.120145</td>\n",
              "      <td>0.276607</td>\n",
              "      <td>0.196018</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>-0.170626</td>\n",
              "      <td>0.252692</td>\n",
              "      <td>-0.268878</td>\n",
              "      <td>-0.217491</td>\n",
              "      <td>0.147395</td>\n",
              "      <td>-0.572655</td>\n",
              "      <td>0.380876</td>\n",
              "      <td>0.289344</td>\n",
              "      <td>-0.043541</td>\n",
              "      <td>0.157844</td>\n",
              "      <td>-0.332122</td>\n",
              "      <td>0.148886</td>\n",
              "      <td>-0.010367</td>\n",
              "      <td>0.202192</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.206844</td>\n",
              "      <td>-0.081264</td>\n",
              "      <td>-0.471057</td>\n",
              "      <td>0.140375</td>\n",
              "      <td>-0.063057</td>\n",
              "      <td>-0.002115</td>\n",
              "      <td>0.073763</td>\n",
              "      <td>-0.474488</td>\n",
              "      <td>0.242383</td>\n",
              "      <td>0.175105</td>\n",
              "      <td>0.275230</td>\n",
              "      <td>0.044017</td>\n",
              "      <td>-0.205368</td>\n",
              "      <td>0.058384</td>\n",
              "      <td>0.172249</td>\n",
              "      <td>-0.102876</td>\n",
              "      <td>-0.189868</td>\n",
              "      <td>-0.261747</td>\n",
              "      <td>0.148610</td>\n",
              "      <td>-0.112971</td>\n",
              "      <td>0.131911</td>\n",
              "      <td>0.155130</td>\n",
              "      <td>-0.222576</td>\n",
              "      <td>0.047662</td>\n",
              "      <td>0.265184</td>\n",
              "      <td>-0.273562</td>\n",
              "      <td>-0.271861</td>\n",
              "      <td>-0.124783</td>\n",
              "      <td>0.192386</td>\n",
              "      <td>-0.365533</td>\n",
              "      <td>0.443806</td>\n",
              "      <td>-0.239396</td>\n",
              "      <td>0.134510</td>\n",
              "      <td>0.148306</td>\n",
              "      <td>-0.162905</td>\n",
              "      <td>-0.408268</td>\n",
              "      <td>-0.452173</td>\n",
              "      <td>0.399745</td>\n",
              "      <td>0.218130</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.016609</td>\n",
              "      <td>0.016900</td>\n",
              "      <td>-0.118644</td>\n",
              "      <td>0.151768</td>\n",
              "      <td>0.130500</td>\n",
              "      <td>0.054971</td>\n",
              "      <td>0.138735</td>\n",
              "      <td>0.053953</td>\n",
              "      <td>-0.005048</td>\n",
              "      <td>0.306748</td>\n",
              "      <td>-0.026827</td>\n",
              "      <td>-0.010944</td>\n",
              "      <td>0.375674</td>\n",
              "      <td>-0.363020</td>\n",
              "      <td>-0.183340</td>\n",
              "      <td>-0.268040</td>\n",
              "      <td>-0.013342</td>\n",
              "      <td>0.314043</td>\n",
              "      <td>-0.242718</td>\n",
              "      <td>-0.145447</td>\n",
              "      <td>-0.065175</td>\n",
              "      <td>0.385084</td>\n",
              "      <td>0.072946</td>\n",
              "      <td>0.176400</td>\n",
              "      <td>0.139250</td>\n",
              "      <td>-0.053828</td>\n",
              "      <td>-0.077631</td>\n",
              "      <td>-0.066355</td>\n",
              "      <td>-0.285096</td>\n",
              "      <td>-0.202286</td>\n",
              "      <td>0.174235</td>\n",
              "      <td>-0.427592</td>\n",
              "      <td>0.409958</td>\n",
              "      <td>0.296559</td>\n",
              "      <td>-0.063159</td>\n",
              "      <td>0.065870</td>\n",
              "      <td>-0.251822</td>\n",
              "      <td>0.169947</td>\n",
              "      <td>0.040708</td>\n",
              "      <td>-0.031718</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.163135</td>\n",
              "      <td>0.032812</td>\n",
              "      <td>-0.317298</td>\n",
              "      <td>0.077077</td>\n",
              "      <td>-0.046047</td>\n",
              "      <td>0.083449</td>\n",
              "      <td>0.138843</td>\n",
              "      <td>-0.258913</td>\n",
              "      <td>0.212825</td>\n",
              "      <td>0.177699</td>\n",
              "      <td>0.225979</td>\n",
              "      <td>0.085439</td>\n",
              "      <td>-0.179993</td>\n",
              "      <td>0.179730</td>\n",
              "      <td>0.193651</td>\n",
              "      <td>-0.002948</td>\n",
              "      <td>-0.235174</td>\n",
              "      <td>-0.196216</td>\n",
              "      <td>0.147436</td>\n",
              "      <td>-0.022483</td>\n",
              "      <td>0.047708</td>\n",
              "      <td>0.157341</td>\n",
              "      <td>-0.332480</td>\n",
              "      <td>0.177485</td>\n",
              "      <td>0.173061</td>\n",
              "      <td>-0.162505</td>\n",
              "      <td>-0.127359</td>\n",
              "      <td>-0.124054</td>\n",
              "      <td>0.199129</td>\n",
              "      <td>-0.377687</td>\n",
              "      <td>0.416810</td>\n",
              "      <td>-0.197260</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.079870</td>\n",
              "      <td>-0.242736</td>\n",
              "      <td>-0.338641</td>\n",
              "      <td>-0.449158</td>\n",
              "      <td>0.446382</td>\n",
              "      <td>0.228753</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.021652</td>\n",
              "      <td>-0.093424</td>\n",
              "      <td>0.013075</td>\n",
              "      <td>-0.053378</td>\n",
              "      <td>0.294928</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>0.236357</td>\n",
              "      <td>0.084905</td>\n",
              "      <td>0.117252</td>\n",
              "      <td>0.482831</td>\n",
              "      <td>-0.069400</td>\n",
              "      <td>-0.032449</td>\n",
              "      <td>0.322154</td>\n",
              "      <td>-0.478949</td>\n",
              "      <td>-0.071717</td>\n",
              "      <td>-0.232559</td>\n",
              "      <td>-0.124388</td>\n",
              "      <td>0.374382</td>\n",
              "      <td>-0.439318</td>\n",
              "      <td>-0.263468</td>\n",
              "      <td>-0.069116</td>\n",
              "      <td>0.298439</td>\n",
              "      <td>0.155038</td>\n",
              "      <td>0.304691</td>\n",
              "      <td>0.136873</td>\n",
              "      <td>-0.022667</td>\n",
              "      <td>-0.177196</td>\n",
              "      <td>0.317893</td>\n",
              "      <td>-0.207849</td>\n",
              "      <td>-0.181367</td>\n",
              "      <td>0.173520</td>\n",
              "      <td>-0.518931</td>\n",
              "      <td>0.331305</td>\n",
              "      <td>0.225216</td>\n",
              "      <td>-0.066281</td>\n",
              "      <td>0.177772</td>\n",
              "      <td>-0.315808</td>\n",
              "      <td>0.177454</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.233071</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.150479</td>\n",
              "      <td>-0.124831</td>\n",
              "      <td>-0.476495</td>\n",
              "      <td>0.111134</td>\n",
              "      <td>-0.031006</td>\n",
              "      <td>-0.041041</td>\n",
              "      <td>0.031492</td>\n",
              "      <td>-0.421679</td>\n",
              "      <td>0.210084</td>\n",
              "      <td>0.142972</td>\n",
              "      <td>0.189210</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>-0.189697</td>\n",
              "      <td>0.004462</td>\n",
              "      <td>0.153993</td>\n",
              "      <td>-0.127870</td>\n",
              "      <td>-0.225679</td>\n",
              "      <td>-0.228893</td>\n",
              "      <td>0.142717</td>\n",
              "      <td>-0.144081</td>\n",
              "      <td>0.102233</td>\n",
              "      <td>0.137043</td>\n",
              "      <td>-0.141119</td>\n",
              "      <td>0.010715</td>\n",
              "      <td>0.248722</td>\n",
              "      <td>-0.268513</td>\n",
              "      <td>-0.282812</td>\n",
              "      <td>-0.024337</td>\n",
              "      <td>0.152969</td>\n",
              "      <td>-0.282796</td>\n",
              "      <td>0.388519</td>\n",
              "      <td>-0.183850</td>\n",
              "      <td>0.118803</td>\n",
              "      <td>0.154545</td>\n",
              "      <td>-0.108651</td>\n",
              "      <td>-0.363321</td>\n",
              "      <td>-0.379276</td>\n",
              "      <td>0.303758</td>\n",
              "      <td>0.222120</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5532</th>\n",
              "      <td>0.042773</td>\n",
              "      <td>-0.033910</td>\n",
              "      <td>-0.019512</td>\n",
              "      <td>0.124129</td>\n",
              "      <td>0.270523</td>\n",
              "      <td>0.022117</td>\n",
              "      <td>0.205680</td>\n",
              "      <td>0.063389</td>\n",
              "      <td>0.047388</td>\n",
              "      <td>0.454888</td>\n",
              "      <td>-0.067632</td>\n",
              "      <td>-0.039229</td>\n",
              "      <td>0.396700</td>\n",
              "      <td>-0.481904</td>\n",
              "      <td>-0.141814</td>\n",
              "      <td>-0.294432</td>\n",
              "      <td>-0.090833</td>\n",
              "      <td>0.345053</td>\n",
              "      <td>-0.436716</td>\n",
              "      <td>-0.252580</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>0.445133</td>\n",
              "      <td>0.143825</td>\n",
              "      <td>0.276805</td>\n",
              "      <td>0.149618</td>\n",
              "      <td>-0.035198</td>\n",
              "      <td>-0.153910</td>\n",
              "      <td>0.123092</td>\n",
              "      <td>-0.301102</td>\n",
              "      <td>-0.276662</td>\n",
              "      <td>0.175708</td>\n",
              "      <td>-0.524604</td>\n",
              "      <td>0.405777</td>\n",
              "      <td>0.281886</td>\n",
              "      <td>-0.101340</td>\n",
              "      <td>0.169330</td>\n",
              "      <td>-0.332386</td>\n",
              "      <td>0.234019</td>\n",
              "      <td>0.049256</td>\n",
              "      <td>0.173430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.146826</td>\n",
              "      <td>-0.090468</td>\n",
              "      <td>-0.432032</td>\n",
              "      <td>0.072156</td>\n",
              "      <td>-0.008402</td>\n",
              "      <td>-0.015855</td>\n",
              "      <td>0.076635</td>\n",
              "      <td>-0.383907</td>\n",
              "      <td>0.216137</td>\n",
              "      <td>0.112590</td>\n",
              "      <td>0.259788</td>\n",
              "      <td>0.061561</td>\n",
              "      <td>-0.218172</td>\n",
              "      <td>0.065737</td>\n",
              "      <td>0.182227</td>\n",
              "      <td>-0.115643</td>\n",
              "      <td>-0.303886</td>\n",
              "      <td>-0.238156</td>\n",
              "      <td>0.143345</td>\n",
              "      <td>-0.066513</td>\n",
              "      <td>0.058191</td>\n",
              "      <td>0.178420</td>\n",
              "      <td>-0.243007</td>\n",
              "      <td>0.099867</td>\n",
              "      <td>0.233152</td>\n",
              "      <td>-0.273105</td>\n",
              "      <td>-0.217491</td>\n",
              "      <td>-0.089177</td>\n",
              "      <td>0.168070</td>\n",
              "      <td>-0.376143</td>\n",
              "      <td>0.458129</td>\n",
              "      <td>-0.175970</td>\n",
              "      <td>0.122528</td>\n",
              "      <td>0.113854</td>\n",
              "      <td>-0.166427</td>\n",
              "      <td>-0.404936</td>\n",
              "      <td>-0.394547</td>\n",
              "      <td>0.427159</td>\n",
              "      <td>0.274950</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5533</th>\n",
              "      <td>0.013893</td>\n",
              "      <td>-0.076324</td>\n",
              "      <td>-0.026763</td>\n",
              "      <td>-0.111633</td>\n",
              "      <td>0.273897</td>\n",
              "      <td>0.023860</td>\n",
              "      <td>0.247044</td>\n",
              "      <td>0.085839</td>\n",
              "      <td>0.136947</td>\n",
              "      <td>0.443313</td>\n",
              "      <td>-0.085441</td>\n",
              "      <td>-0.022686</td>\n",
              "      <td>0.298942</td>\n",
              "      <td>-0.400823</td>\n",
              "      <td>-0.077466</td>\n",
              "      <td>-0.236035</td>\n",
              "      <td>-0.127532</td>\n",
              "      <td>0.366725</td>\n",
              "      <td>-0.374236</td>\n",
              "      <td>-0.237173</td>\n",
              "      <td>-0.029510</td>\n",
              "      <td>0.221616</td>\n",
              "      <td>0.133812</td>\n",
              "      <td>0.266094</td>\n",
              "      <td>0.119620</td>\n",
              "      <td>-0.048346</td>\n",
              "      <td>-0.147803</td>\n",
              "      <td>0.346367</td>\n",
              "      <td>-0.206448</td>\n",
              "      <td>-0.166556</td>\n",
              "      <td>0.132702</td>\n",
              "      <td>-0.492453</td>\n",
              "      <td>0.270371</td>\n",
              "      <td>0.211515</td>\n",
              "      <td>-0.051666</td>\n",
              "      <td>0.139050</td>\n",
              "      <td>-0.271715</td>\n",
              "      <td>0.115399</td>\n",
              "      <td>-0.006158</td>\n",
              "      <td>0.229068</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.169659</td>\n",
              "      <td>-0.114769</td>\n",
              "      <td>-0.455857</td>\n",
              "      <td>0.135100</td>\n",
              "      <td>-0.057368</td>\n",
              "      <td>-0.046160</td>\n",
              "      <td>0.042890</td>\n",
              "      <td>-0.419948</td>\n",
              "      <td>0.217905</td>\n",
              "      <td>0.140417</td>\n",
              "      <td>0.195439</td>\n",
              "      <td>0.031060</td>\n",
              "      <td>-0.170694</td>\n",
              "      <td>-0.006172</td>\n",
              "      <td>0.120757</td>\n",
              "      <td>-0.142441</td>\n",
              "      <td>-0.153915</td>\n",
              "      <td>-0.194800</td>\n",
              "      <td>0.133955</td>\n",
              "      <td>-0.125504</td>\n",
              "      <td>0.130532</td>\n",
              "      <td>0.100192</td>\n",
              "      <td>-0.137326</td>\n",
              "      <td>0.013513</td>\n",
              "      <td>0.261847</td>\n",
              "      <td>-0.263738</td>\n",
              "      <td>-0.252756</td>\n",
              "      <td>-0.054820</td>\n",
              "      <td>0.144323</td>\n",
              "      <td>-0.251898</td>\n",
              "      <td>0.349563</td>\n",
              "      <td>-0.213699</td>\n",
              "      <td>0.131564</td>\n",
              "      <td>0.142152</td>\n",
              "      <td>-0.094559</td>\n",
              "      <td>-0.352258</td>\n",
              "      <td>-0.344519</td>\n",
              "      <td>0.285948</td>\n",
              "      <td>0.170679</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5534</th>\n",
              "      <td>0.019444</td>\n",
              "      <td>-0.123006</td>\n",
              "      <td>0.013086</td>\n",
              "      <td>0.020298</td>\n",
              "      <td>0.186744</td>\n",
              "      <td>-0.007693</td>\n",
              "      <td>0.199454</td>\n",
              "      <td>0.079005</td>\n",
              "      <td>0.084409</td>\n",
              "      <td>0.336043</td>\n",
              "      <td>-0.071559</td>\n",
              "      <td>-0.033479</td>\n",
              "      <td>0.307649</td>\n",
              "      <td>-0.364952</td>\n",
              "      <td>-0.067122</td>\n",
              "      <td>-0.208200</td>\n",
              "      <td>-0.072977</td>\n",
              "      <td>0.314538</td>\n",
              "      <td>-0.310495</td>\n",
              "      <td>-0.180362</td>\n",
              "      <td>-0.051056</td>\n",
              "      <td>0.264249</td>\n",
              "      <td>0.102505</td>\n",
              "      <td>0.227511</td>\n",
              "      <td>0.095517</td>\n",
              "      <td>-0.012313</td>\n",
              "      <td>-0.130748</td>\n",
              "      <td>0.199181</td>\n",
              "      <td>-0.223900</td>\n",
              "      <td>-0.107914</td>\n",
              "      <td>0.148378</td>\n",
              "      <td>-0.372694</td>\n",
              "      <td>0.240089</td>\n",
              "      <td>0.239687</td>\n",
              "      <td>-0.060801</td>\n",
              "      <td>0.137069</td>\n",
              "      <td>-0.278501</td>\n",
              "      <td>0.143130</td>\n",
              "      <td>-0.028939</td>\n",
              "      <td>0.128117</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.138774</td>\n",
              "      <td>-0.073579</td>\n",
              "      <td>-0.363259</td>\n",
              "      <td>0.068012</td>\n",
              "      <td>-0.031999</td>\n",
              "      <td>-0.028014</td>\n",
              "      <td>0.052771</td>\n",
              "      <td>-0.348346</td>\n",
              "      <td>0.166279</td>\n",
              "      <td>0.146284</td>\n",
              "      <td>0.174753</td>\n",
              "      <td>-0.007557</td>\n",
              "      <td>-0.197361</td>\n",
              "      <td>0.020451</td>\n",
              "      <td>0.135191</td>\n",
              "      <td>-0.046636</td>\n",
              "      <td>-0.189201</td>\n",
              "      <td>-0.163896</td>\n",
              "      <td>0.115950</td>\n",
              "      <td>-0.088648</td>\n",
              "      <td>0.078650</td>\n",
              "      <td>0.090723</td>\n",
              "      <td>-0.114424</td>\n",
              "      <td>0.021235</td>\n",
              "      <td>0.216427</td>\n",
              "      <td>-0.202674</td>\n",
              "      <td>-0.253422</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.133447</td>\n",
              "      <td>-0.223294</td>\n",
              "      <td>0.301519</td>\n",
              "      <td>-0.138319</td>\n",
              "      <td>0.103003</td>\n",
              "      <td>0.114464</td>\n",
              "      <td>-0.116057</td>\n",
              "      <td>-0.298493</td>\n",
              "      <td>-0.312497</td>\n",
              "      <td>0.258981</td>\n",
              "      <td>0.213762</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5535</th>\n",
              "      <td>0.027485</td>\n",
              "      <td>-0.096662</td>\n",
              "      <td>-0.022338</td>\n",
              "      <td>0.041827</td>\n",
              "      <td>0.201357</td>\n",
              "      <td>0.013700</td>\n",
              "      <td>0.206378</td>\n",
              "      <td>0.068648</td>\n",
              "      <td>0.075013</td>\n",
              "      <td>0.365812</td>\n",
              "      <td>-0.061896</td>\n",
              "      <td>-0.036581</td>\n",
              "      <td>0.335613</td>\n",
              "      <td>-0.393043</td>\n",
              "      <td>-0.096819</td>\n",
              "      <td>-0.238215</td>\n",
              "      <td>-0.067163</td>\n",
              "      <td>0.337011</td>\n",
              "      <td>-0.326831</td>\n",
              "      <td>-0.192409</td>\n",
              "      <td>-0.063799</td>\n",
              "      <td>0.309077</td>\n",
              "      <td>0.109091</td>\n",
              "      <td>0.240717</td>\n",
              "      <td>0.121904</td>\n",
              "      <td>-0.020351</td>\n",
              "      <td>-0.123188</td>\n",
              "      <td>0.151447</td>\n",
              "      <td>-0.235924</td>\n",
              "      <td>-0.149663</td>\n",
              "      <td>0.164830</td>\n",
              "      <td>-0.424557</td>\n",
              "      <td>0.322382</td>\n",
              "      <td>0.259922</td>\n",
              "      <td>-0.073411</td>\n",
              "      <td>0.138122</td>\n",
              "      <td>-0.284431</td>\n",
              "      <td>0.159832</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>0.109156</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.155459</td>\n",
              "      <td>-0.066404</td>\n",
              "      <td>-0.390222</td>\n",
              "      <td>0.086765</td>\n",
              "      <td>-0.041641</td>\n",
              "      <td>-0.001746</td>\n",
              "      <td>0.080250</td>\n",
              "      <td>-0.348324</td>\n",
              "      <td>0.202490</td>\n",
              "      <td>0.167994</td>\n",
              "      <td>0.198196</td>\n",
              "      <td>0.021989</td>\n",
              "      <td>-0.201767</td>\n",
              "      <td>0.046117</td>\n",
              "      <td>0.163671</td>\n",
              "      <td>-0.041646</td>\n",
              "      <td>-0.223425</td>\n",
              "      <td>-0.189196</td>\n",
              "      <td>0.133569</td>\n",
              "      <td>-0.088239</td>\n",
              "      <td>0.084470</td>\n",
              "      <td>0.125058</td>\n",
              "      <td>-0.178939</td>\n",
              "      <td>0.059720</td>\n",
              "      <td>0.226439</td>\n",
              "      <td>-0.224004</td>\n",
              "      <td>-0.227356</td>\n",
              "      <td>-0.037542</td>\n",
              "      <td>0.164866</td>\n",
              "      <td>-0.278810</td>\n",
              "      <td>0.366966</td>\n",
              "      <td>-0.154087</td>\n",
              "      <td>0.101483</td>\n",
              "      <td>0.119828</td>\n",
              "      <td>-0.155908</td>\n",
              "      <td>-0.330470</td>\n",
              "      <td>-0.385494</td>\n",
              "      <td>0.318774</td>\n",
              "      <td>0.231287</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5536</th>\n",
              "      <td>0.035878</td>\n",
              "      <td>-0.099399</td>\n",
              "      <td>-0.024144</td>\n",
              "      <td>0.093658</td>\n",
              "      <td>0.209708</td>\n",
              "      <td>0.027722</td>\n",
              "      <td>0.219527</td>\n",
              "      <td>0.074445</td>\n",
              "      <td>0.062642</td>\n",
              "      <td>0.389698</td>\n",
              "      <td>-0.074708</td>\n",
              "      <td>-0.028839</td>\n",
              "      <td>0.386219</td>\n",
              "      <td>-0.454158</td>\n",
              "      <td>-0.113080</td>\n",
              "      <td>-0.287527</td>\n",
              "      <td>-0.043544</td>\n",
              "      <td>0.379921</td>\n",
              "      <td>-0.358513</td>\n",
              "      <td>-0.208380</td>\n",
              "      <td>-0.077489</td>\n",
              "      <td>0.385313</td>\n",
              "      <td>0.114705</td>\n",
              "      <td>0.254107</td>\n",
              "      <td>0.139920</td>\n",
              "      <td>-0.009485</td>\n",
              "      <td>-0.135276</td>\n",
              "      <td>0.111385</td>\n",
              "      <td>-0.305001</td>\n",
              "      <td>-0.163284</td>\n",
              "      <td>0.196201</td>\n",
              "      <td>-0.471589</td>\n",
              "      <td>0.383558</td>\n",
              "      <td>0.308253</td>\n",
              "      <td>-0.083044</td>\n",
              "      <td>0.150749</td>\n",
              "      <td>-0.315473</td>\n",
              "      <td>0.192584</td>\n",
              "      <td>0.011947</td>\n",
              "      <td>0.086382</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.182453</td>\n",
              "      <td>-0.059387</td>\n",
              "      <td>-0.415509</td>\n",
              "      <td>0.071264</td>\n",
              "      <td>-0.042320</td>\n",
              "      <td>0.014003</td>\n",
              "      <td>0.115031</td>\n",
              "      <td>-0.373614</td>\n",
              "      <td>0.203116</td>\n",
              "      <td>0.204437</td>\n",
              "      <td>0.235795</td>\n",
              "      <td>0.017457</td>\n",
              "      <td>-0.242464</td>\n",
              "      <td>0.073866</td>\n",
              "      <td>0.202333</td>\n",
              "      <td>-0.042044</td>\n",
              "      <td>-0.268096</td>\n",
              "      <td>-0.197623</td>\n",
              "      <td>0.148564</td>\n",
              "      <td>-0.096150</td>\n",
              "      <td>0.076318</td>\n",
              "      <td>0.145729</td>\n",
              "      <td>-0.228241</td>\n",
              "      <td>0.089626</td>\n",
              "      <td>0.231556</td>\n",
              "      <td>-0.241980</td>\n",
              "      <td>-0.240403</td>\n",
              "      <td>-0.044367</td>\n",
              "      <td>0.214566</td>\n",
              "      <td>-0.327487</td>\n",
              "      <td>0.423870</td>\n",
              "      <td>-0.185832</td>\n",
              "      <td>0.114524</td>\n",
              "      <td>0.127587</td>\n",
              "      <td>-0.208522</td>\n",
              "      <td>-0.366741</td>\n",
              "      <td>-0.443395</td>\n",
              "      <td>0.398762</td>\n",
              "      <td>0.271234</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5537 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...        98        99  Labels\n",
              "0     0.038422 -0.054940 -0.026879  ...  0.418054  0.274729     Yes\n",
              "1     0.041158 -0.022320 -0.000589  ...  0.475543  0.311070     Yes\n",
              "2     0.013122 -0.032088 -0.061312  ...  0.399745  0.218130     Yes\n",
              "3     0.016609  0.016900 -0.118644  ...  0.446382  0.228753     Yes\n",
              "4     0.021652 -0.093424  0.013075  ...  0.303758  0.222120     Yes\n",
              "...        ...       ...       ...  ...       ...       ...     ...\n",
              "5532  0.042773 -0.033910 -0.019512  ...  0.427159  0.274950     NaN\n",
              "5533  0.013893 -0.076324 -0.026763  ...  0.285948  0.170679      No\n",
              "5534  0.019444 -0.123006  0.013086  ...  0.258981  0.213762      No\n",
              "5535  0.027485 -0.096662 -0.022338  ...  0.318774  0.231287      No\n",
              "5536  0.035878 -0.099399 -0.024144  ...  0.398762  0.271234     NaN\n",
              "\n",
              "[5537 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0WyL8xtoV4v",
        "colab_type": "text"
      },
      "source": [
        "### I.d Fast2vec pre-trained representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjqhU6eyoV4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text=import_clean_text()\n",
        "\n",
        "def load_vectors(fname, amount):\n",
        "    fin = io.open(fname, 'r', encoding ='utf-8', newline = '\\n', errors = 'ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    i = 0\n",
        "    for line in fin:\n",
        "        i += 1 \n",
        "        print(\"Done : \", i/amount * 100, \"%\")\n",
        "        if i < amount:\n",
        "            tokens = line.rstrip().split(' ')\n",
        "            data[tokens[0]] = tokens[1:]\n",
        "        else:\n",
        "            break\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JNStdwroV4y",
        "colab_type": "code",
        "outputId": "6d639eb5-58c6-4eef-a688-89c08753af0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "fastext = load_vectors('wiki-news-300d-1M.vec', 500000)\n",
        "\n",
        "\n",
        "clean_text=clean_text_first(df.Tweet)\n",
        "\n",
        "dic_words = {}\n",
        "unknown_list = []\n",
        "\n",
        "unknown = 0\n",
        "count_word = 0\n",
        "\n",
        "for tweet in clean_text:\n",
        "    for word in tweet:\n",
        "        count_word += 1\n",
        "        if word in fastext.keys():\n",
        "            dic_words[word] = fastext[word]\n",
        "        else:\n",
        "            unknown += 1\n",
        "            unknown_list.append(word)\n",
        "\n",
        "marshal.dump(dic_words, open('sample_data/Projet_NLP/dic_words.dat', 'wb'))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfastext = load_vectors('wiki-news-300d-1M.vec', 500000)\\n\\n\\nclean_text=clean_text_first(df.Tweet)\\n\\ndic_words = {}\\nunknown_list = []\\n\\nunknown = 0\\ncount_word = 0\\n\\nfor tweet in clean_text:\\n    for word in tweet:\\n        count_word += 1\\n        if word in fastext.keys():\\n            dic_words[word] = fastext[word]\\n        else:\\n            unknown += 1\\n            unknown_list.append(word)\\n\\nmarshal.dump(dic_words, open('sample_data/Projet_NLP/dic_words.dat', 'wb'))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRJMXR8HoV41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inf = open('sample_data/Projet_NLP/dic_words.dat', 'rb')\n",
        "dic_words = marshal.load(inf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcS1xLkoV45",
        "colab_type": "text"
      },
      "source": [
        "##### With clustering on the Fast2vec output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbApBqz5oV45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_ft = pd.DataFrame(data=dic_words)\n",
        "dataset_ft = dataset_ft.T\n",
        "clustering = AgglomerativeClustering(200).fit(dataset_ft)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GrlZ0csoV48",
        "colab_type": "code",
        "outputId": "6285c92f-fc0f-46d1-da1f-6ba482ad7dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for j in range(200):\n",
        "    names = []\n",
        "    for k, cluster in enumerate(clustering.labels_):\n",
        "        if cluster == j:\n",
        "            names.append(dataset_ft.index[k])\n",
        "    print(\"Le Cluster numéro : \", j, \"contient \", len(names), \" mots\")\n",
        "    print(\" \")\n",
        "    print(\" \")\n",
        "    print(names)\n",
        "    print(\" \")\n",
        "    print(\" \")\n",
        "    print(\"################\")\n",
        "    print(\" \")\n",
        "    print(\" \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Le Cluster numéro :  0 contient  40  mots\n",
            " \n",
            " \n",
            "['talk', 'admin', 'data', 'article', 'pic', 'environment', 'photo', 'link', 'overview', 'lede', 'user', 'cn', 'slideshow', 'ip', 'image', 'material', 'content', 'poster', 'tag', 'info', 'collage', 'page', 'bio', 'brochure', 'summary', 'nutshell', 'information', 'cleanup', 'text', 'picture', 'msg', 'section', 'enviroment', 'click', 'caption', 'blurb', 'detail', 'bios', 'photograph', 'intro']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  1 contient  52  mots\n",
            " \n",
            " \n",
            "['angus', 'harry', 'ken', 'heather', 'mont', 'wor', 'dy', 'bobby', 'redd', 'abu', 'ira', 'ny', 'marc', 'dan', 'sh', 'ker', 'ted', 'turner', 'wy', 'wi', 'noel', 'doe', 'sally', 'boer', 'teamster', 'ed', 'holt', 'no1', 'drudge', 'wh', 'stan', 'ty', 'smith', 'ford', 'cud', 'batman', 'beck', 'penn', 'glen', 'twain', 'miller', 'ck', 'ing', 'lorraine', 'ned', 'mak', 'porter', 'colin', 'ling', 'bec', 'dana', 'whi']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  2 contient  32  mots\n",
            " \n",
            " \n",
            "['season', 'toon', 'tale', 'film', 'animation', 'remake', 'trek', 'voyage', 'journey', 'epic', 'story', 'cartoon', 'adventure', 'character', 'adventurer', 'song', 'narrative', 'hero', 'odyssey', 'expedition', 'episode', 'plot', 'chronicle', 'flashback', 'trip', 'sequel', 'cartoonist', 'movie', 'novel', 'album', 'homer', 'explorer']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  3 contient  74  mots\n",
            " \n",
            " \n",
            "['government', 'county', 'community', 'group', 'fellow', 'student', 'city', 'administration', 'frontier', 'official', 'state', 'professor', 'league', 'team', 'society', 'member', 'institute', 'college', 'staff', 'council', 'province', 'locality', 'prof', 'university', 'alumnus', 'teacher', 'school', 'emirate', 'town', 'senior', 'capitol', 'instruction', 'game', 'veteran', 'center', 'club', 'parent', 'federal', 'participation', 'capital', 'area', 'field', 'union', 'separation', 'elite', 'resident', 'high-school', 'dean', 'organization', 'network', 'education', 'representative', 'division', 'homeland', 'unit', 'volunteer', 'region', 'metro', 'institution', 'centre', 'beneficiary', 'hotbed', 'statewide', 'frontline', 'association', 'campus', 'graduate', 'participant', 'educational', 'academy', 'municipal', 'emeritus', 'heartland', 'hub']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  4 contient  69  mots\n",
            " \n",
            " \n",
            "['natural', 'life', 'miracle', 'mind', 'real', 'nature', 'limbo', 'grand', 'hapless', 'live', 'heart', 'namesake', 'death', 'beauty', 'counterpart', 'wild', 'living', 'runaway', 'gift', 'destiny', 'dead', 'giant', 'monster', 'imaginary', 'soul', 'identity', 'twin', 'body', 'name', 'supreme', 'creator', 'homegrown', 'actual', 'head', 'creation', 'reality', 'nick', 'lone', 'hybrid', 'die', 'existence', 'fantasy', 'politic', 'unreal', 'ghost', 'grace', 'phantom', 'grave', 'mystery', 'brain', 'pre-eminently', 'nightmare', 'master', 'disappearance', 'non-reality', 'allure', 'brilliance', 'ultimate', 'everyday', 'pesky', 'genius', 'predecessor', 'toll', 'big-time', 'epitome', 'dream', 'paradise', 'conscience', 'title']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  5 contient  33  mots\n",
            " \n",
            " \n",
            "['bell', 'bomb', 'early-warning', 'plane', 'explosive', 'satellite', 'weapon', 'gun', 'ring', 'ballistic', 'missile', 'trainer', 'boxing', 'train', 'contrail', 'pilot', 'timer', 'blast', 'clock', 'fly-by', 'ammo', 'bout', 'training', 'aviation', 'flight', 'sabre', 'cannon', 'dagger', 'knife', 'clockwork', 'explosion', 'airport', 'barrel']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  6 contient  27  mots\n",
            " \n",
            " \n",
            "['fiction', 'tradition', 'culture', 'politics', 'economy', 'capitalism', 'filmmaker', 'economics', 'history', 'socialist', 'libertarian', 'documentary', 'realism', 'journalism', 'literature', 'philosophy', 'bestseller', 'nonfiction', 'medicine', 'realist', 'journalistic', 'liberalism', 'marxism', 'romanticism', 'psychology', 'photojournalism', 'documentarian']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  7 contient  39  mots\n",
            " \n",
            " \n",
            "['usa', 'american', 'europe', 'japan', 'canadian', 'asian', 'european', 'china', 'america', 'sydney', 'asia', 'italy', 'olympic', 'uk', 'canada', 'australian', 'england', 'germany', 'armenia', 'spain', 'chinese', 'mexico', 'german', 'russia', 'vancouver', 'ottawa', 'olympics', 'montreal', 'australia', 'british', 'melbourne', 'portugal', 'bulgaria', 'taiwan', 'japanese', 'greece', 'poland', 'brit', 'britain']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  8 contient  20  mots\n",
            " \n",
            " \n",
            "['meat', 'agriculture', 'farmer', 'pastoralists', 'pastoralist', 'rural', 'cow', 'sheep', 'slaughter', 'milk', 'ag', 'goat', 'grazing', 'farm', 'dairy', 'hog', 'livestock', 'beef', 'bull', 'agricultural']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  9 contient  48  mots\n",
            " \n",
            " \n",
            "['wheat', 'rice', 'peanut', 'plant', 'leaf', 'cherry', 'blossom', 'bloom', 'ash', 'tree', 'tulip', 'crop', 'greenhouse', 'flower', 'tea', 'hydroponics', 'weaver', 'root', 'vegetable', 'blooming', 'bush', 'seed', 'tomato', 'monkey', 'chocolate', 'maple', 'syrup', 'bud', 'onion', 'coffee', 'rat', 'bonsai', 'wood', 'prairie', 'grass', 'acorn', 'quilt', 'carrot', 'hothouse', 'olive', 'nit', 'banana', 'oak', 'guinea', 'nursery', 'candy', 'loom', 'palm']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  10 contient  26  mots\n",
            " \n",
            " \n",
            "['will', 'could', 'must', 'may', 'might', 'wont', 'should', 'wouldnt', 'don', 'haven', 'can', 'doesn', 'would', 'cannot', 'isn', 'dont', 'doesnt', 'gotta', 'wouldn', 'cant', 'lemme', 'gonna', 'wil', 'shall', 'didnt', 'wld']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  11 contient  22  mots\n",
            " \n",
            " \n",
            "['migratory', 'migration', 'immigration', 'dispersal', 'relocation', 'long-distance', 'transportation', 'traffic', 'transport', 'trucking', 'immigrant', 'freight', 'refugee', 'congestion', 'humanitarian', 'transit', 'airlift', 'long-haul', 'short-haul', 'influx', 'evacuation', 'gridlock']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  12 contient  30  mots\n",
            " \n",
            " \n",
            "['digg', 'thoreau', 'time.com', 'wired.com', 'examiner.com', 'npr', 'nytimes.com', 'alt', 'gawker', 'stormfront', 'usatoday.com', 'guardian.co.uk', 'webby', 'artikel', 'orkut', 'msnbc.com', 'hannity', 'cbc.ca', 'drupal', 'oped', 'latimes.com', 'forbes.com', 'dailykos', 'blip.tv', 'dramatica', 'allafrica.com', 'wikipedia', 'telegraph.co.uk', 'diff', 'encyclopedia']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  13 contient  108  mots\n",
            " \n",
            " \n",
            "['a', 'those', 'most', 'our', 'all', 'new', 'preliminary', 'little', 'an', 'any', 'no', 'my', 'main', 'next', 'these', 'one', 'their', 'vary', 'present', 'basic', 'third', 'many', 'general', 'certain', 'future', 'whole', 'another', 'familiar', 'same', 'few', 'past', 'traditional', 'your', 'special', 'first', 'popular', 'other', 'universal', 'major', 'without', 'single', 'entire', 'exclusive', 'such', 'multiple', 'every', 'several', 'recent', 'last', 'current', 'core', 'his', 'earlier', 'central', 'some', 'own', 'principal', 'separate', 'different', 'available', 'long-term', 'favorite', 'standard', 'both', 'various', 'opposite', 'old', 'classic', 'modern', 'chief', 'former', 'final', 'complementary', 'independent', 'exactly', 'abnormal', 'comparable', 'her', 'each', 'fundamental', 'complex', 'additional', 'second', 'upcoming', 'spontaneous', 'secondary', 'obligatory', 'limited', 'unaltered', 'usual', 'previous', 'original', 'normal', 'primary', 'long-standing', 'abundant', 'common', 'correlative', 'congruent', 'varied', 'random', 'specific', 'superior', 'mixed', 'norm', 'roughly', 'so-called', 'unchanging']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  14 contient  46  mots\n",
            " \n",
            " \n",
            "['silly', 'scary', 'off-the-charts', 'ridiculous', 'out-of-touch', 'stupid', 'weird', 'bizarre', 'strange', 'industry-friendly', 'scaled-back', 'outdated', 'hypocritical', 'worthless', 'unclear', 'vague', 'rusty', 'skeptical', 'idiotic', 'freaky', 'dishonest', 'laughable', 'untrustworthy', 'oddball', 'inane', 'sloppy', 'subpar', 'off-the-mark', 'senseless', 'pathetic', 'certifiably', 'sceptical', 'out-of-step', 'crappy', 'incomprehensible', 'offbeat', 'disingenuous', 'revenue-neutral', 'wait-and-see', 'deficient', 'lousy', 'fuzzy', 'shabby', 'unpersuaded', 'inconclusive', 'creepy']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  15 contient  27  mots\n",
            " \n",
            " \n",
            "['explore', 'monitor', 'regulate', 'update', 'fix', 'examine', 'monitoring', 'suspend', 'revise', 'upgrade', 'investigate', 'cancel', 'supervise', 'reassess', 'rewrite', 'alignment', 'reschedule', 'defer', 'tweak', 'align', 'postpone', 'televise', 'recalculate', 'assess', 'determine', 'inspect', 'coordinate']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  16 contient  45  mots\n",
            " \n",
            " \n",
            "['ap', 'ac', 'ca', 'lo', 'mich', 'sen', 'ben', 'dc', 'mit', 'vol', 'iv', 'tr', 'az', 'ar', 'ii', 'fe', 'fab', 'cont', 'em', 'id', 'ab', 'meg', 'bi', 'sm', 'ch', 'xi', 'ob', 'mac', 'dem', 'ter', 'el', 'va', 'van', 'com', 'ir', 'fr', 'trans', 'dow', 'ist', 'fl', 'vi', 'ff', 'vo', 'ea', 'han']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  17 contient  23  mots\n",
            " \n",
            " \n",
            "['confirm', 'refute', 'implicate', 'exonerate', 'downplay', 'discredit', 'ignore', 'absolve', 'disproves', 'contradict', 'verify', 'disprove', 'falsify', 'disbelieve', 'debunk', 'debunks', 'embarrass', 'debunked', 'dismiss', 'vindicate', 'refutes', 'validate', 'bode']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  18 contient  29  mots\n",
            " \n",
            " \n",
            "['home', 'mountain', 'house', 'property', 'resort', 'land', 'oceanfront', 'summit', 'hill', 'family', 'vacation', 'subterranean', 'retreat', 'climb', 'estate', 'cave', 'peak', 'underwater', 'mansion', 'ruin', 'elevation', 'cliff', 'hideaway', 'mire', 'hotel', 'ridge', 'ocean-view', 'villa', 'wreck']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  19 contient  20  mots\n",
            " \n",
            " \n",
            "['accountability', 'subjectivity', 'legitimacy', 'stewardship', 'crony', 'policymaking', 'corrupt', 'credibility', 'bureaucracy', 'politicization', 'credibilty', 'distortion', 'credence', 'crooked', 'corruption', 'manipulation', 'accountable', 'deniability', 'egotism', 'governance']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  20 contient  74  mots\n",
            " \n",
            " \n",
            "['urge', 'fail', 'need', 'believe', 'belive', 'enjoy', 'want', 'argue', 'agree', 'guess', 'accept', 'invitation', 'consider', 'love', 'understand', 'like', 'hate', 'favor', 'feel', 'doubt', 'recommend', 'think', 'advise', 'conclude', 'wonder', 'wish', 'oppose', 'sense', 'dare', 'predict', 'suspect', 'forgot', 'flunk', 'fails', 'suggest', 'assume', 'try', 'favour', 'suppose', 'refuse', 'expect', 'appreciate', 'forget', 'remember', 'forgotten', 'regard', 'invite', 'disagree', 'realise', 'attempt', 'propose', 'admit', 'acknowledge', 'foresee', 'respect', 'contend', 'grasp', 'imagine', 'despise', 'contemplate', 'accepted', 'reject', 'approve', 'concede', 'accepts', 'comprehend', 'insist', 'prefer', 'recall', 'concur', 'tend', 'endorse', 'realize', 'disagreed']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  21 contient  76  mots\n",
            " \n",
            " \n",
            "['auth', 'int', 'ct', 'temp', 'dr', 'pb', 'alpha', 'phi', 'co2', 'conf', 'uw', 'nw', 'rds', 'hr', 'sec', 'nat', 'cst', 'max', 'thurs', 'min', 'pr', 'gt', 'cl', 'bp', 'lt', 'comp', 'pf', 'md', 'gr', 'nite', 'sr', 'kb', 'tx', 'ht', 'nj', 'vp', 'wks', 'gl', 'mil', 'datetime', 'rb', 'xtra', 'celsius', '12h', 'fm', 'mag', 'wk', 'ph', 'colo', 'jr', 'mr', 'beta', 'br', 'nh', 'sci', 'mk', 'mth', 'op', 'warner', '2day', 'luv', 'co', 'cal', '30min', 'g77', 'reg', 'gf', 'st', 'rad', 'str', 'maj', 'sc', 'exp', 'tp', 'std', 'assoc']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  22 contient  30  mots\n",
            " \n",
            " \n",
            "['bird', 'phoenix', 'lizard', 'gecko', 'snake', 'thrush', 'fox', 'iguana', 'eagle', 'egg', 'wolf', 'lemming', 'griffin', 'robin', 'blackbird', 'wolverine', 'chicken', 'panda', 'newt', 'waterfowl', 'swan', 'lion', 'duck', 'groundhog', 'bunny', 'panther', 'chick', 'penguin', 'beaver', 'hibernate']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  23 contient  19  mots\n",
            " \n",
            " \n",
            "['800,000', '30000', '3,000', '6bn', '3k', '500k', '50million', '7,500', '8000', '10,000', '30,000', '4000', '3000', '25,000', '43m', '200,000', '7,000', '15k', '1,000']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  24 contient  18  mots\n",
            " \n",
            " \n",
            "['please', 'praise', 'thanks', 'plz', 'pls', 'bye', 'dear', 'thanx', 'thank', 'thx', 'goody', 'compliment', 'kudos', 'hello', 'sir', 'thnx', 'oy', 'amen']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  25 contient  38  mots\n",
            " \n",
            " \n",
            "['hack', 'stall', 'pat', 'bet', 'flounder', 'stunner', 'boffin', 'kibosh', 'sideline', 'flip-flop', 'crunch', 'ass', 'hash', 'freak', 'right-winger', 'nerd', 'hug', 'clunker', 'winger', 'underbelly', 'bust', 'butt', 'upside', 'barney', 'hugger', 'shocker', 'groupie', 'loser', 'dud', 'hacker', 'pervert', 'livewire', 'fudge', 'flipflop', 'cutie', 'ups', 'lowdown', 'flop']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  26 contient  13  mots\n",
            " \n",
            " \n",
            "['reopen', 'awaken', 'close', 'opening', 'closure', 'open', 'closer', 'wakeup', 'shuts', 'shut', 'wake', 'door', 'woken']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  27 contient  28  mots\n",
            " \n",
            " \n",
            "['glacier', 'artic', 'melt', 'arctic', 'antarctica', 'ski', 'ice', 'thaw', 'snowy', 'glacial', 'frozen', 'freeze', 'hockey', 'cream', 'kayak', 'iceberg', 'polar', 'igloo', 'tundra', 'snow-laden', 'snowless', 'iced', 'antarctic', 'sled', 'glaciologists', 'ice-covered', 'snowman', 'skiing']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  28 contient  14  mots\n",
            " \n",
            " \n",
            "['cafe', 'grape', 'wine', 'vino', 'bottle', 'brewer', 'starbucks', 'beer', 'mead', 'margarita', 'partyers', 'speakeasy', 'partiers', 'caf']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  29 contient  46  mots\n",
            " \n",
            " \n",
            "['vulnerable', 'high', 'serious', 'strong', 'hugh', 'sizable', 'large', 'small', 'long', 'vast', 'extreme', 'hard', 'poor', 'big', 'hardest', 'delicate', 'low', 'moderate', 'broad', 'rich', 'lean', 'huge', 'powerful', 'weak', 'massive', 'tough', 'super', 'sensitive', 'slim', 'intense', 'tiny', 'tepid', 'heavy', 'short', 'soft', 'susceptible', 'deep', 'severe', 'mild', 'ginormous', 'solid', 'thin', 'steady', 'harder', 'poorer', 'toughest']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  30 contient  29  mots\n",
            " \n",
            " \n",
            "['deal', 'accord', 'hope', 'sign', 'stake', 'denial', 'renewal', 'signing', 'affirmation', 'negotiate', 'renew', 'compromise', 'dedication', 'promise', 'agreement', 'negotiation', 'pledge', 'sacrifice', 'acceptance', 'behest', 'sake', 'commitment', 'unwillingness', 'inability', 'co-signing', 'reaffirmation', 'honor', 'plea', 'willingness']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  31 contient  20  mots\n",
            " \n",
            " \n",
            "['peruvian', 'tibetan', 'irish', 'cisco', 'hispanic', 'inuit', 'scottish', 'mayan', 'eskimo', 'scot', 'tibet', 'austrian', 'latin', 'trad', 'dalai', 'lama', 'latino', 'spanish', 'scotsman', 'brazilian']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  32 contient  28  mots\n",
            " \n",
            " \n",
            "['forest', 'ecosystem', 'environmentalist', 'eco', 'environmental', 'sustainability', 'biodiversity', 'environmentalism', 'renewable', 'wildlife', 'conservation', 'habitat', 'greenpeace', 'sustainable', 'tropical', 'enviro', 'ecology', 'deforestation', 'ecologic', 'conservationist', 'rainforest', 'conservancy', 'reforestation', 'wetland', 'hydrology', 'forestry', 'ecological', 'greenway']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  33 contient  39  mots\n",
            " \n",
            " \n",
            "['driver', 'park', 'pew', 'car', 'lane', 'tire', 'aisle', 'automotive', 'motorsports', 'nascar', 'ferris', 'cart', 'tudor', 'elevator', 'limo', 'slush', 'legged', 'auto', 'hood', 'toolshed', 'snowdrift', 'parking', 'brake', 'sidewalk', 'driveway', 'blower', 'ride', 'pothole', 'bike', 'amusement', 'carpool', '2ft', 'yard', 'backyard', 'snowplow', 'weatherboarding', 'horse', 'tipper', 'wheel']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  34 contient  62  mots\n",
            " \n",
            " \n",
            "['honest', 'simple', 'cute', 'active', 'creative', 'porous', 'medium', 'clever', 'elliptical', 'fun', 'frank', 'brave', 'fancy', 'hostile', 'expensive', 'fluent', 'tolerant', 'glossy', 'raw', 'lucrative', 'tart', 'verdant', 'attractive', 'neutral', 'humble', 'dynamic', 'rugged', 'delicious', 'rough', 'jolly', 'profitable', 'smart', 'interpretive', 'fragrant', 'interactive', 'niche', 'cheaply', 'flammable', 'plain', 'productive', 'nicer', 'slick', 'fresh', 'transformative', 'crude', 'malleable', 'stubborn', 'healthy', 'steep', 'partisan', 'aromatic', 'cheap', 'truthful', 'passive', 'ardent', 'digital', 'loyal', 'blitz', 'quiet', 'respectful', 'visual', 'dumb']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  35 contient  42  mots\n",
            " \n",
            " \n",
            "['18', '25', '14', '40', '26', '12', '30', '22', '24', '10', '11', '21', '20', '15', '13', '33', '16', '52', '44', '39', '27', '28', '29', '31', '32', '34', '35', '36', '59', '09', '37', '55', '50', '19', '17', '43', '08', '07', '02', '45', '48', '23']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  36 contient  71  mots\n",
            " \n",
            " \n",
            "['bite', 'push', 'pour', 'poison', 'cleaner', 'clean', 'cut', 'sink', 'trigger', 'bury', 'kick', 'pull', 'reset', 'wear', 'hang', 'drawn', 'sting', 'tie', 'bump', 'sweep', 'shave', 'swallow', 'ditch', 'rip', 'shake', 'smack', 'slap', 'boost', 'stick', 'slam', 'dump', 'shed', 'rid', 'shred', 'pinch', 'grip', 'wash', 'drain', 'whack', 'contact', 'sever', 'dig', 'knock', 'shovel', 'pack', 'beat', 'scoop', 'punch', 'sprinkle', 'brush', 'stuck', 'jolt', 'snarl', 'scratch', 'tear', 'chop', 'pummel', 'puncture', 'scar', 'touch', 'wipe', 'chunk', 'lash', 'perk', 'draw', 'tap', 'drown', 'snap', 'shove', 'swamp', 'slice']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  37 contient  6  mots\n",
            " \n",
            " \n",
            "['chiller', 'conditioner', 'refrigerant', 'buster', 'yin', 'yang']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  38 contient  52  mots\n",
            " \n",
            " \n",
            "['computer', 'schedule', 'pen', 'lesson', 'gear', 'tv', 'test', 'curve', 'software', 'technology', 'discovery', 'innovation', 'diary', 'supercomputer', 'experiment', 'desk', 'console', 'television', 'quiz', 'visualization', 'mobile', 'phone', 'machine', 'load', 'timeline', 'radio', 'graph', 'synching', 'logbook', 'pencil', 'ink', 'homework', 'calculator', 'application', 'invention', 'app', 'patent', 'teleprompter', 'tech', 'sync', 'exploration', 'lab', 'chart', 'installation', 'cell', 'setup', 'laboratory', 'broadcasting', 'device', 'math', 'newsdesk', 'calendar']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  39 contient  74  mots\n",
            " \n",
            " \n",
            "['evidence', 'measurable', 'bad', 'good', 'illegal', 'proof', 'wrong', 'true', 'right', 'effective', 'key', 'prove', 'false', 'important', 'comprehensive', 'necessary', 'best', 'reliable', 'safe', 'secure', 'possible', 'full', 'timely', 'logic', 'proper', 'perfect', 'ok', 'truth', 'commonsense', 'related', 'useful', 'significant', 'fair', 'complete', 'rational', 'logical', 'prominent', 'precise', 'valid', 'baseless', 'consistent', 'informative', 'unsubstantiated', 'helpful', 'practical', 'relevant', 'well-founded', 'decent', 'fine', 'okay', 'credible', 'proven', 'legit', 'robust', 'integral', 'valuable', 'visible', 'observable', 'prompt', 'definitive', 'critical', 'accurate', 'vital', 'essential', 'better', 'realistic', 'logically', 'legitimate', 'tangible', 'reasonable', 'sensible', 'handy', 'legal', 'worthwhile']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  40 contient  24  mots\n",
            " \n",
            " \n",
            "['human', 'gore', 'race', 'class', 'animal', 'codex', 'labor', 'labour', 'canine', 'uncensored', 'gender', 'sex', 'male', 'censor', 'abstinence', 'waterboarding', 'slavery', 'prostitution', 'abortion', 'anatomy', 'masturbation', 'onanism', 'redaction', 'prostitute']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  41 contient  25  mots\n",
            " \n",
            " \n",
            "['fight', 'war', 'combat', 'struggle', 'attack', 'petition', 'battle', 'boycott', 'sortie', 'anti-union', 'engagement', 'rally', 'skirmish', 'opposition', 'protest', 'march', 'fortress', 'snub', 'picket', 'battling', 'conflict', 'invasion', 'siege', 'assault', 'ambush']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  42 contient  27  mots\n",
            " \n",
            " \n",
            "['senate', 'congress', 'politician', 'lawmaker', 'democracy', 'republican', 'governor', 'legislative', 'bipartisan', 'senator', 'citizen', 'democrat', 'democratic', 'monarchy', 'mayor', 'parliament', 'republic', 'recess', 'commerce', 'chamber', 'legislature', 'patriot', 'caucus', 'congressman', 'bipartisanship', 'legislator', 'congressional']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  43 contient  10  mots\n",
            " \n",
            " \n",
            "['biofuel', 'bioethicists', 'ethanol', 'cleantech', 'public-policy', 'think-tanks', 'biofuels', 'bioscience', 'biotech', 'biotechnology']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  44 contient  79  mots\n",
            " \n",
            " \n",
            "['how', 'this', 'thing', 'that', 'it', 'because', 'there', 'if', 'here', 'so', 'well', 'when', 'least', 'what', 'or', 'perhaps', 'then', 'either', 'due', 'once', 'nothing', 'before', 'why', 'though', 'anymore', 'after', 'later', 'lot', 'but', 'since', 'despite', 'instead', 'kind', 'enough', 'something', 'which', 'where', 'too', 'while', 'ago', 'as', 'unless', 'regardless', 'amid', 'except', 'maybe', 'everything', 'plus', 'otherwise', 'during', 'midst', 'again', 'type', 'bunch', 'although', 'however', 'sort', 'whether', 'wherever', 'whenever', 'anyway', 'stuff', 'itself', 'bit', 'kinda', 'else', 'nowhere', 'slightly', 'anything', 'spite', 'everywhere', 'dispite', 'directly', 'thus', 'anywhere', 'whatever', 'everytime', 'whose', 'besides']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  45 contient  99  mots\n",
            " \n",
            " \n",
            "['henderson', 'graham', 'klein', 'rogers', 'hansen', 'cahill', 'denis', 'hayes', 'goldberg', 'barrett', 'stevens', 'milton', 'jacob', 'webb', 'sebastian', 'jones', 'clive', 'thompson', 'simpson', 'armstrong', 'mann', 'jenkins', 'kyle', 'levin', 'stein', 'benjamin', 'phillips', 'knott', 'kemp', 'joseph', 'colbert', 'walter', 'williams', 'dixon', 'goodman', 'justin', 'wv', 'ross', 'elliott', 'turnbull', 'thomas', 'malcolm', 'mccarthy', 'richardson', 'perkins', 'brennan', 'adam', 'norris', 'clarkson', 'abraham', 'lincoln', 'winston', 'churchill', 'shannon', 'arnold', 'ian', 'katz', 'locke', 'kennedy', 'floyd', 'reagan', 'margaret', 'roosevelt', 'peterson', 'katrina', 'alan', 'bryan', 'walsh', 'dylan', 'robinson', 'brandon', 'noah', 'ark', 'wayne', 'benny', 'russell', 'giles', 'coleman', 'terry', 'gardner', 'garrett', 'chavez', 'harris', 'adler', 'morton', 'charlie', 'cohen', 'weber', 'henry', 'bieber', 'roy', 'spencer', 'gregory', 'hopkins', 'alec', 'alexander', 'haiti', 'carter', 'johnson']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  46 contient  32  mots\n",
            " \n",
            " \n",
            "['kar', 'ge', 'ma', 'pa', 'har', 'bo', 'ga', 'om', 'jeg', 'og', 'wa', 'aap', 'ni', 'mo', 'pri', 'jo', 'cha', 'bho', 'hum', 'hon', 'po', 'ba', 'bal', 'ho', 'ki', 'hu', 'ding', 'som', 'ri', 'nan', 'gb', 'ko']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  47 contient  24  mots\n",
            " \n",
            " \n",
            "['more', 'than', 'faster', 'much', 'browner', 'less', 'greener', 'warmer', 'wetter', 'rather', 'pricier', 'longer', 'greenest', 'hotter', 'cooler', 'cheaper', 'heavier', 'colder', 'thatn', 'shorter', 'smellier', 'weirder', 'lighter', 'funnier']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  48 contient  31  mots\n",
            " \n",
            " \n",
            "['presentation', 'conference', 'meeting', 'panelist', 'podcast', 'gathering', 'lecture', 'sponsorship', 'host', 'sponsor', 'co-host', 'sermon', 'confab', 'guest', 'webcast', 'speech', 'capstone', 'assembly', 'fete', 'webinar', 'hustings', 'colloquium', 'internship', 'tea-party', 'cosponsor', 'briefing', 'mini-grants', 'session', 'speaker', 'co-op', 'workshop']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  49 contient  15  mots\n",
            " \n",
            " \n",
            "['activist', 'investor', 'advocate', 'protester', 'advocacy', 'stakeholder', 'anti-poverty', 'campaigner', 'activism', 'lobbyist', 'shareholder', 'lobby', 'taxpayer', 'grassroots', 'dividend']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  50 contient  14  mots\n",
            " \n",
            " \n",
            "['preview', 'near-record', 'launch', 'long-awaited', 'premiere', 'blast-off', 'celebrity-filled', 'ballyhooed', 'appearance', 'record-breaking', 'debut', 'never-before-seen', 'record-setting', 'heatwave']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  51 contient  25  mots\n",
            " \n",
            " \n",
            "['mercury', 'lake', 'mine', 'valley', 'coal', 'drill', 'drilling', 'steam', 'mining', 'rig', 'tungsten', 'acupuncture', 'river', 'engine', 'hydropower', 'pond', 'basin', 'turbine', 'generator', 'off-shore', 'tugboat', 'dam', 'offshore', 'refinery', 'miner']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  52 contient  37  mots\n",
            " \n",
            " \n",
            "['mental', 'science', 'scientific', 'academic', 'clinical', 'commercial', 'economic', 'service', 'industrial', 'military', 'financial', 'social', 'utility', 'public', 'technical', 'political', 'private', 'mainstream', 'emotional', 'meta', 'psychological', 'finacial', 'intellectual', 'civil', 'abstract', 'physical', 'fringe', 'professional', 'pseudo-scientific', 'mechanistic', 'psychic', 'institutional', 'observational', 'behavioral', 'unforseen', 'data-driven', 'peer-reviewed']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  53 contient  34  mots\n",
            " \n",
            " \n",
            "['production', 'buyer', 'buying', 'demand', 'price', 'trade', 'merchant', 'broker', 'clearinghouse', 'bank', 'shopping', 'market', 'swap', 'retail', 'stock', 'product', 'retailer', 'trading', 'consumer', 'home-improvement', 'supply', 'transaction', 'manufacture', 'exchange', 'store', 'shop', 'banker', 'purchasing', 'commodity', 'distribution', 'delivery', 'mall', 'one-stop-shop', 'one-stop']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  54 contient  26  mots\n",
            " \n",
            " \n",
            "['p2', 'g8', '0', '2c', '8p', 'e2', 'c3', '2.89', '912', '6.30', 'pm', '873', '1010', 'n2', '93.5', '11a', '2p', '1001', '4.10', '2132', '2b', '91.3', '756', 'a5', '26.95', '33,700']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  55 contient  33  mots\n",
            " \n",
            " \n",
            "['al', 'ha', 'per', 'la', 'un', 'et', 'dio', 'se', 'ani', 'ne', 'en', 'ex', 'sa', 'asa', 'pas', 'contra', 'mi', 'de', 'non', 'con', 'di', 'pe', 'versus', 'ai', 'est', 'cu', 'ou', 'pro', 'aka', 'au', 'da', 'su', 'tu']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  56 contient  57  mots\n",
            " \n",
            " \n",
            "['upper', 'platform', 'spin', 'discus', 'brace', 'round', 'bat', 'yorkers', 'wire', 'floor', 'strand', 'ground', 'front', 'top', 'telegraph', 'party', 'groove', 'drum', 'line', 'shield', 'side', 'bottom', 'flag', 'ball', 'corner', 'string', 'base', 'jam', 'quarter', 'wing', 'pole', 'column', 'x2', 'rope', 'lid', 'jack', 'coin', 'middle', 'row', 'pitch', 'trap', 'foundation', 'compass', 'bronze', 'roof', 'anchor', 'gold', 'square', 'midway', 'circle', 'seal', 'snare', 'rim', 'vertical', 'plate', 'dial', 'jig']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  57 contient  35  mots\n",
            " \n",
            " \n",
            "['fuck', 'screw', 'doodoo', 'ya', 'damn', 'workin', 'suck', 'dis', 'sic', 'backstabbed', 'heck', 'darn', 'ye', 'ol', 'lyin', 'dang', 'piss', 'effed', 'goddammit', 'dat', 'stfu', 'hell', 'fuckin', 'doin', 'eff', 'yo', 'assed', 'stoopid', 'shitload', 'gon', 'lil', 'gettin', 'smarta', 'dickens', 'wised']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  58 contient  20  mots\n",
            " \n",
            " \n",
            "['ft', 'million', 'kuna', 'billion', 'mole', 'dollar', 'ton', 'cc', 'pound', 'mile', 'trillion', 'mol', 'euro', 'inch', 'meter', 'gallon', 'lb', 'mm', 'oz', 'watt']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  59 contient  36  mots\n",
            " \n",
            " \n",
            "['crazy', 'ignorant', 'hysteria', 'impotent', 'panic', 'insane', 'unwashed', 'ill', 'mad', 'chaotic', 'toothless', 'blind', 'knee-jerk', 'reckless', 'desperate', 'chaos', 'hysterical', 'frenzy', 'blindness', 'clueless', 'numb', 'sore', 'orderly', 'sick', 'illiterate', 'demented', 'suicidal', 'hysteric', 'thoughtless', 'paranoid', 'delusional', 'drunk', 'drunken', 'uninformed', 'madness', 'one-eyed']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  60 contient  27  mots\n",
            " \n",
            " \n",
            "['surprise', 'sigh', 'wrath', 'horror', 'shock', 'unrest', 'backlash', 'relief', 'furor', 'rage', 'turmoil', 'instability', 'solitude', 'mockery', 'scrutiny', 'comfort', 'attention', 'scorn', 'ridicule', 'outrage', 'joy', 'disappointment', 'pain', 'discontent', 'warmth', 'solace', 'anger']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  61 contient  26  mots\n",
            " \n",
            " \n",
            "['blog', 'researcher', 'scientist', 'correspondent', 'economist', 'biologist', 'author', 'writer', 'letter-writer', 'blogosphere', 'politico', 'editor', 'wonk', 'columnist', 'commentator', 'reader', 'ecologist', 'physicist', 'journalist', 'theologian', 'blogger', 'reporter', 'contributor', 'pundit', 'analyst', 'scholar']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  62 contient  32  mots\n",
            " \n",
            " \n",
            "['toe', 'hair', 'foot', 'eye', 'ear', 'glove', 'teeth', 'smile', 'hand', 'fingernail', 'rectum', 'kidney', 'camera', 'lip', 'bald', 'beard', 'eyeball', 'grin', 'mouth', 'vagina', 'lens', 'shank', 'finger', 'arm', 'boot', 'shoe', 'shin', 'nail', 'tooth', 'thumb', 'throat', 'gaze']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  63 contient  17  mots\n",
            " \n",
            " \n",
            "['syed', 'meena', 'ramesh', 'delhi', 'rohit', 'shyam', 'raj', 'varanasi', 'noida', 'uttar', 'pradesh', 'pune', 'dubey', 'menon', 'kolkata', 'krishna', 'islamabad']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  64 contient  45  mots\n",
            " \n",
            " \n",
            "['u', 'abt', 'i', 'tht', 'thr', 'th', 'ur', 're', 'tha', 'althoug', 'itz', 'ppl', 'fro', 'cuz', 'yr', 'b4', 'etc', 'forbiden', 'mabye', 'agre', 'soo', 'bc', 'newsflash', 'imo', 'htink', 'fwiw', 'ot', 'esp', 'ike', 'btw', 'teh', 'ie', 'everythin', 'believ', 'somethings', 'vry', 'secondly', 'altho', 'fyi', 'tho', 'fo', 'ppls', 'wht', 'changin', 'thi']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  65 contient  21  mots\n",
            " \n",
            " \n",
            "['vatican', 'jesus', 'angel', 'carol', 'thanksgiving', 'god', 'christian', 'evil', 'lord', 'methodist', 'christianity', 'bible', 'nazism', 'islam', 'pope', 'prophet', 'hitler', 'catholic', 'devil', 'karma', 'easter']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  66 contient  19  mots\n",
            " \n",
            " \n",
            "['recycle', 'waste', 'trash', 'single-use', 'plastic', 'bag', 'junk', 'stack', 'packaging', 'manila', 'package', 'pile', 'bin', 'envelope', 'accumulation', 'garbage', 'tupperware', 'heap', 'accumulate']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  67 contient  23  mots\n",
            " \n",
            " \n",
            "['intensify', 'impact', 'affect', 'effect', 'intensified', 'influence', 'alter', 'affected', 'exaggerated', 'exaggerates', 'underestimated', 'underway', 'outweigh', 'negate', 'hurt', 'weighs', 'misread', 'alters', 'negates', 'overrate', 'outnumbers', 'weigh', 'exaggerate']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  68 contient  29  mots\n",
            " \n",
            " \n",
            "['flaw', 'solution', 'damage', 'peril', 'threat', 'vulnerability', 'danger', 'problem', 'risk', 'harm', 'proble', 'hazard', 'distraction', 'roadblock', 'dilemma', 'setback', 'nuisance', 'quandry', 'curse', 'menace', 'disruption', 'jigsaw', 'puzzle', 'barrier', 'bane', 'interference', 'hiccup', 'unsolved', 'lifeline']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  69 contient  14  mots\n",
            " \n",
            " \n",
            "['climate', 'storm', 'tornado', 'weather', 'prediction', 'meteorology', 'meteorologist', 'climatologist', 'forecast', 'hurricane', 'weathercasters', 'weatherman', 'forecasting', 'forecaster']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  70 contient  34  mots\n",
            " \n",
            " \n",
            "['patient', 'business', 'industry', 'health', 'corporation', 'company', 'architect', 'care', 'audit', 'sector', 'doc', 'insurance', 'manufacturer', 'enterprise', 'corporate', 'healthcare', 'engineering', 'maker', 'sanitation', 'firm', 'foundry', 'profession', 'accountancy', 'engineer', 'vet', 'academia', 'e-health', 'career', 'insurer', 'architecture', 're-insurance', 'industy', 'doctor', 'med']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  71 contient  30  mots\n",
            " \n",
            " \n",
            "['sun', 'green', 'fairy', 'white', 'blinking', 'wave', 'black', 'dim', 'red', 'light', 'yellow', 'dark', 'brown', 'domino', 'rainbow', 'sky', 'bright', 'ray', 'blue', 'ripple', 'fading', 'fleeting', 'gray', 'grey', 'shadow', 'violet', 'unicorn', 'moon', 'flash', 'sunset']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  72 contient  77  mots\n",
            " \n",
            " \n",
            "['act', 'show', 'track', 'lead', 'support', 'play', 'notice', 'way', 'place', 'billing', 'lose', 'stretch', 'toward', 'end', 'help', 'win', 'trouble', 'credit', 'listing', 'call', 'cost', 'time', 'benefit', 'part', 'loss', 'advance', 'series', 'return', 'spot', 'loses', 'worth', 'rescue', 'order', 'set', 'towards', 'date', 'fit', 'advantage', 'free', 'aid', 'list', 'merit', 'worthiness', 'note', 'retuning', 'rank', 'pay', 'advancement', 'turn', 'yield', 'progress', 'sing', 'victory', 'card', 'ranked', 'record', 'backing', 'form', 'gain', 'tune', 'rent', 'course', 'expense', 'bother', 'role', 'charge', 'justify', 'warrant', 'match', 'feature', 'paid', 'fee', 'advanced', 'fold', 'backup', 'rating', 'assist']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  73 contient  35  mots\n",
            " \n",
            " \n",
            "['report', 'analysis', 'study', 'reporting', 'expert', 'morale', 'review', 'work', 'research', 'adviser', 'conduct', 'knowledge', 'panel', 'guru', 'advisory', 'advisor', 'advice', 'guidance', 'insider', 'board', 'wisdom', 'inquiry', 'request', 'chair', 'commission', 'investigation', 'bulletin', 'specialist', 'committee', 'intel', 'assessment', 'experience', 'probe', 'insight', 'intelligence']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  74 contient  17  mots\n",
            " \n",
            " \n",
            "['minister', 'bishop', 'episcopal', 'earl', 'admiral', 'royal', 'sultan', 'clergy', 'church', 'king', 'palace', 'duchess', 'imperial', 'navy', 'queen', 'prince', 'kingdom']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  75 contient  24  mots\n",
            " \n",
            " \n",
            "['wed', 'mother', 'age', 'birthday', 'grandson', 'wife', 'lover', 'wedding', 'married', 'uncle', 'sister', 'father', 'marriage', 'friend', 'born', 'neighbor', 'birth', 'colleague', 'pal', 'roommate', 'grandchild', 'daughter', 'brother', 'couple']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  76 contient  18  mots\n",
            " \n",
            " \n",
            "['brilliant', 'great', 'beautiful', 'nice', 'excellent', 'world-famous', 'superb', 'awesome', 'famous', 'unbelievable', 'tremendous', 'wonderful', 'terrific', 'lovely', 'eminent', 'renowned', 'world-renowned', 'famed']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  77 contient  8  mots\n",
            " \n",
            " \n",
            "['geopolitics', 'intergovernmental', 'non-proliferation', 'diplomatic', 'diplomacy', 'geopolitical', 'inter-governmental', 'inter-agency']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  78 contient  104  mots\n",
            " \n",
            " \n",
            "['look', 'compete', 'bring', 'do', 'say', 'attribute', 'read', 'see', 'learn', 'put', 'check', 'make', 'give', 'qualify', 'find', 'take', 'join', 'get', 'know', 'display', 'ask', 'marked', 'mention', 'explain', 'tell', 'watch', 'include', 'compare', 'treat', 'raise', 'seek', 'contain', 'offer', 'remind', 'warn', 'reading', 'follow', 'meet', 'receive', 'write', 'reach', 'pick', 'carry', 'cover', 'provide', 'teach', 'choose', 'visit', 'attend', 'displayed', 'testify', 'select', 'enter', 'mark', 'add', 'introduce', 'substitute', 'satisfy', 'ensure', 'pursue', 'share', 'speak', 'afford', 'send', 'repeat', 'fill', 'behave', 'alert', 'witness', 'remove', 'highlight', 'showcase', 'forewarn', 'imply', 'insure', 'dispatch', 'hire', 'discuss', 'clarify', 'achieve', 'beware', 'appoint', 'echo', 'communicate', 'specify', 'represent', 'sent', 'assure', 'connote', 'juxtapose', 'elect', 'distinguish', 'indicate', 'herald', 'replace', 'duplicate', 'inform', 'deliver', 'observe', 'insert', 'express', 'nominate', 'chosen', 'serve']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  79 contient  51  mots\n",
            " \n",
            " \n",
            "['nyc', 'mississippi', 'california', 'virginia', 'washington', 'colorado', 'york', 'alaska', 'florida', 'paris', 'maine', 'connecticut', 'chicago', 'wisconsin', 'brooklyn', 'montana', 'arkansas', 'nevada', 'hollywood', 'milwaukee', 'vermont', 'michigan', 'ohio', 'utah', 'houston', 'oregon', 'denver', 'dallas', 'philly', 'mid-atlantic', 'seattle', 'texas', 'orleans', 'massachusetts', 'baltimore', 'illinois', 'cali', 'montgomery', 'orlando', 'manitoba', 'midwest', 'atlanta', 'minnesota', 'maryland', 'newark', 'hawaii', 'berlin', 'jerusalem', 'pittsburgh', 'cincinnati', 'minneapolis']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  80 contient  39  mots\n",
            " \n",
            " \n",
            "['killer', 'culprit', 'conspiracy', 'rebel', 'abuse', 'racist', 'guilty', 'crime', 'cover-up', 'racism', 'drug', 'victim', 'steroid', 'wrongdoing', 'mafia-style', 'terrorism', 'guerrilla', 'violent', 'neglect', 'terrorist', 'suicide', 'gang', 'mob', 'guilt', 'homosexuality', 'homophobic', 'punishment', 'breakaway', 'rebellious', 'offense', 'abuser', 'predator', 'treason', 'substance', 'malpractice', 'serial', 'prescription', 'apex', 'violence']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  81 contient  13  mots\n",
            " \n",
            " \n",
            "['50-50', 'award', 'roulette', 'probability', 'nobel', 'laureate', 'reward', 'odds', 'contest', 'lottery', 'competition', 'pulitzer', 'prize']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  82 contient  49  mots\n",
            " \n",
            " \n",
            "['rate', 'degree', 'tip', 'level', 'average', 'estimate', 'compute', 'sheet', 'ease', 'count', 'relative', 'limit', 'pressure', 'concentration', 'momentum', 'traction', 'assortment', 'glue', 'tension', 'number', 'coverage', 'centrifugal', 'balance', 'amount', 'projection', 'camp', 'variety', 'breakdown', 'binding', 'mass', 'figure', 'counting', 'percent', 'overall', 'minimum', 'magnitude', 'imbalance', 'extent', 'weight', 'diversity', 'absolute', 'aggregate', 'sum', 'build-up', 'exposure', 'calculation', 'lock-in', 'stickiness', 'gravity']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  83 contient  28  mots\n",
            " \n",
            " \n",
            "['global', 'national', 'indigenous', 'alien', 'stranger', 'foreign', 'local', 'sacred', 'international', 'small-scale', 'daily', 'regional', 'native', 'ancient', 'ceremony', 'two-day', 'ceremonial', 'annual', 'cross-cultural', 'quadrennial', 'ancestral', 'ritual', 'scale', 'tribal', 'one-man', 'holy', 'tribe', 'weekly']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  84 contient  16  mots\n",
            " \n",
            " \n",
            "['reform', 'development', 'mitigation', 'expansion', 'reduction', 'adaption', 'removal', 'prevention', 'adaptation', 'growth', 'abatement', 'stimulus', 'restoration', 'reorganization', 'modification', 'conversion']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  85 contient  15  mots\n",
            " \n",
            " \n",
            "['2010', '2009', '2000', '1992', '2008', '2007', '1971', '1905', '1995', '1978', '1953', '1772', '1816', '2011', '2003']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  86 contient  17  mots\n",
            " \n",
            " \n",
            "['scream', 'peep', 'whisper', 'boing', 'hurrah', 'bleat', 'cheer', 'shout', 'cry', 'coo', 'mutter', 'yack', 'squawk', 'yap', 'whine', 'boo', 'hoot']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  87 contient  27  mots\n",
            " \n",
            " \n",
            "['cap', 'clothing', 'hat', 'jacket', 'cape', 'jean', 'clothes', 'sleeve', 'cloth', 'shower', 'furniture', 'bed', 'bath', 'undies', 'nakedness', 'shirt', 'waist', 'naked', 'bunk', 'swimsuit', 'strap-on', 'coat', 'jersey', 'plaid', 'twill', 'pant', 'closet']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  88 contient  18  mots\n",
            " \n",
            " \n",
            "['angry', 'mocked', 'concerned', 'upset', 'duped', 'worried', 'annoyed', 'swayed', 'disturbed', 'slimmed', 'tricked', 'alarmed', 'displeased', 'surprised', 'undone', 're-thought', 'remade', 'challenged']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  89 contient  22  mots\n",
            " \n",
            " \n",
            "['campaign', 'election', 'office', 'post', 'electoral', 're-election', 'job', 'position', 'candidate', 'referendum', 'nomination', 'ballot', 'vote', 'nom', 'voting', 'vacancy', 'ticket', 'presidency', 'seat', 'voter', 'employment', 'candidacy']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  90 contient  17  mots\n",
            " \n",
            " \n",
            "['hunger', 'poverty', 'scarcity', 'shortage', 'famine', 'depletion', 'seasonal', 'drought', 'unemployment', 'downswing', 'homelessness', 'rebound', 'illiteracy', 'exhaustion', 'recession', 'unemployed', 'cyclical']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  91 contient  27  mots\n",
            " \n",
            " \n",
            "['speed', 'variability', 'efficiency', 'peace', 'justice', 'independence', 'freedom', 'persistence', 'fairness', 'sensitivity', 'equality', 'transparency', 'resilience', 'confidence', 'skepticism', 'trust', 'clarity', 'equity', 'scepticism', 'accuracy', 'adaptability', 'prudence', 'caution', 'uncertainty', 'prosperity', 'certainty', 'openness']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  92 contient  22  mots\n",
            " \n",
            " \n",
            "['forensics', 'trial', 'attorney', 'regulator', 'counsel', 'cop', 'watchdog', 'police', 'court', 'ombudsman', 'judge', 'suit', 'bail', 'officer', 'inspection', 'detective', 'jail', 'examiner', 'lawsuit', 'prison', 'inspector', 'lawyer']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  93 contient  11  mots\n",
            " \n",
            " \n",
            "['southern', 'south', 'west', 'southernmost', 'east', 'northeast', 'western', 'north', 'eastern', 'northern', 'west-northwest']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  94 contient  35  mots\n",
            " \n",
            " \n",
            "['invasive', 'hazardous', 'catastrophic', 'deadly', 'temper', 'difficult', 'dangerous', 'inconvenient', 'positive', 'untenable', 'disastrous', 'dire', 'fickle', 'temperamental', 'negative', 'ominous', 'negatively', 'destructive', 'hurtful', 'terrible', 'unbearable', 'bloody', 'impossible', 'cruel', 'harsh', 'horrible', 'blunt', 'unpredictable', 'draconian', 'bitter', 'precarious', 'harmful', 'cranky', 'nasty', 'awful']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  95 contient  38  mots\n",
            " \n",
            " \n",
            "['water', 'temperature', 'gas', 'chemistry', 'air', 'tube', 'energy', 'heat', 'organic', 'food', 'atmospheric', 'cloud', 'fuel', 'ozone', 'layer', 'oil', 'heating', 'stratosphere', 'emission', 'nuclear', 'solar', 'wind', 'atmosphere', 'thermal', 'channel', 'flow', 'absorption', 'stream', 'electric', 'electrical', 'magnetic', 'stratospheric', 'vapor', 'chemical', 'hole', 'vaporization', 'flux', 'radiation']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  96 contient  23  mots\n",
            " \n",
            " \n",
            "['opponent', 'foe', 'icon', 'athlete', 'star', 'fanatic', 'zealot', 'worship', 'celebrity', 'believer', 'ideologue', 'supporter', 'follower', 'actress', 'proponent', 'fan', 'worshiper', 'backer', 'critic', 'idol', 'zeal', 'watcher', 'detractor']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  97 contient  46  mots\n",
            " \n",
            " \n",
            "['change', 'urinate', 'fall', 'move', 'pass', 'leap', 'slide', 'rise', 'sneak', 'travel', 'switch', 'miss', 'throw', 'drop', 'rob', 'shift', 'decline', 'passing', 'stole', 'jump', 'caught', 'navigate', 'walk', 'buck', 'shot', 'reverse', 'steal', 'fly', 'chase', 'skip', 'catch', 'chuck', 'bandwagon', 'warp', 'orient', 'transfer', 'drift', 'swim', 'jot', 'shoot', 'flip', 'grab', 'drag', 'arrest', 'teleport', 'capture']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  98 contient  15  mots\n",
            " \n",
            " \n",
            "['population', 'extinct', 'apocalypse', 'annihilation', 'genocidal', 'overpopulation', 'doomsday', 'rapture', 'urbanism', 'extinction', 'populus', 'survivalist', 'populaton', 'genocide', 'ecocide']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  99 contient  11  mots\n",
            " \n",
            " \n",
            "['spark', 'burner', 'kindle', 'fire', 'burning', 'torch', 'burn', 'combustion', 'ignite', 'candle', 'bulb']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  100 contient  58  mots\n",
            " \n",
            " \n",
            "['peter', 'bob', 'james', 'tim', 'fred', 'john', 'richard', 'larry', 'vic', 'jackson', 'matt', 'charles', 'steven', 'daniel', 'brian', 'joe', 'paul', 'michael', 'jeff', 'tom', 'david', 'kevin', 'glenn', 'neil', 'jim', 'george', 'andrew', 'madison', 'jason', 'patrick', 'chris', 'stephen', 'robert', 'jon', 'matthew', 'jake', 'gordon', 'sean', 'steve', 'simon', 'christopher', 'scott', 'mike', 'stewart', 'pete', 'alex', 'dennis', 'rick', 'phil', 'greg', 'jerry', 'monty', 'keith', 'brad', 'lewis', 'doug', 'jimmy', 'martin']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  101 contient  35  mots\n",
            " \n",
            " \n",
            "['money', 'net', 'redistribution', 'alms', 'budget', 'fortune', 'fund', 'allocation', 'funding', 'profiteer', 'profit', 'tax', 'greed', 'fame', 'grant', 'outgo', 'glory', 'cash', 'wealth', 'philanthropy', 'excess', 'income', 'spending', 'entitlement', 'debt', 'subsidy', 'revenge', 'lust', 'deficit', 'pocket', 'windfall', 'loot', 'wallet', 'surplus', 'charity']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  102 contient  9  mots\n",
            " \n",
            " \n",
            "['denier', 'doubter', 'denialist', 'climategate', 'warmist', 'sceptic', 'skeptic', 'denialists', 'denialism']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  103 contient  12  mots\n",
            " \n",
            " \n",
            "['evolution', 'evo', 'evolutionary', 'young-earth', 'creationists', 'creationist', 'fossile', 'fossil', 'dinosaur', 'megafauna', 'homo', 'hominid']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  104 contient  33  mots\n",
            " \n",
            " \n",
            "['contribute', 'heard', 'interview', 'listen', 'briefed', 'contributes', 'told', 'sits', 'accuse', 'overheard', 'amazes', 'complains', 'listens', 'deny', 'spoke', 'speaks', 'interviewed', 'respond', 'belongs', 'complain', 'accuses', 'hear', 'responds', 'refer', 'partakes', 'denies', 'deserves', 'annoys', 'described', 'describes', 'frightens', 'refers', 'joked']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  105 contient  20  mots\n",
            " \n",
            " \n",
            "['imperils', 'threatens', 'exacerbate', 'disrupt', 'worsen', 'threaten', 'endanger', 'devastate', 'paralyze', 'cripple', 'undermine', 'imperil', 'weaken', 'destabilize', 'degrade', 'derail', 'endangers', 'stabilize', 'impede', 'mar']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  106 contient  32  mots\n",
            " \n",
            " \n",
            "['very', 'increasingly', 'primarily', 'unusually', 'environmentally', 'nearly', 'heavily', 'equally', 'fairly', 'almost', 'completely', 'significantly', 'pretty', 'strongly', 'utterly', 'totally', 'largely', 'relatively', 'greatly', 'entirely', 'exclusively', 'practically', 'economically', 'quite', 'widely', 'absolutely', 'especially', 'particularly', 'highly', 'extremely', 'mostly', 'scientifically']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  107 contient  10  mots\n",
            " \n",
            " \n",
            "['fat', 'exercise', 'fitness', 'treadmill', 'workout', 'cardio', 'excercise', 'obesity', 'bmi', 'obese']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  108 contient  11  mots\n",
            " \n",
            " \n",
            "['street-corner', 'retro', 'space-age', 'disco', 'well-worn', 'corny', 'hipster', 'trite', 'twee', 'hippie', 'feel-good']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  109 contient  68  mots\n",
            " \n",
            " \n",
            "['create', 'sue', 'kill', 'use', 'absorb', 'save', 'prepare', 'eat', 'mobilize', 'consume', 'manipulate', 'buy', 'adapt', 'activate', 'protect', 'mobilise', 'convince', 'maintain', 'commit', 'produce', 'publish', 'invest', 'impose', 'perpetrate', 'sell', 'fabricate', 'integrate', 'adopt', 'gather', 'utilize', 'organize', 'pollute', 'destroy', 'forge', 'perpetuate', 'promote', 'prosecute', 'develop', 'incorporate', 'rely', 'distribute', 'connect', 'enact', 'engage', 'defend', 'combine', 'operate', 'build', 'embrace', 'orchestrate', 'transform', 'foster', 'establish', 'abandon', 'adjust', 'enlist', 'invent', 'implement', 'concoct', 'outbid', 'poach', 'sustain', 'convene', 'initiate', 'dedicate', 'exploit', 'donate', 'emit']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  110 contient  11  mots\n",
            " \n",
            " \n",
            "['carbon', 'oxygen', 'methane', 'nitrous', 'oxide', 'ammonia', 'nitrogen', 'dihydrogen', 'monoxide', 'dioxide', 'glucose']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  111 contient  24  mots\n",
            " \n",
            " \n",
            "['be', 'have', 'induced', 'exist', 'remain', 'occur', 'seem', 'require', 'allow', 'involve', 'become', 'happen', 'erupt', 'involves', 'remains', 'exists', 'appear', 'enable', 'happens', 'persists', 'arises', 'becomes', 'ensue', 'induce']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  112 contient  13  mots\n",
            " \n",
            " \n",
            "['cool', 'cold', 'hot', 'dry', 'cloudy', 'warm', 'humid', 'cozy', 'sunny', 'toasty', 'wet', 'chill', 'unseasonably']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  113 contient  14  mots\n",
            " \n",
            " \n",
            "['world', 'country', 'earth', 'nation', 'continent', 'subcontinent', 'planet', 'globe', 'humanity', 'cosmos', 'planetary', 'hemisphere', 'universe', 'mankind']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  114 contient  92  mots\n",
            " \n",
            " \n",
            "['tool', 'cycle', 'strategy', 'perspective', 'symptom', 'myth', 'model', 'view', 'action', 'effort', 'motion', 'element', 'plan', 'account', 'contribution', 'step', 'point', 'event', 'resource', 'revolution', 'awareness', 'pattern', 'activity', 'approach', 'building', 'perception', 'regime', 'scenario', 'programme', 'system', 'fashion', 'program', 'period', 'scheme', 'habit', 'consciousness', 'landscape', 'process', 'sight', 'portfolio', 'diet', 'project', 'structure', 'stage', 'style', 'initiative', 'era', 'guide', 'movement', 'vogue', 'phenomenon', 'transition', 'attitude', 'appetite', 'footprint', 'impression', 'frame', 'dimension', 'factor', 'angle', 'trend', 'background', 'mood', 'profile', 'illusion', 'outcome', 'vision', 'shape', 'resolution', 'hearing', 'epoch', 'behaviour', 'manual', 'aspect', 'method', 'technique', 'phase', 'scene', 'redux', 'mechanism', 'slant', 'severity', 'screen', 'condition', 'occurrence', 'framework', 'radar', 'bias', 'sea-change', 'design', 'behavior', 'horizon']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  115 contient  17  mots\n",
            " \n",
            " \n",
            "['indicator', 'stats', 'measure', 'poll', 'benchmarking', 'survey', 'gauge', 'census', 'measurement', 'statistic', 'barometer', 'benchmark', 'scorecard', 'pollster', 'statistician', 'gallup', 'statistical']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  116 contient  28  mots\n",
            " \n",
            " \n",
            "['ecotone', 'clinique', 'nanofibers', 'inseparability', 'degrowth', 'mid-continent', 'geosphere', 'commercial-grade', 'guar', 'gazelle', 'freeriding', 'data-crunching', 'soy-based', 'mosquitofish', 'nyala', 'natgas', 'carbon-based', 'pika', 'co-benefits', 'redeye', 'himalayan', 'no-take', 'vegan', 'anti-allergy', 'pollack', 'gearless', 'remineralization', 'topi']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  117 contient  37  mots\n",
            " \n",
            " \n",
            "['seriously', 'soon', 'publically', 'rapidly', 'effectively', 'politely', 'finally', 'responsibly', 'suavely', 'conveniently', 'gently', 'quietly', 'endlessly', 'easily', 'sensibly', 'slowly', 'boldly', 'suddenly', 'eventually', 'bold', 'unequally', 'quickly', 'wryly', 'concisely', 'sweetly', 'officially', 'formally', 'sincerely', 'inadvertently', 'incorrectly', 'purposely', 'belatedly', 'strategically', 'incessantly', 'publicly', 'innovatively', 'badly']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  118 contient  19  mots\n",
            " \n",
            " \n",
            "['scam', 'hoax', 'fraud', 'bluff', 'fradulent', 'swindle', 'phony', 'ruse', 'fake', 'mock', 'bogus', 'bluffing', 'pretense', 'fool', 'pretend', 'hoaxster', 'fraudster', 'scamsters', 'dummy']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  119 contient  17  mots\n",
            " \n",
            " \n",
            "['celebrate', 'laud', 'rejoice', 'preach', 'tout', 'criticize', 'dissect', 'worry', 'chide', 'proselytize', 'fret', 'fear', 'opine', 'gloat', 'scold', 'overreact', 'criticise']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  120 contient  14  mots\n",
            " \n",
            " \n",
            "['org', 'gov', 'ceo', 'pres', 'dept', 'biz', 'govt', 'exec', 'department', 'corp', 'orgs', 'hq', 'govts', 'rep']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  121 contient  23  mots\n",
            " \n",
            " \n",
            "['ocean', 'coastal', 'whale', 'sea', 'oceanic', 'waterfront', 'shore', 'marine', 'bay', 'island', 'coast', 'ship', 'fishing', 'boat', 'shark', 'turtle', 'leatherback', 'ridley', 'fish', 'beach', 'shrimp', 'fishery', 'squid']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  122 contient  22  mots\n",
            " \n",
            " \n",
            "['sediment', 'fungus', 'frost', 'collagen', 'microbe', 'soil', 'insect', 'organism', 'pollen', 'clot', 'bacteria', 'algae', 'bee', 'fertilizer', 'moisture', 'flake', 'coral', 'bug', 'protein', 'mulch', 'hive', 'gene']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  123 contient  12  mots\n",
            " \n",
            " \n",
            "['26th', '27th', '40th', '2nd', '5th', '4th', '1st', '48th', '13th', '20th', '21st', '18th']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  124 contient  17  mots\n",
            " \n",
            " \n",
            "['tsunami', 'flood', 'spill', 'earthquake', 'landslide', 'crisis', 'leak', 'disaster', 'collapse', 'tragedy', 'meltdown', 'destruction', 'quake', 'deluge', 'devastation', 'aftermath', 'catastrophe']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  125 contient  36  mots\n",
            " \n",
            " \n",
            "['nasa', 'ipcc', 'sachs', 'pentagon', 'conn', 'g20', 'oxfam', 'motorola', 'wwf', 'nationa', 'siemens', 'goldman', 'soylent', 'gmo', 'gove', 'intl', 'tribbles', 'exxon', 'toyota', 'prius', 'saturn', 'nwo', 'chems', 'verizon', 'x-men', 'mythbusters', 'audi', 'todate', 'gw', 'gobal', 'wmd', 'gps', 'suv', 'documentry', 'kyoto', 'pluto']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  126 contient  35  mots\n",
            " \n",
            " \n",
            "['art', 'archive', 'artist', 'crowd', 'audience', 'rock', 'tour', 'music', 'library', 'festival', 'studio', 'museum', 'pop', 'theater', 'viola', 'room', 'space', 'exhibition', 'folk', 'facility', 'harp', 'jazz', 'auditorium', 'entertainment', 'gig', 'collection', 'venue', 'spectator', 'circuit', 'painting', 'dance', 'location', 'tribune', 'storage', 'instrument']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  127 contient  20  mots\n",
            " \n",
            " \n",
            "['ventura', 'san', 'francisco', 'lopez', 'sierra', 'diego', 'carlos', 'nino', 'santiago', 'ivan', 'pablo', 'juan', 'vega', 'amer', 'romero', 'luis', 'rojas', 'fidel', 'castro', 'rio']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  128 contient  20  mots\n",
            " \n",
            " \n",
            "['suggests', 'confirms', 'writes', 'declares', 'warns', 'agrees', 'asks', 'predicts', 'reminds', 'recommends', 'argues', 'reveals', 'proposes', 'insists', 'indicates', 'explains', 'admits', 'presumes', 'discovers', 'contends']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  129 contient  13  mots\n",
            " \n",
            " \n",
            "['youtube', 'amazon', 'fb', 'uploaded', 'tweet', 'twitter', 'facebook', 'linkedin', 'unfollow', 'ebook', 'itunes', 'favorited', 'flickr']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  130 contient  55  mots\n",
            " \n",
            " \n",
            "['to', 'the', 'and', 'in', 'for', 'around', 'of', 'far', 'from', 'on', 'across', 'by', 'back', 'with', 'into', 'at', 'between', 'out', 'over', 'up', 'about', 'apart', 'via', 'against', 'under', 'off', 'down', 'among', 'locally', 'below', 'above', 'away', 'forth', 'together', 'ahead', 'through', 'behind', 'forward', 'worldwide', 'outside', 'along', 'straight', 'abroad', 'beyond', 'beside', 'unnoticed', 'near', 'within', 'indoors', 'inside', 'onto', 'thru', 'globally', 'upwards', 'upon']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  131 contient  7  mots\n",
            " \n",
            " \n",
            "['thursday', 'monday', 'wednesday', 'sunday', 'friday', 'tuesday', 'saturday']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  132 contient  74  mots\n",
            " \n",
            " \n",
            "['message', 'blame', 'cause', 'issue', 'difference', 'explanation', 'focus', 'chance', 'connection', 'fact', 'force', 'alarm', 'fault', 'interest', 'consideration', 'case', 'warning', 'voice', 'power', 'reason', 'alternative', 'hint', 'goal', 'consequence', 'matter', 'option', 'reminder', 'lack', 'example', 'concern', 'target', 'control', 'choice', 'opinion', 'decision', 'result', 'landmark', 'priority', 'topic', 'subject', 'authority', 'aim', 'task', 'possibility', 'opportunity', 'clue', 'command', 'sound', 'prospect', 'inaction', 'importance', 'significance', 'purpose', 'majority', 'relationship', 'potential', 'responsibility', 'loud', 'success', 'excuse', 'distinction', 'failure', 'intention', 'historic', 'absent', 'historical', 'absence', 'reliance', 'potentially', 'relation', 'by-product', 'intent', 'emphasis', 'signal']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  133 contient  47  mots\n",
            " \n",
            " \n",
            "['hey', 'huh', 'yes', 'urgh', 'bleh', 'hmmmm', 'hmmm', 'eh', 'wtf', 'er', 'lol', 'yeah', 'oh', 'mmmm', 'hahah', 'hmm', 'hahaha', 'yikes', 'haha', 'omg', 'ohhhh', 'wow', 'errr', 'noooooo', 'uh-oh', 'crikey', 'um', 'uh', 'ugh', 'smh', 'yea', 'yup', 'lmao', 'noooo', 'hah', 'woot', 'oops', 'yay', 'whoa', 'aww', 'duh', 'argh', 'yuck', 'grrr', 'nah', 'grrrr', 'nope']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  134 contient  21  mots\n",
            " \n",
            " \n",
            "['seclude', 'nationalize', 'reinvent', 'unplug', 'steamroll', 'whiten', 'brighten', 'overturn', 'reclaim', 'convolute', 'reinvigorate', 'decarbonise', 'innovate', 'scuttle', 'hunker', 'regain', 'retrain', 'desiccate', 'depreciate', 'upend', 'rehabilitate']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  135 contient  12  mots\n",
            " \n",
            " \n",
            "['dwindles', 'soar', 'disappear', 'skyrocket', 'unravel', 'plummet', 'dwindle', 'shrink', 'unravels', 'disappears', 'reappear', 'vanished']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  136 contient  64  mots\n",
            " \n",
            " \n",
            "['calif', 'pli', 'foll', 'afp', 'harte', 'sourc', 'worldnews', 'usc', 'spr', 'cli', 'cir', 'macbeth', 'clo', 'tole', 'mlb', 'nyu', 'clima', 'supp', 'cfr', 'acad', 'prc', 'doa', 'nih', 'htt', 'fac', 'edm', 'chron', 'yid', 'dhs', 'rcp', 'syracuse', 'resp', 'ict', 'ste', 'twp', 'byu', 'spe', 'xc', 'fonda', 'glo', 'mkt', 'sundance', 'brr', '2u', 'chg', 'accel', 'cre', 'bx', 'smithsonian', 'bos', 'whiche', 'cymru', 'sociali', 'ernst', 'lars', 'usw', 'rls', 'pgm', 'curr', 'archiv', 'issu', 'edt', 'tol', 'hrd']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  137 contient  35  mots\n",
            " \n",
            " \n",
            "['rapid', 'rare', 'fast', 'irreversible', 'unequivocal', 'slow', 'likely', 'imminent', 'unprecedented', 'immediate', 'exceptional', 'unlikely', 'automatic', 'sudden', 'quick', 'clear', 'surprising', 'dramatic', 'willful', 'obvious', 'imperative', 'emergency', 'unusual', 'intentional', 'inevitable', 'instant', 'easy', 'undeniable', 'plausible', 'unexpected', 'necessity', 'accidental', 'deliberate', 'abrupt', 'urgent']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  138 contient  34  mots\n",
            " \n",
            " \n",
            "['spring', 'late', 'year', 'day', 'tonight', 'morning', 'week', 'early', 'weekend', 'until', 'winter', 'decade', '1970s', 'minute', 'afternoon', 'noon', 'hour', 'summer', 'month', 'till', 'yesterday', 'autumn', 'mid', 'tomorrow', 'asap', 'midnight', 'til', 'night', 'mid-day', 'sooner', 'century', 'eve', 'overnight', 'tommorow']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  139 contient  27  mots\n",
            " \n",
            " \n",
            "['chat', 'debate', 'discussion', 'buzz', 'dialogue', 'conversation', 'consensus', 'controversy', 'discourse', 'dispute', 'argument', 'thread', 'rhetoric', 'forum', 'dialectic', 'bashing', 'vent', 'monologue', 'froth', 'fodder', 'arguement', 'discusson', 'hype', 'chatter', 'spar', 'grist', 'rant']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  140 contient  5  mots\n",
            " \n",
            " \n",
            "['7pm', '3pm', '1pm', '2pm', '10pm']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  141 contient  24  mots\n",
            " \n",
            " \n",
            "['specie', 'cara', 'humani', 'veritas', 'alexia', 'maria', 'los', 'angeles', 'alba', 'imperforate', 'costa', 'buen', 'vivir', 'prima', 'fascia', 'mierda', 'nova', 'pena', 'terra', 'cambio', 'riba', 'paz', 'scintilla', 'ahora']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  142 contient  16  mots\n",
            " \n",
            " \n",
            "['2', '4', '1', '3', '8', '7', '9', '5', '6', '19-22', '10-20', '4-5', '10-1', '12-20', '1-2', '10-14']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  143 contient  11  mots\n",
            " \n",
            " \n",
            "['phd', 'harvard', 'grad', 'uni', 'cambridge', 'econ', 'undergrad', 'yale', 'dissertation', 'univ', 'princeton']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  144 contient  20  mots\n",
            " \n",
            " \n",
            "['epa', 'gop', 'teaparty', 'reid', 'obama', 'barack', 'kerry', 'palin', 'biden', 'obozo', 'whitehouse', 'pelosi', 'mccain', 'dnc', 'clinton', 'obamas', 'atty', 'scotus', 'barry', 'hillary']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  145 contient  15  mots\n",
            " \n",
            " \n",
            "['two', 'three', 'thousand', 'eight', 'nine', 'fifty', 'six', 'twenty', 'ten', 'hundred', 'twelve', 'four', 'thirty', 'twenty-five', 'forty-nine']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  146 contient  45  mots\n",
            " \n",
            " \n",
            "['news', 'book', 'magazine', 'journal', 'memo', 'press', 'release', 'newspaper', 'item', 'agenda', 'protocol', 'backgrounder', 'paper', 'editorial', 'coda', 'newswire', 'announcement', 'word', 'plank', 'headline', 'letter', 'phrase', 'term', 'version', 'charter', 'edition', 'chapter', 'draft', 'manifesto', 'roadmap', 'publication', 'roundup', 'piece', 'tidbit', 'rumor', 'theme', 'passage', 'metaphor', 'treaty', 'excerpt', 'communique', 'convention', 'primer', 'gazette', 'recommendation']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  147 contient  45  mots\n",
            " \n",
            " \n",
            "['stay', 'start', 'wait', 'stop', 'settle', 'come', 'delay', 'arrive', 'go', 'curb', 'await', 'suffer', 'continue', 'dawdle', 'spur', 'keep', 'let', 'migrate', 'lie', 'left', 'decide', 'halt', 'begin', 'refrain', 'emerge', 'kept', 'sack', 'finish', 'leave', 'hurry', 'withstand', 'sit', 'lay', 'alone', 'rush', 'bear', 'endure', 'rest', 'stand', 'survive', 'delayed', 'thrive', 'resign', 'quit', 'lag']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  148 contient  35  mots\n",
            " \n",
            " \n",
            "['thought', 'hold', 'gotten', 'met', 'mean', 'risen', 'saw', 'held', 'developed', 'experienced', 'won', 'brimming', 'run', 'found', 'knew', 'drive', 'feed', 'spread', 'pumped', 'fed', 'unearthed', 'grow', 'fell', 'ran', 'finding', 'clocked', 'grown', 'laden', 'drip', 'meant', 'rode', 'rose', 'driven', 'bought', 'spews']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  149 contient  27  mots\n",
            " \n",
            " \n",
            "['we', 'you', 'they', 'people', 'person', 'me', 'everyone', 'yourselves', 'man', 'them', 'woman', 'someone', 'he', 'him', 'she', 'themselves', 'anyone', 'others', 'guy', 'somebody', 'himself', 'myself', 'everybody', 'men', 'dude', 'us', 'ourselves']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  150 contient  10  mots\n",
            " \n",
            " \n",
            "['sneeze', 'excrement', 'poo', 'fart', 'flatulence', 'sweat', 'breath', 'gassy', 'smell', 'poop']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  151 contient  45  mots\n",
            " \n",
            " \n",
            "['lindsay', 'lindsey', 'rebecca', 'mary', 'ellen', 'cynthia', 'sarah', 'lori', 'tracy', 'claire', 'gail', 'brooke', 'geoff', 'jessica', 'meredith', 'marley', 'cory', 'laura', 'christie', 'jackie', 'jesse', 'freddie', 'tina', 'whitney', 'amy', 'juliet', 'kate', 'jennifer', 'naomi', 'geldof', 'jane', 'rachel', 'kim', 'lisa', 'laurie', 'billie', 'irene', 'michelle', 'cathy', 'kendall', 'julia', 'carrie', 'amanda', 'wendy', 'suzanne']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  152 contient  21  mots\n",
            " \n",
            " \n",
            "['farce', 'scandal', 'indiscretion', 'blunder', 'embarassment', 'shame', 'error', 'mess', 'stupidity', 'disgrace', 'bungle', 'ignorance', 'mistake', 'contradiction', 'incoherence', 'idiocy', 'gaffe', 'goof', 'fiasco', 'stupidness', 'confusion']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  153 contient  10  mots\n",
            " \n",
            " \n",
            "['climate-change', 'global-warming', 'biochar', 'geoengineering', 'pollutant', 'pollution', 'cap-and-trade', 'polluter', 'air-quality', 'sequestration']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  154 contient  11  mots\n",
            " \n",
            " \n",
            "['topography', 'volcano', 'map', 'geologist', 'volcanic', 'eruption', 'magmatic', 'geological', 'mapping', 'geography', 'tectonics']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  155 contient  14  mots\n",
            " \n",
            " \n",
            "['discover', 'hide', 'unveiled', 'unveil', 'reveal', 'disclosure', 'expose', 'secret', 'disclose', 'announces', 'hiding', 'announce', 'hidden', 'uncover']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  156 contient  16  mots\n",
            " \n",
            " \n",
            "['rationalist', 'religious', 'evangelical', 'moral', 'epistemic', 'evangelicals', 'faith-based', 'religion', 'secular', 'ethical', 'faith', 'belief', 'non-believers', 'atheist', 'ethic', 'worldviews']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  157 contient  22  mots\n",
            " \n",
            " \n",
            "['probs', 'definately', 'isnt', 'hav', 'youve', 'im', 'thats', 'prob', 'hv', 'youre', 'prolly', 'certaintly', 'musta', 'whats', 'wheres', 'wouldve', 'happend', 'havent', 'hows', 'wasnt', 'ive', 'probly']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  158 contient  42  mots\n",
            " \n",
            " \n",
            "['africa', 'uruguay', 'india', 'bolivia', 'mediterranean', 'indian', 'african', 'ethiopia', 'nigerian', 'kenya', 'arabia', 'ethiopian', 'bolivian', 'zealand', 'namibia', 'malaysia', 'indonesia', 'peru', 'arab', 'atlantic', 'thailand', 'wwii', 'greenland', 'kerala', 'uganda', 'nile', 'maldives', 'nigeria', 'pakistan', 'iran', 'mongolia', 'iraqi', 'lagos', 'siberia', 'papua', 'muslim', 'jakarta', 'worl', 'briton', 'delta', 'indus', 'ecuador']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  159 contient  7  mots\n",
            " \n",
            " \n",
            "['gravest', 'coldest', 'coolest', 'snowiest', 'most-watched', 'hottest', 'deadliest']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  160 contient  23  mots\n",
            " \n",
            " \n",
            "['fortunate', 'conscious', 'aware', 'sorry', 'able', 'ready', 'keen', 'sure', 'astute', 'happy', 'lucky', 'capable', 'welcome', 'luck', 'sad', 'unwilling', 'willing', 'interested', 'prepared', 'busy', 'glad', 'responsible', 'curious']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  161 contient  41  mots\n",
            " \n",
            " \n",
            "['warms', 'identifies', 'brings', 'sends', 'expands', 'lessens', 'delivers', 'avoids', 'brought', 'replaces', 'dismisses', 'downplays', 'promotes', 'defends', 'creates', 'solves', 'investigates', 'receives', 'likens', 'adopts', 'considers', 'nears', 'extends', 'destroys', 'explores', 'decries', 'rearranges', 'reinforces', 'violates', 'reorganizes', 'enhances', 'encourages', 'conflates', 'coincides', 'ignores', 'eliminates', 'enters', 'reaffirms', 'undermined', 'recognizes', 'undermines']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  162 contient  19  mots\n",
            " \n",
            " \n",
            "['increase', 'offset', 'reduce', 'extend', 'prevent', 'improve', 'accelerate', 'expand', 'shorten', 'double', 'avoid', 'mitigate', 'compensate', 'minimise', 'redouble', 'quadruple', 'eliminate', 'decrease', 'discourage']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  163 contient  9  mots\n",
            " \n",
            " \n",
            "['boil', 'cure', 'nettle', 'tincture', 'lance', 'physic', 'heal', 'salve', 'remedy']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  164 contient  7  mots\n",
            " \n",
            " \n",
            "['baseball', 'cricket', 'soccer', 'golf', 'sport', 'tennis', 'golfer']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  165 contient  15  mots\n",
            " \n",
            " \n",
            "['dinner', 'meal', 'kitchen', 'bake', 'lunch', 'burger', 'buffet', 'soup', 'microwave', 'takeaway', 'cooked', 'pot', 'cook', 'pub', 'oven']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  166 contient  6  mots\n",
            " \n",
            " \n",
            "['saltiness', 'acidification', 'salinity', 'acid', 'acidity', 'acidic']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  167 contient  20  mots\n",
            " \n",
            " \n",
            "['safety', 'security', 'defense', 'hedge', 'sentinel', 'guardian', 'caretaker', 'deposit', 'protection', 'observatory', 'keeper', 'lighthouse', 'buffer', 'survival', 'protective', 'reserve', 'defence', 'defensive', 'survivability', 'guard']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  168 contient  10  mots\n",
            " \n",
            " \n",
            "['looney', 'nut', 'maniac', 'lunatic', 'loon', 'wacko', 'fruitcake', 'nutcase', 'crank', 'loony']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  169 contient  8  mots\n",
            " \n",
            " \n",
            "['rain', 'snow', 'blizzard', 'sleet', 'snowstorm', 'rainstorm', 'snowfall', 'precipitation']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  170 contient  54  mots\n",
            " \n",
            " \n",
            "['just', 'previously', 'not', 'today', 'ever', 'also', 'who', 'still', 'never', 'now', 'typically', 'clearly', 'only', 'even', 'actually', 'already', 'yet', 'lately', 'purportedly', 'always', 'really', 'rarely', 'truly', 'regularly', 'often', 'half', 'sometimes', 'probably', 'nor', 'apparently', 'currently', 'unsurprisingly', 'barely', 'recently', 'usually', 'definitely', 'specifically', 'oftentimes', 'sadly', 'fundamentally', 'indeed', 'necessarily', 'predictably', 'essentially', 'surprisingly', 'neither', 'basically', 'supposedly', 'certainly', 'scarcely', 'originally', 'one-third', 'literally', 'thankfully']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  171 contient  29  mots\n",
            " \n",
            " \n",
            "['ama', 'iri', 'mau', 'chu', 'lau', 'chan', 'fu', 'aro', 'liu', 'huang', 'kun', 'sev', 'matai', 'kari', 'gue', 'kita', 'deh', 'sama', 'chang', 'pon', 'saran', 'buz', 'koko', 'hou', 'hea', 'oba', 'gua', 'nao', 'anu']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  172 contient  14  mots\n",
            " \n",
            " \n",
            "['reuters', 'newsweek', 'bbc', 'bloomberg', 'cnn', 'foxnews', 'nbc', 'abc', 'wsj', 'nyt', 'foxnews.com', 'msnbc', 'nytimes', 'cbs']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  173 contient  16  mots\n",
            " \n",
            " \n",
            "['ikut', 'dgn', 'bumi', 'udah', 'buat', 'cari', 'aja', 'hujan', 'panas', 'baca', 'tentang', 'bisa', 'mungkin', 'rumah', 'masih', 'industri']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  174 contient  14  mots\n",
            " \n",
            " \n",
            "['funniest', 'funny', 'joke', 'hilarious', 'comedy', 'parody', 'laugh', 'irony', 'comedian', 'ironic', 'humor', 'wit', 'spoof', 'satirical']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  175 contient  10  mots\n",
            " \n",
            " \n",
            "['london', 'bristol', 'fc', 'sheffield', 'cfc', 'wakefield', 'kent', 'chelsea', 'bradford', 'preston']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  176 contient  33  mots\n",
            " \n",
            " \n",
            "['idea', 'source', 'assumption', 'claim', 'implication', 'comment', 'quote', 'declaration', 'question', 'answer', 'remark', 'statement', 'reflection', 'cite', 'complaint', 'hypothesis', 'proposal', 'notion', 'premise', 'response', 'conclusion', 'theory', 'reply', 'rebuttal', 'assertations', 'reference', 'stmt', 'allegation', 'observation', 'commentary', 'presumption', 'misquote', 'assertion']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  177 contient  19  mots\n",
            " \n",
            " \n",
            "['alarmist', 'birthers', 'statists', 'anti-americanism', 'nutjobs', 'sheeple', 'fearmongering', 'alarmism', 'enviros', 'antiscience', 'doomsayers', 'knuckle-draggers', 'anti-intellectualism', 'asshats', 'pervs', 'greenies', 'pseudoskeptics', 'interwebs', 'cultism']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  178 contient  10  mots\n",
            " \n",
            " \n",
            "['ad', 'advertisement', 'advert', 'advertising', 'hoc', 'marketing', 'branding', 'label', 'brand', 'tourism']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  179 contient  31  mots\n",
            " \n",
            " \n",
            "['75', '86', '90', '65', '350', '100', '85', '140', '66', '70', '60', '77', '110', '82', '120', '76', '210', '406', '73', '225', '201', '124', '78', '62', '298', '80', '102', '101', '287', '141', '83']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  180 contient  8  mots\n",
            " \n",
            " \n",
            "['warming', 'man-made', 'human-caused', 'human-induced', 'human-driven', 'anthropogenic', 'man-caused', 'manmade']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  181 contient  23  mots\n",
            " \n",
            " \n",
            "['complicate', 'daunt', 'excite', 'simplify', 'whet', 'illuminate', 'amaze', 'confuse', 'inspire', 'empower', 'entertain', 'amuse', 'mislead', 'motivate', 'overwhelm', 'tempt', 'animate', 'scare', 'enrich', 'frighten', 'dominate', 'enlighten', 'terrify']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  182 contient  8  mots\n",
            " \n",
            " \n",
            "['1.1', '2.5', '1.6', '9.1', '2.4', '3.5', '4.3', '8.8']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  183 contient  32  mots\n",
            " \n",
            " \n",
            "['leader', 'negotiator', 'director', 'founder', 'secretary', 'president', 'associate', 'vice', 'management', 'secretary-general', 'assistant', 'envoy', 'chairman', 'leadership', 'agent', 'producer', 'agency', 'delegate', 'organiser', 'secretariat', 'executive', 'spokesman', 'spokesperson', 'delegation', 'ambassador', 'manager', 'vice-chairman', 'worker', 'organizer', 'charismatic', 'boss', 'owner']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  184 contient  29  mots\n",
            " \n",
            " \n",
            "['face', 'solve', 'fracture', 'tackle', 'hit', 'unite', 'address', 'united', 'confront', 'struck', 'challenge', 'gulf', 'divide', 'cross', 'break', 'pose', 'broke', 'bridge', 'dealt', 'blow', 'head-on', 'strike', 'collide', 'encounter', 'crush', 'crack', 'resolve', 'broken', 'clash']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  185 contient  18  mots\n",
            " \n",
            " \n",
            "['brussels', 'dems', 'hague', 'alp', 'blair', 'cameron', 'lib', 'clegg', 'eu', 'mp', 'ukip', 'tory', 'rudd', 'abbott', 'libs', 'msm', 'rupert', 'murdoch']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  186 contient  39  mots\n",
            " \n",
            " \n",
            "['exit', 'video', 'clip', 'takedown', 'ramp', 'file', 'document', 'vid', 'internet', 'download', 'anonymous', 'site', 'block', 'online', 'password', 'mail', 'e-mail', 'extension', 'yahoo', 'web', 'copy', 'google', 'vein', 'email', 'avatar', 'pointer', 'confidential', 'portal', 'cyberspace', 'bookmark', 'access', 'sign-up', 'tracker', 'homepage', 'website', 'log', 'pdf', 'peer', 'search']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  187 contient  16  mots\n",
            " \n",
            " \n",
            "['belgium', 'swedish', 'iceland', 'oslo', 'icelandic', 'finnish', 'copenhagen', 'dk', 'denmark', 'danish', 'dutch', 'netherlands', 'norwegian', 'estonian', 'holland', 'amsterdam']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  188 contient  3  mots\n",
            " \n",
            " \n",
            "['2030', '2050', '2035']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  189 contient  17  mots\n",
            " \n",
            " \n",
            "['scum', 'idiot', 'douche', 'jackass', 'twerp', 'scumbag', 'moron', 'whiner', 'bitch', 'slut', 'dumbass', 'dick', 'dipshit', 'asshole', 'liar', 'cocksucker', 'dickhead']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  190 contient  14  mots\n",
            " \n",
            " \n",
            "['cooperation', 'co-operation', 'coalition', 'partnership', 'partner', 'alliance', 'joint', 'collaborative', 'cooperative', 'communication', 'teamwork', 'venture', 'collaboration', 'consortium']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  191 contient  15  mots\n",
            " \n",
            " \n",
            "['baloney', 'bullshit', 'propaganda', 'shit', 'troll', 'hooey', 'indoctrination', 'rubbish', 'crock', 'brainwashing', 'nonsense', 'agitprop', 'crap', 'shill', 'crapola']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  192 contient  12  mots\n",
            " \n",
            " \n",
            "['scientifique', 'climat', 'austral', 'bel', 'bon', 'aire', 'mer', 'cru', 'ont', 'pu', 'mon', 'environ']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  193 contient  10  mots\n",
            " \n",
            " \n",
            "['liberal', 'conservative', 'right-wing', 'leftist', 'progressive', 'rightwing', 'left-wing', 'radical', 'extremist', 'lefty']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  194 contient  13  mots\n",
            " \n",
            " \n",
            "['apr', 'april', 'november', 'january', 'aug', 'feb', 'february', 'july', 'dec', 'jan', 'august', 'september', 'december']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  195 contient  4  mots\n",
            " \n",
            " \n",
            "['spent', 'spend', 'spends', 'devotes']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  196 contient  18  mots\n",
            " \n",
            " \n",
            "['cancer', 'illness', 'allergy', 'curable', 'disease', 'smallpox', 'asthma', 'lyme', 'headache', 'migraine', 'fever', 'flu', 'autism', 'vaccine', 'polio', 'infectious', 'malaria', 'allergic']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  197 contient  16  mots\n",
            " \n",
            " \n",
            "['child', 'baby', 'boomer', 'mom', 'youth', 'kiddo', 'kid', 'daddy', 'girl', 'boy', 'young', 'adult', 'mama', 'dad', 'teen', 'impressionable']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  198 contient  17  mots\n",
            " \n",
            " \n",
            "['salt', 'aerosol', 'cement', 'grit', 'slate', 'tobacco', 'stone', 'dust', 'concrete', 'asbestos', 'talcum', 'spray', 'powder', 'sand', 'desert', 'smoke', 'chalk']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n",
            "Le Cluster numéro :  199 contient  20  mots\n",
            " \n",
            " \n",
            "['ban', 'law', 'legislation', 'bill', 'policy', 'regulation', 'mandate', 'rule', 'copyright', 'prohibition', 'provision', 'amendment', 'liability', 'enforcement', 'outlaw', 'violation', 'edict', 'regulatory', 'doctrine', 'legistlation']\n",
            " \n",
            " \n",
            "################\n",
            " \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lFG0tpGoV5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors_ft = {}\n",
        "id_tweets = {}\n",
        "words_list = dataset_ft.index.tolist()\n",
        "\n",
        "\n",
        "\n",
        "for i, tweet in enumerate(clean_text):\n",
        "    vector = [0] * 200\n",
        "    for word in tweet:\n",
        "        if word in dic_words.keys():\n",
        "            vector[clustering.labels_[words_list.index(word)]] += 1\n",
        "    vectors_ft[\"T\" + str(i)] = vector\n",
        "    id_tweets[\"T\" + str(i)] = tweet    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mdnGWNToV5E",
        "colab_type": "code",
        "outputId": "fa77e521-6a6e-451b-fbc4-f34f01815e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "df2 = df.copy()\n",
        "df_rep = pd.DataFrame(vectors_ft)\n",
        "df_rep = df_rep.T\n",
        "\n",
        "existence = []\n",
        "for i, elt in enumerate(df2[\"Existence\"]):\n",
        "    existence.append(elt)\n",
        "df_rep.columns=[str(x) for x in range(df_rep.shape[1])]\n",
        "df_rep['Labels'] = existence\n",
        "\n",
        "df_fast2vec_cluster=df_rep.reset_index(drop=True)\n",
        "df_fast2vec_cluster.to_csv('sample_data/Projet_NLP/representations/fast2vec_cluster.csv')\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndf2 = df.copy()\\ndf_rep = pd.DataFrame(vectors_ft)\\ndf_rep = df_rep.T\\n\\nexistence = []\\nfor i, elt in enumerate(df2[\"Existence\"]):\\n    existence.append(elt)\\ndf_rep.columns=[str(x) for x in range(df_rep.shape[1])]\\ndf_rep[\\'Labels\\'] = existence\\n\\ndf_fast2vec_cluster=df_rep.reset_index(drop=True)\\ndf_fast2vec_cluster.to_csv(\\'sample_data/Projet_NLP/representations/fast2vec_cluster.csv\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0AagoNloV5L",
        "colab_type": "code",
        "outputId": "2f416aa7-33c6-4002-be0a-7d6036603144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df_fast2vec_cluster=pd.read_csv('sample_data/Projet_NLP/representations/fast2vec_cluster.csv',index_col=0)\n",
        "df_fast2vec_cluster                 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5534</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5535</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5536</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5537</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5538</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5539 rows × 201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0  1  2  3  4  5  6  7  8  ...  192  193  194  195  196  197  198  199  Labels\n",
              "0     0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     Yes\n",
              "1     0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     Yes\n",
              "2     0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     Yes\n",
              "3     0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     Yes\n",
              "4     0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     Yes\n",
              "...  .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...\n",
              "5534  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     NaN\n",
              "5535  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0      No\n",
              "5536  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0      No\n",
              "5537  0  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0    0    0      No\n",
              "5538  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0     NaN\n",
              "\n",
              "[5539 rows x 201 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usFBIW8xoV5P",
        "colab_type": "text"
      },
      "source": [
        "##### With averaging on the Fast2vec output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAKiZ-URoV5Q",
        "colab_type": "code",
        "outputId": "6cb23aad-f72e-46b9-ed4b-5b0eb75bd349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "vectors_ft_moy = {}\n",
        "id_tweets = {}\n",
        "\n",
        "\n",
        "for i, tweet in enumerate(clean_text):\n",
        "    vector = [0] * 300\n",
        "    qtt = 0\n",
        "    for word in tweet:\n",
        "        if word in dic_words.keys():\n",
        "            s = []\n",
        "            for elt in list(dataset_ft.loc[word]):\n",
        "                s.append(float(elt))\n",
        "            qtt += 1\n",
        "            vector = list( map(add, vector, s) )\n",
        "    if qtt != 0:\n",
        "        vectors_ft_moy[\"T\" + str(i)] = [x/qtt for x in vector]\n",
        "    else:\n",
        "        vectors_ft_moy[\"T\" + str(i)] = vector\n",
        "    id_tweets[\"T\" + str(i)] = tweet \n",
        "    \n",
        "df_moy = pd.DataFrame(vectors_ft_moy)\n",
        "df_moy = df_moy.T\n",
        "df_moy.columns=[str(x) for x in range(df_moy.shape[1])]\n",
        "df_moy['Labels'] = existence\n",
        "\n",
        "df_fast2vec_mean=df_moy.reset_index(drop=True)\n",
        "df_fast2vec_mean.to_csv(\"sample_data/Projet_NLP/representations/fast2vec_mean.csv\")\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvectors_ft_moy = {}\\nid_tweets = {}\\n\\n\\nfor i, tweet in enumerate(clean_text):\\n    vector = [0] * 300\\n    qtt = 0\\n    for word in tweet:\\n        if word in dic_words.keys():\\n            s = []\\n            for elt in list(dataset_ft.loc[word]):\\n                s.append(float(elt))\\n            qtt += 1\\n            vector = list( map(add, vector, s) )\\n    if qtt != 0:\\n        vectors_ft_moy[\"T\" + str(i)] = [x/qtt for x in vector]\\n    else:\\n        vectors_ft_moy[\"T\" + str(i)] = vector\\n    id_tweets[\"T\" + str(i)] = tweet \\n    \\ndf_moy = pd.DataFrame(vectors_ft_moy)\\ndf_moy = df_moy.T\\ndf_moy.columns=[str(x) for x in range(df_moy.shape[1])]\\ndf_moy[\\'Labels\\'] = existence\\n\\ndf_fast2vec_mean=df_moy.reset_index(drop=True)\\ndf_fast2vec_mean.to_csv(\"sample_data/Projet_NLP/representations/fast2vec_mean.csv\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GovP45aSoV5S",
        "colab_type": "code",
        "outputId": "9ac6872d-b51d-4ede-8d53-b0c356cc0350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df_fast2vec_mean=pd.read_csv(\"sample_data/Projet_NLP/representations/fast2vec_mean.csv\",index_col=0)\n",
        "df_fast2vec_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.023414</td>\n",
              "      <td>0.005850</td>\n",
              "      <td>-0.041093</td>\n",
              "      <td>-0.032021</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.039657</td>\n",
              "      <td>0.020650</td>\n",
              "      <td>0.031514</td>\n",
              "      <td>-0.048857</td>\n",
              "      <td>-0.005964</td>\n",
              "      <td>-0.029879</td>\n",
              "      <td>0.057693</td>\n",
              "      <td>-0.000607</td>\n",
              "      <td>-0.028543</td>\n",
              "      <td>-0.018771</td>\n",
              "      <td>0.011857</td>\n",
              "      <td>-0.020807</td>\n",
              "      <td>0.007100</td>\n",
              "      <td>-0.013857</td>\n",
              "      <td>-0.106514</td>\n",
              "      <td>0.018357</td>\n",
              "      <td>0.013786</td>\n",
              "      <td>-0.027336</td>\n",
              "      <td>-0.027043</td>\n",
              "      <td>-0.048336</td>\n",
              "      <td>0.036207</td>\n",
              "      <td>0.042264</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>-0.005479</td>\n",
              "      <td>0.022729</td>\n",
              "      <td>-0.033936</td>\n",
              "      <td>0.007764</td>\n",
              "      <td>0.003279</td>\n",
              "      <td>0.009814</td>\n",
              "      <td>0.006450</td>\n",
              "      <td>-0.040693</td>\n",
              "      <td>0.025693</td>\n",
              "      <td>-0.008421</td>\n",
              "      <td>-0.007957</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.033364</td>\n",
              "      <td>-0.002564</td>\n",
              "      <td>-0.049821</td>\n",
              "      <td>-0.197571</td>\n",
              "      <td>0.019686</td>\n",
              "      <td>0.023307</td>\n",
              "      <td>-0.010964</td>\n",
              "      <td>-0.097536</td>\n",
              "      <td>0.036914</td>\n",
              "      <td>0.067307</td>\n",
              "      <td>0.061079</td>\n",
              "      <td>-0.017550</td>\n",
              "      <td>-0.074857</td>\n",
              "      <td>-0.009607</td>\n",
              "      <td>0.050071</td>\n",
              "      <td>0.031607</td>\n",
              "      <td>0.006079</td>\n",
              "      <td>0.056236</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>-0.018607</td>\n",
              "      <td>0.013664</td>\n",
              "      <td>-0.036743</td>\n",
              "      <td>-0.015007</td>\n",
              "      <td>-0.033686</td>\n",
              "      <td>0.020993</td>\n",
              "      <td>-0.057393</td>\n",
              "      <td>-0.029650</td>\n",
              "      <td>-0.020000</td>\n",
              "      <td>-0.039186</td>\n",
              "      <td>-0.033821</td>\n",
              "      <td>-0.002979</td>\n",
              "      <td>-0.011879</td>\n",
              "      <td>-0.015871</td>\n",
              "      <td>-0.042950</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>0.093157</td>\n",
              "      <td>0.018993</td>\n",
              "      <td>-0.028171</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.021233</td>\n",
              "      <td>0.064300</td>\n",
              "      <td>-0.068433</td>\n",
              "      <td>0.039700</td>\n",
              "      <td>0.008767</td>\n",
              "      <td>0.032167</td>\n",
              "      <td>0.017867</td>\n",
              "      <td>0.007667</td>\n",
              "      <td>0.114133</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>-0.063900</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.081567</td>\n",
              "      <td>0.030567</td>\n",
              "      <td>-0.028967</td>\n",
              "      <td>0.033733</td>\n",
              "      <td>0.008933</td>\n",
              "      <td>-0.025833</td>\n",
              "      <td>0.053600</td>\n",
              "      <td>0.005633</td>\n",
              "      <td>-0.186667</td>\n",
              "      <td>0.042933</td>\n",
              "      <td>-0.004667</td>\n",
              "      <td>0.082767</td>\n",
              "      <td>-0.019867</td>\n",
              "      <td>-0.071633</td>\n",
              "      <td>0.117067</td>\n",
              "      <td>0.041733</td>\n",
              "      <td>-0.029700</td>\n",
              "      <td>-0.030167</td>\n",
              "      <td>0.040467</td>\n",
              "      <td>-0.068533</td>\n",
              "      <td>-0.021367</td>\n",
              "      <td>0.111367</td>\n",
              "      <td>0.070600</td>\n",
              "      <td>0.037967</td>\n",
              "      <td>-0.084733</td>\n",
              "      <td>-0.019700</td>\n",
              "      <td>-0.037100</td>\n",
              "      <td>0.049967</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016400</td>\n",
              "      <td>-0.012100</td>\n",
              "      <td>-0.010100</td>\n",
              "      <td>-0.193767</td>\n",
              "      <td>0.034567</td>\n",
              "      <td>0.042967</td>\n",
              "      <td>-0.060333</td>\n",
              "      <td>-0.166400</td>\n",
              "      <td>0.062867</td>\n",
              "      <td>0.122200</td>\n",
              "      <td>0.097433</td>\n",
              "      <td>-0.040633</td>\n",
              "      <td>-0.008833</td>\n",
              "      <td>-0.021733</td>\n",
              "      <td>0.116200</td>\n",
              "      <td>0.007200</td>\n",
              "      <td>-0.075567</td>\n",
              "      <td>0.002867</td>\n",
              "      <td>0.063300</td>\n",
              "      <td>-0.073167</td>\n",
              "      <td>-0.016567</td>\n",
              "      <td>-0.006633</td>\n",
              "      <td>0.028033</td>\n",
              "      <td>0.044067</td>\n",
              "      <td>-0.044833</td>\n",
              "      <td>-0.040233</td>\n",
              "      <td>-0.137500</td>\n",
              "      <td>0.038433</td>\n",
              "      <td>0.060133</td>\n",
              "      <td>0.019067</td>\n",
              "      <td>-0.038667</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>-0.007900</td>\n",
              "      <td>0.012667</td>\n",
              "      <td>0.049933</td>\n",
              "      <td>-0.054100</td>\n",
              "      <td>0.070800</td>\n",
              "      <td>0.014133</td>\n",
              "      <td>-0.050967</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.047160</td>\n",
              "      <td>0.005480</td>\n",
              "      <td>0.012520</td>\n",
              "      <td>-0.022360</td>\n",
              "      <td>-0.002520</td>\n",
              "      <td>-0.084140</td>\n",
              "      <td>-0.029600</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>0.055100</td>\n",
              "      <td>-0.033980</td>\n",
              "      <td>-0.017140</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>-0.056240</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.015280</td>\n",
              "      <td>0.058720</td>\n",
              "      <td>-0.061700</td>\n",
              "      <td>0.033560</td>\n",
              "      <td>-0.016680</td>\n",
              "      <td>0.069860</td>\n",
              "      <td>0.028740</td>\n",
              "      <td>0.031420</td>\n",
              "      <td>-0.009360</td>\n",
              "      <td>0.034200</td>\n",
              "      <td>0.027700</td>\n",
              "      <td>-0.044900</td>\n",
              "      <td>-0.018100</td>\n",
              "      <td>-0.050760</td>\n",
              "      <td>-0.008460</td>\n",
              "      <td>0.009040</td>\n",
              "      <td>0.075580</td>\n",
              "      <td>-0.024120</td>\n",
              "      <td>-0.020320</td>\n",
              "      <td>0.005580</td>\n",
              "      <td>0.011400</td>\n",
              "      <td>-0.106320</td>\n",
              "      <td>0.044720</td>\n",
              "      <td>-0.055600</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019560</td>\n",
              "      <td>-0.044760</td>\n",
              "      <td>-0.008140</td>\n",
              "      <td>-0.044180</td>\n",
              "      <td>-0.013500</td>\n",
              "      <td>0.047480</td>\n",
              "      <td>-0.023340</td>\n",
              "      <td>0.017320</td>\n",
              "      <td>0.050760</td>\n",
              "      <td>0.081780</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>-0.053200</td>\n",
              "      <td>0.035300</td>\n",
              "      <td>0.044680</td>\n",
              "      <td>0.054140</td>\n",
              "      <td>-0.051180</td>\n",
              "      <td>-0.014580</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>-0.023620</td>\n",
              "      <td>0.075180</td>\n",
              "      <td>0.035300</td>\n",
              "      <td>0.047940</td>\n",
              "      <td>-0.086700</td>\n",
              "      <td>0.056220</td>\n",
              "      <td>0.027840</td>\n",
              "      <td>-0.062140</td>\n",
              "      <td>-0.002160</td>\n",
              "      <td>-0.040660</td>\n",
              "      <td>0.018620</td>\n",
              "      <td>-0.007900</td>\n",
              "      <td>-0.003260</td>\n",
              "      <td>-0.014620</td>\n",
              "      <td>0.018440</td>\n",
              "      <td>-0.022480</td>\n",
              "      <td>0.009400</td>\n",
              "      <td>-0.023080</td>\n",
              "      <td>0.209880</td>\n",
              "      <td>0.079020</td>\n",
              "      <td>0.044420</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.004100</td>\n",
              "      <td>0.014900</td>\n",
              "      <td>-0.017750</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>-0.040650</td>\n",
              "      <td>-0.120650</td>\n",
              "      <td>-0.033950</td>\n",
              "      <td>0.036200</td>\n",
              "      <td>-0.010050</td>\n",
              "      <td>-0.011100</td>\n",
              "      <td>0.016400</td>\n",
              "      <td>0.036750</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.040600</td>\n",
              "      <td>-0.067700</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.020150</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>-0.091700</td>\n",
              "      <td>0.008300</td>\n",
              "      <td>-0.026700</td>\n",
              "      <td>0.032250</td>\n",
              "      <td>0.072650</td>\n",
              "      <td>0.018050</td>\n",
              "      <td>-0.065150</td>\n",
              "      <td>0.042200</td>\n",
              "      <td>0.014100</td>\n",
              "      <td>-0.022300</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.093500</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>0.016950</td>\n",
              "      <td>0.025500</td>\n",
              "      <td>-0.036750</td>\n",
              "      <td>-0.023200</td>\n",
              "      <td>-0.029700</td>\n",
              "      <td>0.022700</td>\n",
              "      <td>0.013850</td>\n",
              "      <td>0.028150</td>\n",
              "      <td>0.060050</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051950</td>\n",
              "      <td>-0.001100</td>\n",
              "      <td>0.025350</td>\n",
              "      <td>-0.205450</td>\n",
              "      <td>-0.029150</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.046150</td>\n",
              "      <td>0.138650</td>\n",
              "      <td>-0.024450</td>\n",
              "      <td>0.089250</td>\n",
              "      <td>0.016050</td>\n",
              "      <td>-0.013150</td>\n",
              "      <td>0.010750</td>\n",
              "      <td>-0.011400</td>\n",
              "      <td>0.056400</td>\n",
              "      <td>-0.048250</td>\n",
              "      <td>-0.005400</td>\n",
              "      <td>-0.001250</td>\n",
              "      <td>-0.085950</td>\n",
              "      <td>-0.054800</td>\n",
              "      <td>0.178250</td>\n",
              "      <td>-0.025100</td>\n",
              "      <td>-0.103700</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>-0.007350</td>\n",
              "      <td>-0.078200</td>\n",
              "      <td>-0.041150</td>\n",
              "      <td>0.044950</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>-0.022650</td>\n",
              "      <td>-0.007550</td>\n",
              "      <td>-0.043650</td>\n",
              "      <td>0.019400</td>\n",
              "      <td>-0.031350</td>\n",
              "      <td>-0.021100</td>\n",
              "      <td>-0.049050</td>\n",
              "      <td>0.225300</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>-0.010300</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.052200</td>\n",
              "      <td>0.022450</td>\n",
              "      <td>-0.183250</td>\n",
              "      <td>0.029650</td>\n",
              "      <td>-0.077250</td>\n",
              "      <td>-0.056450</td>\n",
              "      <td>-0.046750</td>\n",
              "      <td>-0.042300</td>\n",
              "      <td>0.083000</td>\n",
              "      <td>-0.031850</td>\n",
              "      <td>-0.002500</td>\n",
              "      <td>-0.012100</td>\n",
              "      <td>0.044500</td>\n",
              "      <td>0.028450</td>\n",
              "      <td>-0.043950</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.051050</td>\n",
              "      <td>0.118900</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>0.100850</td>\n",
              "      <td>-0.055800</td>\n",
              "      <td>-0.029950</td>\n",
              "      <td>-0.014300</td>\n",
              "      <td>0.016600</td>\n",
              "      <td>0.040150</td>\n",
              "      <td>-0.002450</td>\n",
              "      <td>0.054000</td>\n",
              "      <td>0.077700</td>\n",
              "      <td>-0.022900</td>\n",
              "      <td>0.047750</td>\n",
              "      <td>0.079400</td>\n",
              "      <td>0.019300</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>-0.013300</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>-0.060350</td>\n",
              "      <td>0.020550</td>\n",
              "      <td>-0.036950</td>\n",
              "      <td>0.044600</td>\n",
              "      <td>0.020250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030750</td>\n",
              "      <td>-0.020150</td>\n",
              "      <td>-0.022150</td>\n",
              "      <td>-0.105050</td>\n",
              "      <td>0.038700</td>\n",
              "      <td>0.101900</td>\n",
              "      <td>0.022500</td>\n",
              "      <td>-0.064100</td>\n",
              "      <td>0.024500</td>\n",
              "      <td>-0.097350</td>\n",
              "      <td>0.116950</td>\n",
              "      <td>-0.040650</td>\n",
              "      <td>0.007200</td>\n",
              "      <td>0.004150</td>\n",
              "      <td>-0.025750</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>-0.007900</td>\n",
              "      <td>-0.065400</td>\n",
              "      <td>-0.029900</td>\n",
              "      <td>0.077150</td>\n",
              "      <td>0.185400</td>\n",
              "      <td>0.020350</td>\n",
              "      <td>0.014450</td>\n",
              "      <td>0.052550</td>\n",
              "      <td>-0.052750</td>\n",
              "      <td>-0.052000</td>\n",
              "      <td>0.034450</td>\n",
              "      <td>-0.010400</td>\n",
              "      <td>0.078200</td>\n",
              "      <td>-0.027600</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>-0.058650</td>\n",
              "      <td>0.013300</td>\n",
              "      <td>-0.014750</td>\n",
              "      <td>-0.095300</td>\n",
              "      <td>-0.062300</td>\n",
              "      <td>0.187300</td>\n",
              "      <td>-0.010800</td>\n",
              "      <td>0.036300</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5534</th>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.068600</td>\n",
              "      <td>0.007980</td>\n",
              "      <td>0.049280</td>\n",
              "      <td>-0.011000</td>\n",
              "      <td>0.042040</td>\n",
              "      <td>0.105280</td>\n",
              "      <td>-0.005640</td>\n",
              "      <td>0.039080</td>\n",
              "      <td>-0.023800</td>\n",
              "      <td>0.019340</td>\n",
              "      <td>0.066400</td>\n",
              "      <td>0.010860</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.088220</td>\n",
              "      <td>0.039100</td>\n",
              "      <td>0.002680</td>\n",
              "      <td>-0.081160</td>\n",
              "      <td>0.052580</td>\n",
              "      <td>-0.045400</td>\n",
              "      <td>-0.173020</td>\n",
              "      <td>0.022040</td>\n",
              "      <td>0.071380</td>\n",
              "      <td>0.119280</td>\n",
              "      <td>0.053380</td>\n",
              "      <td>-0.029020</td>\n",
              "      <td>0.011580</td>\n",
              "      <td>0.058500</td>\n",
              "      <td>-0.025520</td>\n",
              "      <td>-0.102500</td>\n",
              "      <td>0.115380</td>\n",
              "      <td>0.017520</td>\n",
              "      <td>0.013860</td>\n",
              "      <td>0.142040</td>\n",
              "      <td>0.017140</td>\n",
              "      <td>0.036840</td>\n",
              "      <td>-0.017180</td>\n",
              "      <td>0.049620</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023740</td>\n",
              "      <td>-0.049120</td>\n",
              "      <td>0.020820</td>\n",
              "      <td>-0.258740</td>\n",
              "      <td>0.050340</td>\n",
              "      <td>0.036440</td>\n",
              "      <td>-0.018800</td>\n",
              "      <td>-0.084620</td>\n",
              "      <td>-0.066060</td>\n",
              "      <td>-0.040740</td>\n",
              "      <td>0.043340</td>\n",
              "      <td>-0.032100</td>\n",
              "      <td>-0.009340</td>\n",
              "      <td>-0.033980</td>\n",
              "      <td>-0.013600</td>\n",
              "      <td>0.046340</td>\n",
              "      <td>-0.130060</td>\n",
              "      <td>0.127980</td>\n",
              "      <td>0.053300</td>\n",
              "      <td>0.046340</td>\n",
              "      <td>-0.009580</td>\n",
              "      <td>-0.031300</td>\n",
              "      <td>0.052360</td>\n",
              "      <td>0.066780</td>\n",
              "      <td>-0.025460</td>\n",
              "      <td>-0.071140</td>\n",
              "      <td>-0.180300</td>\n",
              "      <td>-0.009860</td>\n",
              "      <td>0.043980</td>\n",
              "      <td>0.030620</td>\n",
              "      <td>-0.099220</td>\n",
              "      <td>0.037980</td>\n",
              "      <td>0.084960</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.066540</td>\n",
              "      <td>-0.000320</td>\n",
              "      <td>0.015800</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>-0.024940</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5535</th>\n",
              "      <td>-0.096400</td>\n",
              "      <td>-0.005040</td>\n",
              "      <td>-0.030640</td>\n",
              "      <td>-0.004840</td>\n",
              "      <td>0.003840</td>\n",
              "      <td>-0.017080</td>\n",
              "      <td>0.009400</td>\n",
              "      <td>-0.070480</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.011100</td>\n",
              "      <td>0.051920</td>\n",
              "      <td>0.059200</td>\n",
              "      <td>-0.007080</td>\n",
              "      <td>-0.031880</td>\n",
              "      <td>-0.063140</td>\n",
              "      <td>-0.016340</td>\n",
              "      <td>-0.021660</td>\n",
              "      <td>-0.056520</td>\n",
              "      <td>-0.017980</td>\n",
              "      <td>0.023800</td>\n",
              "      <td>-0.144240</td>\n",
              "      <td>-0.053580</td>\n",
              "      <td>0.067260</td>\n",
              "      <td>0.012880</td>\n",
              "      <td>-0.061160</td>\n",
              "      <td>-0.009040</td>\n",
              "      <td>-0.053320</td>\n",
              "      <td>0.076260</td>\n",
              "      <td>0.055660</td>\n",
              "      <td>0.056340</td>\n",
              "      <td>0.065940</td>\n",
              "      <td>-0.006820</td>\n",
              "      <td>0.037640</td>\n",
              "      <td>-0.030380</td>\n",
              "      <td>0.018560</td>\n",
              "      <td>0.021560</td>\n",
              "      <td>-0.001720</td>\n",
              "      <td>0.033460</td>\n",
              "      <td>0.006340</td>\n",
              "      <td>-0.023880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009200</td>\n",
              "      <td>-0.043980</td>\n",
              "      <td>0.023940</td>\n",
              "      <td>-0.398520</td>\n",
              "      <td>-0.026260</td>\n",
              "      <td>-0.009320</td>\n",
              "      <td>0.006460</td>\n",
              "      <td>-0.075860</td>\n",
              "      <td>-0.003340</td>\n",
              "      <td>-0.018100</td>\n",
              "      <td>0.061420</td>\n",
              "      <td>-0.075740</td>\n",
              "      <td>0.026120</td>\n",
              "      <td>-0.081180</td>\n",
              "      <td>0.055300</td>\n",
              "      <td>-0.023480</td>\n",
              "      <td>0.024980</td>\n",
              "      <td>0.018400</td>\n",
              "      <td>-0.015780</td>\n",
              "      <td>0.060780</td>\n",
              "      <td>0.004760</td>\n",
              "      <td>0.040500</td>\n",
              "      <td>-0.019600</td>\n",
              "      <td>-0.036680</td>\n",
              "      <td>-0.050980</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.018060</td>\n",
              "      <td>-0.033200</td>\n",
              "      <td>0.038040</td>\n",
              "      <td>0.015880</td>\n",
              "      <td>-0.085400</td>\n",
              "      <td>0.056460</td>\n",
              "      <td>0.015820</td>\n",
              "      <td>-0.029300</td>\n",
              "      <td>-0.073400</td>\n",
              "      <td>-0.051820</td>\n",
              "      <td>0.199580</td>\n",
              "      <td>-0.021740</td>\n",
              "      <td>-0.001900</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5536</th>\n",
              "      <td>0.013713</td>\n",
              "      <td>-0.016407</td>\n",
              "      <td>-0.069427</td>\n",
              "      <td>-0.011700</td>\n",
              "      <td>0.005907</td>\n",
              "      <td>0.010667</td>\n",
              "      <td>0.012180</td>\n",
              "      <td>0.028887</td>\n",
              "      <td>-0.005393</td>\n",
              "      <td>0.038313</td>\n",
              "      <td>-0.002620</td>\n",
              "      <td>-0.009627</td>\n",
              "      <td>0.003340</td>\n",
              "      <td>-0.017367</td>\n",
              "      <td>-0.013107</td>\n",
              "      <td>0.021360</td>\n",
              "      <td>0.022860</td>\n",
              "      <td>0.041607</td>\n",
              "      <td>-0.003533</td>\n",
              "      <td>-0.022213</td>\n",
              "      <td>-0.083573</td>\n",
              "      <td>-0.000613</td>\n",
              "      <td>0.037260</td>\n",
              "      <td>-0.068167</td>\n",
              "      <td>-0.009887</td>\n",
              "      <td>-0.000893</td>\n",
              "      <td>0.021593</td>\n",
              "      <td>0.047107</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>-0.081673</td>\n",
              "      <td>0.027847</td>\n",
              "      <td>-0.000993</td>\n",
              "      <td>0.047020</td>\n",
              "      <td>-0.024607</td>\n",
              "      <td>0.014193</td>\n",
              "      <td>-0.019733</td>\n",
              "      <td>-0.027533</td>\n",
              "      <td>-0.044407</td>\n",
              "      <td>0.035507</td>\n",
              "      <td>-0.032847</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033413</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>-0.182013</td>\n",
              "      <td>0.009753</td>\n",
              "      <td>0.003567</td>\n",
              "      <td>-0.022813</td>\n",
              "      <td>-0.052687</td>\n",
              "      <td>-0.009633</td>\n",
              "      <td>-0.008640</td>\n",
              "      <td>0.004613</td>\n",
              "      <td>0.022267</td>\n",
              "      <td>-0.031140</td>\n",
              "      <td>0.032507</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>0.072133</td>\n",
              "      <td>-0.023167</td>\n",
              "      <td>0.043813</td>\n",
              "      <td>0.003360</td>\n",
              "      <td>0.068053</td>\n",
              "      <td>0.029093</td>\n",
              "      <td>-0.016560</td>\n",
              "      <td>-0.027627</td>\n",
              "      <td>0.022747</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>-0.013247</td>\n",
              "      <td>0.017487</td>\n",
              "      <td>-0.063173</td>\n",
              "      <td>0.060247</td>\n",
              "      <td>0.062560</td>\n",
              "      <td>0.043380</td>\n",
              "      <td>-0.026667</td>\n",
              "      <td>0.035780</td>\n",
              "      <td>-0.004920</td>\n",
              "      <td>-0.018840</td>\n",
              "      <td>-0.004713</td>\n",
              "      <td>0.126353</td>\n",
              "      <td>0.048653</td>\n",
              "      <td>-0.038813</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5537</th>\n",
              "      <td>-0.007162</td>\n",
              "      <td>0.026894</td>\n",
              "      <td>-0.014887</td>\n",
              "      <td>0.013413</td>\n",
              "      <td>0.022212</td>\n",
              "      <td>-0.037306</td>\n",
              "      <td>0.027094</td>\n",
              "      <td>-0.007219</td>\n",
              "      <td>0.005625</td>\n",
              "      <td>0.003894</td>\n",
              "      <td>0.041969</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>0.048637</td>\n",
              "      <td>0.025325</td>\n",
              "      <td>-0.066700</td>\n",
              "      <td>0.032119</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.025175</td>\n",
              "      <td>-0.033019</td>\n",
              "      <td>-0.044538</td>\n",
              "      <td>-0.081131</td>\n",
              "      <td>0.060294</td>\n",
              "      <td>-0.011031</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.001475</td>\n",
              "      <td>0.011625</td>\n",
              "      <td>0.046750</td>\n",
              "      <td>0.014525</td>\n",
              "      <td>-0.011169</td>\n",
              "      <td>-0.000156</td>\n",
              "      <td>0.013275</td>\n",
              "      <td>0.006906</td>\n",
              "      <td>-0.033150</td>\n",
              "      <td>-0.005800</td>\n",
              "      <td>0.023969</td>\n",
              "      <td>-0.006094</td>\n",
              "      <td>-0.030069</td>\n",
              "      <td>-0.045925</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.021719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046475</td>\n",
              "      <td>-0.036425</td>\n",
              "      <td>-0.015706</td>\n",
              "      <td>-0.198644</td>\n",
              "      <td>-0.000494</td>\n",
              "      <td>0.021700</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>-0.060150</td>\n",
              "      <td>-0.024738</td>\n",
              "      <td>0.016300</td>\n",
              "      <td>0.023762</td>\n",
              "      <td>-0.004181</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>-0.028794</td>\n",
              "      <td>0.012587</td>\n",
              "      <td>0.038769</td>\n",
              "      <td>-0.014481</td>\n",
              "      <td>0.029263</td>\n",
              "      <td>-0.012781</td>\n",
              "      <td>0.003194</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.013881</td>\n",
              "      <td>0.003144</td>\n",
              "      <td>0.021731</td>\n",
              "      <td>-0.010431</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.016075</td>\n",
              "      <td>-0.070900</td>\n",
              "      <td>0.008906</td>\n",
              "      <td>-0.039413</td>\n",
              "      <td>-0.029994</td>\n",
              "      <td>-0.009288</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.012325</td>\n",
              "      <td>0.002912</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>0.145725</td>\n",
              "      <td>0.043256</td>\n",
              "      <td>-0.017325</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5538</th>\n",
              "      <td>-0.003313</td>\n",
              "      <td>0.019009</td>\n",
              "      <td>-0.007722</td>\n",
              "      <td>-0.007883</td>\n",
              "      <td>-0.022217</td>\n",
              "      <td>-0.039978</td>\n",
              "      <td>0.031226</td>\n",
              "      <td>0.013878</td>\n",
              "      <td>-0.010235</td>\n",
              "      <td>-0.038139</td>\n",
              "      <td>0.007061</td>\n",
              "      <td>0.011270</td>\n",
              "      <td>0.018865</td>\n",
              "      <td>-0.043617</td>\n",
              "      <td>-0.014252</td>\n",
              "      <td>-0.045765</td>\n",
              "      <td>0.031639</td>\n",
              "      <td>0.004991</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>-0.010483</td>\n",
              "      <td>-0.070417</td>\n",
              "      <td>0.004178</td>\n",
              "      <td>-0.003317</td>\n",
              "      <td>-0.017417</td>\n",
              "      <td>-0.025661</td>\n",
              "      <td>0.004109</td>\n",
              "      <td>0.030065</td>\n",
              "      <td>0.013239</td>\n",
              "      <td>-0.024430</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>-0.004796</td>\n",
              "      <td>-0.022043</td>\n",
              "      <td>0.015961</td>\n",
              "      <td>0.016913</td>\n",
              "      <td>0.054200</td>\n",
              "      <td>0.010561</td>\n",
              "      <td>-0.023004</td>\n",
              "      <td>-0.008013</td>\n",
              "      <td>0.037526</td>\n",
              "      <td>0.016313</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005952</td>\n",
              "      <td>-0.003243</td>\n",
              "      <td>-0.003187</td>\n",
              "      <td>-0.245183</td>\n",
              "      <td>0.011013</td>\n",
              "      <td>-0.034222</td>\n",
              "      <td>-0.006517</td>\n",
              "      <td>-0.066374</td>\n",
              "      <td>0.001617</td>\n",
              "      <td>0.028326</td>\n",
              "      <td>0.014261</td>\n",
              "      <td>0.019730</td>\n",
              "      <td>-0.014952</td>\n",
              "      <td>-0.026861</td>\n",
              "      <td>0.046535</td>\n",
              "      <td>0.008904</td>\n",
              "      <td>0.014630</td>\n",
              "      <td>0.032109</td>\n",
              "      <td>-0.001778</td>\n",
              "      <td>0.019878</td>\n",
              "      <td>0.047304</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>-0.006183</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>-0.002122</td>\n",
              "      <td>0.003809</td>\n",
              "      <td>-0.001722</td>\n",
              "      <td>-0.022413</td>\n",
              "      <td>0.038500</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.005626</td>\n",
              "      <td>-0.020522</td>\n",
              "      <td>0.007996</td>\n",
              "      <td>0.011543</td>\n",
              "      <td>-0.023609</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>0.129130</td>\n",
              "      <td>0.011243</td>\n",
              "      <td>0.011800</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5539 rows × 301 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...       298       299  Labels\n",
              "0     0.023414  0.005850 -0.041093  ...  0.018993 -0.028171     Yes\n",
              "1     0.021233  0.064300 -0.068433  ...  0.014133 -0.050967     Yes\n",
              "2    -0.047160  0.005480  0.012520  ...  0.079020  0.044420     Yes\n",
              "3    -0.004100  0.014900 -0.017750  ...  0.009250 -0.010300     Yes\n",
              "4     0.052200  0.022450 -0.183250  ... -0.010800  0.036300     Yes\n",
              "...        ...       ...       ...  ...       ...       ...     ...\n",
              "5534  0.012700  0.068600  0.007980  ...  0.041500 -0.024940     NaN\n",
              "5535 -0.096400 -0.005040 -0.030640  ... -0.021740 -0.001900      No\n",
              "5536  0.013713 -0.016407 -0.069427  ...  0.048653 -0.038813      No\n",
              "5537 -0.007162  0.026894 -0.014887  ...  0.043256 -0.017325      No\n",
              "5538 -0.003313  0.019009 -0.007722  ...  0.011243  0.011800     NaN\n",
              "\n",
              "[5539 rows x 301 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8wLrVmUoV5X",
        "colab_type": "text"
      },
      "source": [
        "### I.e Bert pre-trained representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TuVr4wAoV5X",
        "colab_type": "code",
        "outputId": "f359a561-fd8c-40d9-d7b5-f66e715291e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clean_text=import_clean_text()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# Input formating for BERT\n",
        "\n",
        "#tokenization spécifique BERT\n",
        "def tokenization_BERT(tweet,tokenizer):\n",
        "    text=''\n",
        "    for word in tweet:\n",
        "        text=text+' '+word\n",
        "    tokenized_text='[CLS]'+text+' [SEP]'\n",
        "    tokenized_text = tokenizer.tokenize(tokenized_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [0] * len(tokenized_text)\n",
        "    \n",
        "    return(indexed_tokens,segments_ids)\n",
        "\n",
        "##########################################\n",
        "##########################################\n",
        "\n",
        "# Embedding of tweet with BERT\n",
        "def BERT_embedding(tweet,tokenizer):\n",
        "    \n",
        "    #tokenization and conversion of input on tensors\n",
        "    index, segments= tokenization_BERT(tweet,tokenizer)\n",
        "    \n",
        "    tokens_tensor = torch.tensor([index])\n",
        "    segments_tensors = torch.tensor([segments])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "    token_embeddings = token_embeddings.permute(1,0,2)\n",
        "    \n",
        "    token_vecs = encoded_layers[11][0]\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "    \n",
        "    return(list(sentence_embedding.numpy())) #return vector of tweet\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 5617218.91B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTwThLdnoV5c",
        "colab_type": "code",
        "outputId": "cf12c343-2f3f-4bcc-9670-519f8deb4df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Import BERT model\n",
        "\"\"\"\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load pre-trained model (weights)\\nmodel = BertModel.from_pretrained(\\'bert-base-uncased\\')\\n\\n# Put the model in \"evaluation\" mode, meaning feed-forward operation.\\nmodel.eval()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rubel-E1oV5g",
        "colab_type": "code",
        "outputId": "6cd588f1-64c9-42b7-bc8d-6f78f1ea60b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Representation of the first 5 tweets of the database\n",
        "\"\"\"\n",
        "for tweet_index in range(5):\n",
        "    print('Initial tweet : '+df.iloc[tweet_index]['Tweet'])\n",
        "    print(' ')\n",
        "    text=''\n",
        "    for word in clean_text[tweet_index]:\n",
        "        text=text+' '+word\n",
        "    print('Cleaning up tweet  : '+text)\n",
        "    print(' ')\n",
        "    print('First 10 values of the vector representation  : '+str(BERT_embedding(clean_text[tweet_index],tokenizer)[0:10]))\n",
        "    print(' ')\n",
        "    print('#'*20)\n",
        "    print(' ')\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor tweet_index in range(5):\\n    print('Initial tweet : '+df.iloc[tweet_index]['Tweet'])\\n    print(' ')\\n    text=''\\n    for word in clean_text[tweet_index]:\\n        text=text+' '+word\\n    print('Cleaning up tweet  : '+text)\\n    print(' ')\\n    print('First 10 values of the vector representation  : '+str(BERT_embedding(clean_text[tweet_index],tokenizer)[0:10]))\\n    print(' ')\\n    print('#'*20)\\n    print(' ')\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whmDJFhvoV5j",
        "colab_type": "code",
        "outputId": "4962f65f-2825-4b06-aff7-8e0178385098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Representation of the entire database by BERT\n",
        "\"\"\"\n",
        "dataframe={}\n",
        "for tweet_index in tqdm(range(len(clean_text))):\n",
        "    dataframe[tweet_index]=BERT_embedding(clean_text[tweet_index],tokenizer)\n",
        "df_bert=pd.DataFrame(dataframe)\n",
        "df_bert=df_bert.T\n",
        "df_bert['Labels']=list(df.Existence)\n",
        "\n",
        "df_bert.to_csv(\"sample_data/Projet_NLP/representations/bert.csv\")\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataframe={}\\nfor tweet_index in tqdm(range(len(clean_text))):\\n    dataframe[tweet_index]=BERT_embedding(clean_text[tweet_index],tokenizer)\\ndf_bert=pd.DataFrame(dataframe)\\ndf_bert=df_bert.T\\ndf_bert[\\'Labels\\']=list(df.Existence)\\n\\ndf_bert.to_csv(\"sample_data/Projet_NLP/representations/bert.csv\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_9WZDztoV5m",
        "colab_type": "code",
        "outputId": "f36c4747-ffa2-478b-9941-771eaf8aaea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df_bert=pd.read_csv(\"sample_data/Projet_NLP/representations/bert.csv\",index_col=0)\n",
        "\n",
        "df_bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.194920</td>\n",
              "      <td>-0.112881</td>\n",
              "      <td>0.388437</td>\n",
              "      <td>-0.109874</td>\n",
              "      <td>0.041914</td>\n",
              "      <td>-0.137204</td>\n",
              "      <td>0.109907</td>\n",
              "      <td>0.556634</td>\n",
              "      <td>-0.304503</td>\n",
              "      <td>-0.419192</td>\n",
              "      <td>0.252019</td>\n",
              "      <td>-0.271957</td>\n",
              "      <td>-0.236879</td>\n",
              "      <td>0.526489</td>\n",
              "      <td>-0.307396</td>\n",
              "      <td>0.055422</td>\n",
              "      <td>0.146274</td>\n",
              "      <td>0.087574</td>\n",
              "      <td>0.040901</td>\n",
              "      <td>0.402384</td>\n",
              "      <td>0.551161</td>\n",
              "      <td>-0.479833</td>\n",
              "      <td>0.463003</td>\n",
              "      <td>0.361936</td>\n",
              "      <td>0.400630</td>\n",
              "      <td>-0.387164</td>\n",
              "      <td>-0.118802</td>\n",
              "      <td>0.047623</td>\n",
              "      <td>-0.247820</td>\n",
              "      <td>-0.211108</td>\n",
              "      <td>0.111076</td>\n",
              "      <td>0.415706</td>\n",
              "      <td>-0.189024</td>\n",
              "      <td>0.014821</td>\n",
              "      <td>0.022923</td>\n",
              "      <td>-0.065167</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>-0.011446</td>\n",
              "      <td>0.054942</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.517920</td>\n",
              "      <td>-0.177412</td>\n",
              "      <td>0.212346</td>\n",
              "      <td>-0.149375</td>\n",
              "      <td>-0.347338</td>\n",
              "      <td>0.082016</td>\n",
              "      <td>0.426818</td>\n",
              "      <td>0.316553</td>\n",
              "      <td>-0.135410</td>\n",
              "      <td>-0.365129</td>\n",
              "      <td>0.123805</td>\n",
              "      <td>0.176019</td>\n",
              "      <td>-0.524766</td>\n",
              "      <td>-0.143501</td>\n",
              "      <td>0.203182</td>\n",
              "      <td>-0.032161</td>\n",
              "      <td>0.152152</td>\n",
              "      <td>-0.170747</td>\n",
              "      <td>0.022008</td>\n",
              "      <td>0.347057</td>\n",
              "      <td>0.038984</td>\n",
              "      <td>-0.075716</td>\n",
              "      <td>0.391549</td>\n",
              "      <td>0.599759</td>\n",
              "      <td>-0.246354</td>\n",
              "      <td>-0.050916</td>\n",
              "      <td>0.055247</td>\n",
              "      <td>0.025254</td>\n",
              "      <td>-0.527605</td>\n",
              "      <td>0.287880</td>\n",
              "      <td>-0.042977</td>\n",
              "      <td>-0.004823</td>\n",
              "      <td>-0.049975</td>\n",
              "      <td>0.218379</td>\n",
              "      <td>-0.276184</td>\n",
              "      <td>0.220082</td>\n",
              "      <td>-0.796053</td>\n",
              "      <td>0.144477</td>\n",
              "      <td>-0.509098</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.174782</td>\n",
              "      <td>-0.099447</td>\n",
              "      <td>-0.468011</td>\n",
              "      <td>0.108068</td>\n",
              "      <td>-0.028635</td>\n",
              "      <td>-0.427431</td>\n",
              "      <td>0.169761</td>\n",
              "      <td>0.651794</td>\n",
              "      <td>-0.305934</td>\n",
              "      <td>-0.692678</td>\n",
              "      <td>0.446670</td>\n",
              "      <td>-0.077159</td>\n",
              "      <td>-0.395966</td>\n",
              "      <td>0.380424</td>\n",
              "      <td>0.030908</td>\n",
              "      <td>0.439997</td>\n",
              "      <td>0.114907</td>\n",
              "      <td>0.111444</td>\n",
              "      <td>-0.254350</td>\n",
              "      <td>0.307634</td>\n",
              "      <td>-0.115742</td>\n",
              "      <td>-0.550135</td>\n",
              "      <td>0.186734</td>\n",
              "      <td>0.998897</td>\n",
              "      <td>0.396392</td>\n",
              "      <td>-0.016821</td>\n",
              "      <td>-0.074414</td>\n",
              "      <td>0.187305</td>\n",
              "      <td>-0.107318</td>\n",
              "      <td>-0.204798</td>\n",
              "      <td>0.547328</td>\n",
              "      <td>0.211427</td>\n",
              "      <td>-0.449972</td>\n",
              "      <td>0.108926</td>\n",
              "      <td>0.248740</td>\n",
              "      <td>0.163986</td>\n",
              "      <td>-0.071117</td>\n",
              "      <td>0.188881</td>\n",
              "      <td>0.104071</td>\n",
              "      <td>1.008380</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.514333</td>\n",
              "      <td>-0.142747</td>\n",
              "      <td>-0.047600</td>\n",
              "      <td>0.084034</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>-0.190112</td>\n",
              "      <td>0.715412</td>\n",
              "      <td>-0.064051</td>\n",
              "      <td>0.253936</td>\n",
              "      <td>-0.350667</td>\n",
              "      <td>0.607856</td>\n",
              "      <td>-0.335587</td>\n",
              "      <td>-0.577345</td>\n",
              "      <td>-0.078113</td>\n",
              "      <td>0.162958</td>\n",
              "      <td>0.047399</td>\n",
              "      <td>0.217143</td>\n",
              "      <td>-0.296648</td>\n",
              "      <td>-0.001054</td>\n",
              "      <td>0.060962</td>\n",
              "      <td>-0.047100</td>\n",
              "      <td>0.122093</td>\n",
              "      <td>0.166842</td>\n",
              "      <td>-0.670087</td>\n",
              "      <td>-0.160917</td>\n",
              "      <td>-0.276417</td>\n",
              "      <td>-0.319979</td>\n",
              "      <td>0.224017</td>\n",
              "      <td>-0.154211</td>\n",
              "      <td>0.446409</td>\n",
              "      <td>-0.023003</td>\n",
              "      <td>-0.002153</td>\n",
              "      <td>-0.354740</td>\n",
              "      <td>0.397673</td>\n",
              "      <td>-0.244706</td>\n",
              "      <td>0.033439</td>\n",
              "      <td>-0.483247</td>\n",
              "      <td>-0.173934</td>\n",
              "      <td>-0.337515</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.039530</td>\n",
              "      <td>-0.073837</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>0.360194</td>\n",
              "      <td>0.181992</td>\n",
              "      <td>-0.366591</td>\n",
              "      <td>0.008656</td>\n",
              "      <td>0.453784</td>\n",
              "      <td>-0.260378</td>\n",
              "      <td>-0.490275</td>\n",
              "      <td>0.320898</td>\n",
              "      <td>-0.041053</td>\n",
              "      <td>-0.256328</td>\n",
              "      <td>0.435963</td>\n",
              "      <td>-0.243160</td>\n",
              "      <td>0.148085</td>\n",
              "      <td>0.136008</td>\n",
              "      <td>0.005530</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.101418</td>\n",
              "      <td>0.327073</td>\n",
              "      <td>-0.209603</td>\n",
              "      <td>0.081753</td>\n",
              "      <td>0.494403</td>\n",
              "      <td>0.239737</td>\n",
              "      <td>0.030100</td>\n",
              "      <td>0.039815</td>\n",
              "      <td>0.365238</td>\n",
              "      <td>-0.033394</td>\n",
              "      <td>-0.119665</td>\n",
              "      <td>0.218011</td>\n",
              "      <td>0.111627</td>\n",
              "      <td>-0.171564</td>\n",
              "      <td>-0.326108</td>\n",
              "      <td>0.220805</td>\n",
              "      <td>-0.006152</td>\n",
              "      <td>0.087125</td>\n",
              "      <td>-0.174394</td>\n",
              "      <td>-0.246253</td>\n",
              "      <td>0.542751</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.497499</td>\n",
              "      <td>-0.074117</td>\n",
              "      <td>-0.295007</td>\n",
              "      <td>0.233488</td>\n",
              "      <td>-0.015049</td>\n",
              "      <td>0.087177</td>\n",
              "      <td>0.589194</td>\n",
              "      <td>0.057009</td>\n",
              "      <td>0.326338</td>\n",
              "      <td>-0.230939</td>\n",
              "      <td>0.226612</td>\n",
              "      <td>-0.063951</td>\n",
              "      <td>-0.332705</td>\n",
              "      <td>0.061205</td>\n",
              "      <td>0.064967</td>\n",
              "      <td>-0.170379</td>\n",
              "      <td>0.057267</td>\n",
              "      <td>-0.206765</td>\n",
              "      <td>-0.269427</td>\n",
              "      <td>0.197632</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>-0.036466</td>\n",
              "      <td>0.133158</td>\n",
              "      <td>-0.242483</td>\n",
              "      <td>-0.135324</td>\n",
              "      <td>-0.048510</td>\n",
              "      <td>0.052693</td>\n",
              "      <td>0.026043</td>\n",
              "      <td>-0.172240</td>\n",
              "      <td>0.300640</td>\n",
              "      <td>0.075246</td>\n",
              "      <td>0.057701</td>\n",
              "      <td>-0.087006</td>\n",
              "      <td>0.320335</td>\n",
              "      <td>-0.269148</td>\n",
              "      <td>0.119730</td>\n",
              "      <td>-0.637774</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>-0.097225</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.264160</td>\n",
              "      <td>-0.126185</td>\n",
              "      <td>-0.099923</td>\n",
              "      <td>0.301749</td>\n",
              "      <td>0.262108</td>\n",
              "      <td>-0.213265</td>\n",
              "      <td>0.108956</td>\n",
              "      <td>0.407808</td>\n",
              "      <td>-0.187299</td>\n",
              "      <td>-0.148701</td>\n",
              "      <td>0.129526</td>\n",
              "      <td>-0.059863</td>\n",
              "      <td>-0.178751</td>\n",
              "      <td>0.254281</td>\n",
              "      <td>-0.235901</td>\n",
              "      <td>0.189875</td>\n",
              "      <td>0.200746</td>\n",
              "      <td>-0.079830</td>\n",
              "      <td>-0.029836</td>\n",
              "      <td>0.138033</td>\n",
              "      <td>0.085178</td>\n",
              "      <td>-0.489360</td>\n",
              "      <td>0.004958</td>\n",
              "      <td>0.595310</td>\n",
              "      <td>0.558241</td>\n",
              "      <td>-0.024622</td>\n",
              "      <td>0.104344</td>\n",
              "      <td>0.171782</td>\n",
              "      <td>0.078880</td>\n",
              "      <td>-0.180526</td>\n",
              "      <td>0.314832</td>\n",
              "      <td>0.288839</td>\n",
              "      <td>-0.447254</td>\n",
              "      <td>-0.247673</td>\n",
              "      <td>-0.069581</td>\n",
              "      <td>0.256659</td>\n",
              "      <td>-0.092849</td>\n",
              "      <td>-0.087451</td>\n",
              "      <td>-0.037326</td>\n",
              "      <td>0.546149</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.718421</td>\n",
              "      <td>-0.124272</td>\n",
              "      <td>0.095470</td>\n",
              "      <td>-0.258871</td>\n",
              "      <td>-0.351159</td>\n",
              "      <td>0.100994</td>\n",
              "      <td>0.730566</td>\n",
              "      <td>0.250747</td>\n",
              "      <td>0.226312</td>\n",
              "      <td>-0.042832</td>\n",
              "      <td>0.288091</td>\n",
              "      <td>0.042168</td>\n",
              "      <td>-0.226615</td>\n",
              "      <td>-0.114359</td>\n",
              "      <td>-0.058647</td>\n",
              "      <td>-0.349131</td>\n",
              "      <td>0.046755</td>\n",
              "      <td>-0.153729</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>-0.061275</td>\n",
              "      <td>0.071258</td>\n",
              "      <td>0.258433</td>\n",
              "      <td>0.517141</td>\n",
              "      <td>-0.003620</td>\n",
              "      <td>-0.321849</td>\n",
              "      <td>-0.138697</td>\n",
              "      <td>-0.092143</td>\n",
              "      <td>-0.226226</td>\n",
              "      <td>-0.162514</td>\n",
              "      <td>0.187495</td>\n",
              "      <td>-0.155414</td>\n",
              "      <td>-0.248788</td>\n",
              "      <td>-0.342570</td>\n",
              "      <td>0.218711</td>\n",
              "      <td>-0.174955</td>\n",
              "      <td>-0.092277</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>-0.256308</td>\n",
              "      <td>-0.358489</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.078526</td>\n",
              "      <td>0.177795</td>\n",
              "      <td>0.488374</td>\n",
              "      <td>0.213472</td>\n",
              "      <td>0.312748</td>\n",
              "      <td>-0.280140</td>\n",
              "      <td>0.397646</td>\n",
              "      <td>0.429720</td>\n",
              "      <td>-0.310175</td>\n",
              "      <td>-0.554000</td>\n",
              "      <td>0.416621</td>\n",
              "      <td>-0.205863</td>\n",
              "      <td>-0.272646</td>\n",
              "      <td>0.641862</td>\n",
              "      <td>-0.203235</td>\n",
              "      <td>0.198760</td>\n",
              "      <td>0.102769</td>\n",
              "      <td>0.142609</td>\n",
              "      <td>0.053375</td>\n",
              "      <td>0.513095</td>\n",
              "      <td>0.122429</td>\n",
              "      <td>-0.219416</td>\n",
              "      <td>0.323260</td>\n",
              "      <td>0.229268</td>\n",
              "      <td>0.257613</td>\n",
              "      <td>-0.268448</td>\n",
              "      <td>-0.145573</td>\n",
              "      <td>0.300857</td>\n",
              "      <td>-0.107836</td>\n",
              "      <td>-0.020201</td>\n",
              "      <td>0.050221</td>\n",
              "      <td>0.425761</td>\n",
              "      <td>-0.395963</td>\n",
              "      <td>0.204856</td>\n",
              "      <td>-0.205492</td>\n",
              "      <td>-0.211314</td>\n",
              "      <td>-0.293029</td>\n",
              "      <td>0.021693</td>\n",
              "      <td>-0.125359</td>\n",
              "      <td>0.275785</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.224076</td>\n",
              "      <td>-0.153711</td>\n",
              "      <td>-0.047315</td>\n",
              "      <td>0.056110</td>\n",
              "      <td>-0.149812</td>\n",
              "      <td>-0.269768</td>\n",
              "      <td>0.104226</td>\n",
              "      <td>0.094469</td>\n",
              "      <td>0.195848</td>\n",
              "      <td>-0.287592</td>\n",
              "      <td>0.122988</td>\n",
              "      <td>-0.076258</td>\n",
              "      <td>0.006481</td>\n",
              "      <td>-0.076061</td>\n",
              "      <td>-0.167218</td>\n",
              "      <td>-0.230914</td>\n",
              "      <td>0.157543</td>\n",
              "      <td>0.037752</td>\n",
              "      <td>0.334316</td>\n",
              "      <td>0.358273</td>\n",
              "      <td>0.335246</td>\n",
              "      <td>0.206793</td>\n",
              "      <td>0.255253</td>\n",
              "      <td>0.083222</td>\n",
              "      <td>-0.219075</td>\n",
              "      <td>-0.026344</td>\n",
              "      <td>0.010851</td>\n",
              "      <td>-0.052550</td>\n",
              "      <td>-0.474309</td>\n",
              "      <td>0.250726</td>\n",
              "      <td>-0.093388</td>\n",
              "      <td>-0.008426</td>\n",
              "      <td>0.207753</td>\n",
              "      <td>0.094225</td>\n",
              "      <td>-0.148403</td>\n",
              "      <td>0.101668</td>\n",
              "      <td>-0.530785</td>\n",
              "      <td>0.069791</td>\n",
              "      <td>-0.241589</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5534</th>\n",
              "      <td>-0.400154</td>\n",
              "      <td>-0.256461</td>\n",
              "      <td>-0.183564</td>\n",
              "      <td>0.033262</td>\n",
              "      <td>0.159400</td>\n",
              "      <td>-0.255320</td>\n",
              "      <td>0.377321</td>\n",
              "      <td>0.061936</td>\n",
              "      <td>-0.094525</td>\n",
              "      <td>-0.187200</td>\n",
              "      <td>0.163531</td>\n",
              "      <td>-0.427981</td>\n",
              "      <td>-0.142525</td>\n",
              "      <td>0.282750</td>\n",
              "      <td>-0.291043</td>\n",
              "      <td>0.194158</td>\n",
              "      <td>0.151796</td>\n",
              "      <td>-0.041816</td>\n",
              "      <td>-0.027638</td>\n",
              "      <td>0.099507</td>\n",
              "      <td>0.128419</td>\n",
              "      <td>-0.267145</td>\n",
              "      <td>-0.081368</td>\n",
              "      <td>0.255303</td>\n",
              "      <td>0.062360</td>\n",
              "      <td>-0.162025</td>\n",
              "      <td>-0.388837</td>\n",
              "      <td>0.250527</td>\n",
              "      <td>-0.218616</td>\n",
              "      <td>0.262326</td>\n",
              "      <td>0.087177</td>\n",
              "      <td>-0.024484</td>\n",
              "      <td>-0.159161</td>\n",
              "      <td>0.155869</td>\n",
              "      <td>-0.261168</td>\n",
              "      <td>-0.020689</td>\n",
              "      <td>0.115769</td>\n",
              "      <td>0.142134</td>\n",
              "      <td>-0.243363</td>\n",
              "      <td>0.514497</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013923</td>\n",
              "      <td>0.069348</td>\n",
              "      <td>0.282298</td>\n",
              "      <td>-0.178568</td>\n",
              "      <td>-0.213402</td>\n",
              "      <td>-0.400905</td>\n",
              "      <td>0.138092</td>\n",
              "      <td>0.240041</td>\n",
              "      <td>0.183354</td>\n",
              "      <td>-0.329683</td>\n",
              "      <td>0.344562</td>\n",
              "      <td>-0.202885</td>\n",
              "      <td>0.177869</td>\n",
              "      <td>0.223293</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>-0.334282</td>\n",
              "      <td>0.186802</td>\n",
              "      <td>-0.369099</td>\n",
              "      <td>0.030811</td>\n",
              "      <td>-0.116344</td>\n",
              "      <td>-0.019728</td>\n",
              "      <td>-0.096278</td>\n",
              "      <td>0.008148</td>\n",
              "      <td>-0.287904</td>\n",
              "      <td>0.026578</td>\n",
              "      <td>-0.266591</td>\n",
              "      <td>-0.112258</td>\n",
              "      <td>-0.041875</td>\n",
              "      <td>-0.654519</td>\n",
              "      <td>0.004374</td>\n",
              "      <td>0.181568</td>\n",
              "      <td>0.113463</td>\n",
              "      <td>-0.217428</td>\n",
              "      <td>-0.007062</td>\n",
              "      <td>-0.436011</td>\n",
              "      <td>-0.095360</td>\n",
              "      <td>-0.296756</td>\n",
              "      <td>0.257059</td>\n",
              "      <td>-0.234672</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5535</th>\n",
              "      <td>0.051658</td>\n",
              "      <td>-0.054771</td>\n",
              "      <td>0.298856</td>\n",
              "      <td>0.032705</td>\n",
              "      <td>0.214817</td>\n",
              "      <td>-0.419218</td>\n",
              "      <td>-0.005545</td>\n",
              "      <td>0.701181</td>\n",
              "      <td>-0.368737</td>\n",
              "      <td>-0.288521</td>\n",
              "      <td>0.317005</td>\n",
              "      <td>0.039178</td>\n",
              "      <td>-0.269730</td>\n",
              "      <td>0.420543</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.282510</td>\n",
              "      <td>-0.201628</td>\n",
              "      <td>0.063244</td>\n",
              "      <td>0.171288</td>\n",
              "      <td>0.440046</td>\n",
              "      <td>0.412295</td>\n",
              "      <td>0.014153</td>\n",
              "      <td>-0.152080</td>\n",
              "      <td>0.189636</td>\n",
              "      <td>0.291203</td>\n",
              "      <td>-0.186579</td>\n",
              "      <td>-0.158608</td>\n",
              "      <td>0.028100</td>\n",
              "      <td>-0.080189</td>\n",
              "      <td>-0.355684</td>\n",
              "      <td>-0.057231</td>\n",
              "      <td>0.240174</td>\n",
              "      <td>-0.170516</td>\n",
              "      <td>-0.120881</td>\n",
              "      <td>0.083689</td>\n",
              "      <td>0.163344</td>\n",
              "      <td>-0.322119</td>\n",
              "      <td>0.162076</td>\n",
              "      <td>0.148084</td>\n",
              "      <td>0.381326</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.433839</td>\n",
              "      <td>0.060247</td>\n",
              "      <td>0.076592</td>\n",
              "      <td>-0.146814</td>\n",
              "      <td>-0.001967</td>\n",
              "      <td>0.292328</td>\n",
              "      <td>0.488722</td>\n",
              "      <td>-0.042474</td>\n",
              "      <td>0.158971</td>\n",
              "      <td>-0.246996</td>\n",
              "      <td>0.220223</td>\n",
              "      <td>-0.084074</td>\n",
              "      <td>-0.253511</td>\n",
              "      <td>-0.062238</td>\n",
              "      <td>-0.211005</td>\n",
              "      <td>-0.467597</td>\n",
              "      <td>-0.015535</td>\n",
              "      <td>-0.333923</td>\n",
              "      <td>0.147085</td>\n",
              "      <td>0.019582</td>\n",
              "      <td>-0.065438</td>\n",
              "      <td>0.196988</td>\n",
              "      <td>0.184509</td>\n",
              "      <td>-0.203195</td>\n",
              "      <td>-0.273199</td>\n",
              "      <td>-0.051144</td>\n",
              "      <td>0.014902</td>\n",
              "      <td>-0.043810</td>\n",
              "      <td>-0.235965</td>\n",
              "      <td>0.113421</td>\n",
              "      <td>0.209300</td>\n",
              "      <td>-0.141448</td>\n",
              "      <td>-0.260576</td>\n",
              "      <td>0.385859</td>\n",
              "      <td>-0.042413</td>\n",
              "      <td>-0.044953</td>\n",
              "      <td>-0.381793</td>\n",
              "      <td>0.155324</td>\n",
              "      <td>-0.293402</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5536</th>\n",
              "      <td>-0.120506</td>\n",
              "      <td>0.175940</td>\n",
              "      <td>0.014781</td>\n",
              "      <td>-0.156842</td>\n",
              "      <td>0.175269</td>\n",
              "      <td>0.039736</td>\n",
              "      <td>0.083392</td>\n",
              "      <td>0.751514</td>\n",
              "      <td>0.105191</td>\n",
              "      <td>-0.167486</td>\n",
              "      <td>0.314353</td>\n",
              "      <td>-0.295102</td>\n",
              "      <td>-0.128456</td>\n",
              "      <td>0.434739</td>\n",
              "      <td>-0.184771</td>\n",
              "      <td>0.574360</td>\n",
              "      <td>0.184420</td>\n",
              "      <td>-0.105185</td>\n",
              "      <td>-0.288563</td>\n",
              "      <td>0.404593</td>\n",
              "      <td>0.161561</td>\n",
              "      <td>-0.253180</td>\n",
              "      <td>0.033822</td>\n",
              "      <td>0.329663</td>\n",
              "      <td>0.610937</td>\n",
              "      <td>0.207025</td>\n",
              "      <td>-0.088484</td>\n",
              "      <td>-0.030551</td>\n",
              "      <td>0.159374</td>\n",
              "      <td>-0.107732</td>\n",
              "      <td>0.437104</td>\n",
              "      <td>-0.097171</td>\n",
              "      <td>0.026479</td>\n",
              "      <td>-0.107998</td>\n",
              "      <td>-0.260738</td>\n",
              "      <td>-0.105141</td>\n",
              "      <td>-0.143778</td>\n",
              "      <td>-0.046506</td>\n",
              "      <td>-0.112839</td>\n",
              "      <td>0.481101</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.177142</td>\n",
              "      <td>-0.414978</td>\n",
              "      <td>0.355021</td>\n",
              "      <td>-0.529804</td>\n",
              "      <td>0.140888</td>\n",
              "      <td>-0.113431</td>\n",
              "      <td>0.605701</td>\n",
              "      <td>0.066830</td>\n",
              "      <td>-0.000781</td>\n",
              "      <td>-0.207028</td>\n",
              "      <td>0.183629</td>\n",
              "      <td>0.110420</td>\n",
              "      <td>-0.184507</td>\n",
              "      <td>0.117419</td>\n",
              "      <td>-0.182778</td>\n",
              "      <td>-0.178086</td>\n",
              "      <td>0.259972</td>\n",
              "      <td>-0.320970</td>\n",
              "      <td>0.004307</td>\n",
              "      <td>0.131893</td>\n",
              "      <td>0.423030</td>\n",
              "      <td>-0.350288</td>\n",
              "      <td>-0.105881</td>\n",
              "      <td>0.515543</td>\n",
              "      <td>0.052342</td>\n",
              "      <td>-0.262308</td>\n",
              "      <td>-0.228609</td>\n",
              "      <td>-0.106319</td>\n",
              "      <td>-0.407964</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.151757</td>\n",
              "      <td>-0.134946</td>\n",
              "      <td>-0.152878</td>\n",
              "      <td>-0.033997</td>\n",
              "      <td>-0.350957</td>\n",
              "      <td>-0.388650</td>\n",
              "      <td>-0.116329</td>\n",
              "      <td>0.089427</td>\n",
              "      <td>-0.230295</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5537</th>\n",
              "      <td>0.073636</td>\n",
              "      <td>0.110835</td>\n",
              "      <td>0.429038</td>\n",
              "      <td>0.118876</td>\n",
              "      <td>0.007961</td>\n",
              "      <td>-0.031842</td>\n",
              "      <td>-0.030791</td>\n",
              "      <td>0.188152</td>\n",
              "      <td>-0.325286</td>\n",
              "      <td>-0.172276</td>\n",
              "      <td>0.274758</td>\n",
              "      <td>-0.115824</td>\n",
              "      <td>-0.294030</td>\n",
              "      <td>0.438202</td>\n",
              "      <td>-0.260124</td>\n",
              "      <td>0.056396</td>\n",
              "      <td>-0.089375</td>\n",
              "      <td>-0.037727</td>\n",
              "      <td>-0.255182</td>\n",
              "      <td>0.159772</td>\n",
              "      <td>0.520437</td>\n",
              "      <td>0.006253</td>\n",
              "      <td>-0.016955</td>\n",
              "      <td>0.417002</td>\n",
              "      <td>0.193030</td>\n",
              "      <td>0.035382</td>\n",
              "      <td>-0.083800</td>\n",
              "      <td>-0.134781</td>\n",
              "      <td>0.072736</td>\n",
              "      <td>0.217197</td>\n",
              "      <td>0.096567</td>\n",
              "      <td>0.152830</td>\n",
              "      <td>-0.378708</td>\n",
              "      <td>-0.050753</td>\n",
              "      <td>0.063839</td>\n",
              "      <td>0.173512</td>\n",
              "      <td>-0.251636</td>\n",
              "      <td>0.073689</td>\n",
              "      <td>-0.209155</td>\n",
              "      <td>0.272125</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.354241</td>\n",
              "      <td>-0.033575</td>\n",
              "      <td>-0.104087</td>\n",
              "      <td>-0.042819</td>\n",
              "      <td>0.052147</td>\n",
              "      <td>0.095681</td>\n",
              "      <td>0.162859</td>\n",
              "      <td>-0.105484</td>\n",
              "      <td>0.091571</td>\n",
              "      <td>-0.100593</td>\n",
              "      <td>0.156326</td>\n",
              "      <td>-0.050897</td>\n",
              "      <td>-0.159961</td>\n",
              "      <td>0.117555</td>\n",
              "      <td>-0.217260</td>\n",
              "      <td>-0.300964</td>\n",
              "      <td>-0.005641</td>\n",
              "      <td>-0.435078</td>\n",
              "      <td>0.097553</td>\n",
              "      <td>0.238863</td>\n",
              "      <td>-0.018140</td>\n",
              "      <td>0.421059</td>\n",
              "      <td>0.144242</td>\n",
              "      <td>0.192435</td>\n",
              "      <td>-0.203151</td>\n",
              "      <td>-0.097250</td>\n",
              "      <td>-0.025309</td>\n",
              "      <td>-0.290886</td>\n",
              "      <td>-0.583427</td>\n",
              "      <td>0.031632</td>\n",
              "      <td>0.337875</td>\n",
              "      <td>-0.096466</td>\n",
              "      <td>-0.481865</td>\n",
              "      <td>0.066156</td>\n",
              "      <td>-0.208680</td>\n",
              "      <td>0.082813</td>\n",
              "      <td>-0.188787</td>\n",
              "      <td>0.365409</td>\n",
              "      <td>0.169572</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5538</th>\n",
              "      <td>-0.029584</td>\n",
              "      <td>0.114278</td>\n",
              "      <td>0.226568</td>\n",
              "      <td>0.230094</td>\n",
              "      <td>-0.027433</td>\n",
              "      <td>-0.308322</td>\n",
              "      <td>0.019580</td>\n",
              "      <td>0.600823</td>\n",
              "      <td>-0.277417</td>\n",
              "      <td>-0.068723</td>\n",
              "      <td>0.233457</td>\n",
              "      <td>-0.377044</td>\n",
              "      <td>-0.238131</td>\n",
              "      <td>0.485302</td>\n",
              "      <td>0.030457</td>\n",
              "      <td>0.347252</td>\n",
              "      <td>-0.241651</td>\n",
              "      <td>0.112304</td>\n",
              "      <td>-0.187540</td>\n",
              "      <td>0.345615</td>\n",
              "      <td>0.077358</td>\n",
              "      <td>-0.034966</td>\n",
              "      <td>-0.059033</td>\n",
              "      <td>0.361709</td>\n",
              "      <td>0.233149</td>\n",
              "      <td>0.235551</td>\n",
              "      <td>-0.144961</td>\n",
              "      <td>-0.026560</td>\n",
              "      <td>-0.266609</td>\n",
              "      <td>0.293908</td>\n",
              "      <td>0.472169</td>\n",
              "      <td>0.177284</td>\n",
              "      <td>-0.256328</td>\n",
              "      <td>-0.100595</td>\n",
              "      <td>0.061316</td>\n",
              "      <td>-0.069822</td>\n",
              "      <td>-0.120298</td>\n",
              "      <td>-0.319250</td>\n",
              "      <td>-0.226260</td>\n",
              "      <td>0.380677</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.296492</td>\n",
              "      <td>-0.250046</td>\n",
              "      <td>-0.113969</td>\n",
              "      <td>-0.310162</td>\n",
              "      <td>0.266566</td>\n",
              "      <td>0.242929</td>\n",
              "      <td>0.494990</td>\n",
              "      <td>0.119118</td>\n",
              "      <td>0.190173</td>\n",
              "      <td>-0.132014</td>\n",
              "      <td>0.327539</td>\n",
              "      <td>0.028799</td>\n",
              "      <td>0.025754</td>\n",
              "      <td>0.105554</td>\n",
              "      <td>-0.272319</td>\n",
              "      <td>-0.118722</td>\n",
              "      <td>0.008730</td>\n",
              "      <td>-0.679434</td>\n",
              "      <td>-0.161445</td>\n",
              "      <td>0.258398</td>\n",
              "      <td>0.043442</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.447369</td>\n",
              "      <td>0.621565</td>\n",
              "      <td>-0.105195</td>\n",
              "      <td>0.105727</td>\n",
              "      <td>-0.171493</td>\n",
              "      <td>0.013880</td>\n",
              "      <td>-0.194170</td>\n",
              "      <td>-0.102389</td>\n",
              "      <td>0.241541</td>\n",
              "      <td>0.066769</td>\n",
              "      <td>-0.011381</td>\n",
              "      <td>0.492085</td>\n",
              "      <td>-0.038748</td>\n",
              "      <td>0.007048</td>\n",
              "      <td>-0.074291</td>\n",
              "      <td>0.399871</td>\n",
              "      <td>-0.195889</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5539 rows × 769 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...       766       767  Labels\n",
              "0    -0.194920 -0.112881  0.388437  ...  0.144477 -0.509098     Yes\n",
              "1    -0.174782 -0.099447 -0.468011  ... -0.173934 -0.337515     Yes\n",
              "2     0.039530 -0.073837 -0.087995  ...  0.000480 -0.097225     Yes\n",
              "3    -0.264160 -0.126185 -0.099923  ... -0.256308 -0.358489     Yes\n",
              "4     0.078526  0.177795  0.488374  ...  0.069791 -0.241589     Yes\n",
              "...        ...       ...       ...  ...       ...       ...     ...\n",
              "5534 -0.400154 -0.256461 -0.183564  ...  0.257059 -0.234672     NaN\n",
              "5535  0.051658 -0.054771  0.298856  ...  0.155324 -0.293402      No\n",
              "5536 -0.120506  0.175940  0.014781  ...  0.089427 -0.230295      No\n",
              "5537  0.073636  0.110835  0.429038  ...  0.365409  0.169572      No\n",
              "5538 -0.029584  0.114278  0.226568  ...  0.399871 -0.195889     NaN\n",
              "\n",
              "[5539 rows x 769 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b321W2LooV5w",
        "colab_type": "text"
      },
      "source": [
        "# II. Tweets classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT0kEJE8oV5x",
        "colab_type": "text"
      },
      "source": [
        "The first objective of this part is to estimate the performance of our different representations. The idea is to train classification models to see if they are able to discern people for/against global warming from the different representations.\n",
        "\n",
        "\n",
        "We used classic classification models such as RandomForest, XGBoost and SVM.\n",
        "\n",
        "To get an idea of the significance of the results, we used a 'representation-free' model in the sense that it does not depend on the representation. It is the LSTM (Long short-term memory) neural network which takes in input directly the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwqRE-vmoV5y",
        "colab_type": "text"
      },
      "source": [
        "###### II.a Database used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q7kqwcVoV5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training databases\n",
        "\n",
        "## List of training databases\n",
        "dic_representations_init={'tf_idf':df_tfidf,'word2vec':df_word2vec,'fast2vec_cluster':df_fast2vec_cluster,\n",
        "                        'fast2vec_mean':df_fast2vec_mean, 'bert':df_bert} \n",
        "dic_representations={}\n",
        "dic_representations_labels={}\n",
        "\n",
        "### fonction\n",
        "def conserv_YN(base_):\n",
        "    base_=base_[(base_.Labels=='Yes')|(base_.Labels=='No')]\n",
        "    labels=list(base_.Labels)\n",
        "    base_=base_[[str(x) for x in range((base_.shape[1]-1))]]\n",
        "    return([base_,labels])\n",
        "\n",
        "### applications aux bases \n",
        "for base_name in dic_representations_init.keys():\n",
        "    dic_representations[base_name],labels=conserv_YN(dic_representations_init[base_name])[0],conserv_YN(dic_representations_init[base_name])[1]\n",
        "    dic_representations_labels[base_name]=[(labels[tweet]=='Yes')*1 for tweet in range(len(labels))]\n",
        "\n",
        "### labels=0 si 'No' \n",
        "###labels=1 si 'Yes'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9UudCeyoV53",
        "colab_type": "text"
      },
      "source": [
        "##### II.b Classical classification algorithms: XGboost, RandomForest, SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-RF3Z2AoV53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creation of train/test bases\n",
        "\n",
        "def X_split(base_name):\n",
        "    return(train_test_split(dic_representations[base_name],dic_representations_labels[base_name], test_size=0.5))\n",
        "\n",
        "dic_Xtrain={}\n",
        "dic_Ytrain={}\n",
        "dic_Xtest={}\n",
        "dic_Ytest={}\n",
        "for base_name in dic_representations.keys():\n",
        "    Xtrain,Xtest,Ytrain,Ytest=X_split(base_name)\n",
        "    dic_Xtrain[base_name]=Xtrain.reset_index()\n",
        "    dic_Ytrain[base_name]=Ytrain\n",
        "    dic_Xtest[base_name]=Xtest.reset_index()\n",
        "    dic_Ytest[base_name]=Ytest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpuP8RCxoV58",
        "colab_type": "code",
        "outputId": "f4afedc3-bcfd-4d9f-825d-9cfb23c5a3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Learning the models for each representation and saving the results on the test\n",
        "\n",
        "## fonction\n",
        "def models_XGB_RF_SVM(name_representation,name_folder_tosave_models):\n",
        "    \n",
        "    df_representation={}\n",
        "\n",
        "    ##RandomForest apprentissage\n",
        "    print('Random Forest learning...')\n",
        "    t=time.time()\n",
        "    rfc = RandomForestClassifier(n_estimators=100)\n",
        "    rfc.fit(dic_Xtrain[name_representation],dic_Ytrain[name_representation])\n",
        "    print('Random Forest learning done ({}s to learn)'.format(str(round(time.time()-t,0))))\n",
        "    with open('sample_data/Projet_NLP/'+name_folder_tosave_models+'/{}_RF_model.p'.format(name_representation), 'wb') as fp:\n",
        "        pickle.dump(rfc, fp)\n",
        "    print(' ')\n",
        "    \n",
        "    ##RandomForest predictions et scores\n",
        "    predictions=rfc.predict(dic_Xtest[name_representation])\n",
        "    precision_yes=precision_score(dic_Ytest[name_representation], predictions,average=None)[1]\n",
        "    precision_no=precision_score(dic_Ytest[name_representation], predictions,average=None)[0]\n",
        "    precision_gen=precision_score(dic_Ytest[name_representation], predictions, average='macro')\n",
        "    f1_yes=f1_score(dic_Ytest[name_representation], predictions,average=None)[1]\n",
        "    f1_no=f1_score(dic_Ytest[name_representation], predictions,average=None)[0]\n",
        "    f1_gen=f1_score(dic_Ytest[name_representation], predictions, average='macro')\n",
        "    df_representation['RandomForest_{}'.format(name_representation)]=[precision_yes,precision_no,precision_gen,f1_yes,f1_no,f1_gen]\n",
        "    \n",
        "\n",
        "    ##XgBoost Classifier\n",
        "    print('XGBoost learning...')\n",
        "    t=time.time()\n",
        "    clf = xgb.XGBClassifier()\n",
        "    clf.fit(dic_Xtrain[name_representation],dic_Ytrain[name_representation])\n",
        "    print('XGBoost learning done ({}s to learn)'.format(str(round(time.time()-t,0))))\n",
        "    with open('sample_data/Projet_NLP/'+name_folder_tosave_models+'/{}_XGB_model.p'.format(name_representation), 'wb') as fp:\n",
        "        pickle.dump(clf, fp)\n",
        "    print(' ')\n",
        "    \n",
        "    ##XgBoost Classifier predictions et scores\n",
        "    predictions=clf.predict(dic_Xtest[name_representation])\n",
        "    precision_yes=precision_score(dic_Ytest[name_representation], predictions,average=None)[1]\n",
        "    precision_no=precision_score(dic_Ytest[name_representation], predictions,average=None)[0]\n",
        "    precision_gen=precision_score(dic_Ytest[name_representation], predictions, average='macro')\n",
        "    f1_yes=f1_score(dic_Ytest[name_representation], predictions,average=None)[1]\n",
        "    f1_no=f1_score(dic_Ytest[name_representation], predictions,average=None)[0]\n",
        "    f1_gen=f1_score(dic_Ytest[name_representation], predictions, average='macro')\n",
        "    df_representation['XGBoost_{}'.format(name_representation)]=[precision_yes,precision_no,precision_gen,f1_yes,f1_no,f1_gen]\n",
        "\n",
        "\n",
        "    ##SVM\n",
        "    print('SVM learning...')\n",
        "    t=time.time()\n",
        "    svm = SVC()\n",
        "    svm.fit(dic_Xtrain[name_representation],dic_Ytrain[name_representation])\n",
        "    print('SVM learning done ({}s to learn)'.format(str(round(time.time()-t,0))))\n",
        "    with open('sample_data/Projet_NLP/'+name_folder_tosave_models+'/{}_SVM_model.p'.format(name_representation), 'wb') as fp:\n",
        "        pickle.dump(svm, fp)\n",
        "    print(' ')\n",
        "    print('#'*60)\n",
        "    print('#'*60)\n",
        "    print(' ')\n",
        "    print('Saving the results...')\n",
        "    ##SVM Classifier predictions et scores\n",
        "    predictions=svm.predict(dic_Xtest[name_representation])\n",
        "    precision_yes=precision_score(dic_Ytest[name_representation], predictions,average=None)[1]\n",
        "    precision_no=precision_score(dic_Ytest[name_representation], predictions,average=None)[0]\n",
        "    precision_gen=precision_score(dic_Ytest[name_representation], predictions, average='macro')\n",
        "    f1_yes=f1_score(dic_Ytest[name_representation], predictions,average=None)[1]\n",
        "    f1_no=f1_score(dic_Ytest[name_representation], predictions,average=None)[0]\n",
        "    f1_gen=f1_score(dic_Ytest[name_representation], predictions, average='macro')\n",
        "    df_representation['SVM_{}'.format(name_representation)]=[precision_yes,precision_no,precision_gen,f1_yes,f1_no,f1_gen]\n",
        "    \n",
        "    \n",
        "    ##DataFrame with results for the three models \n",
        "    df_representation_results=pd.DataFrame(df_representation,index=['precision_yes','precision_no','precision_global','f1_yes','f1_no','f1_global']).T\n",
        "    \n",
        "    return(df_representation_results)\n",
        "\"\"\"    \n",
        "## Learning and saving\n",
        "results = pd.DataFrame({},index=['precision_yes','precision_no','precision_global','f1_yes','f1_no','f1_global']).T\n",
        "print(' ')\n",
        "for name_representation in dic_representations.keys():\n",
        "    print('APPRENTISSAGE {} : '.format(name_representation))\n",
        "    print(' ')\n",
        "    resutls_representation=models_XGB_RF_SVM(name_representation,'models')\n",
        "    results = pd.concat([results, resutls_representation])\n",
        "    print(' ')\n",
        "    print('#'*60)\n",
        "    print('#'*60)\n",
        "\n",
        "### Export of results  \n",
        "results.to_csv('sample_data/Projet_NLP/results_classification_classique.csv')\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"    \\n## Learning and saving\\nresults = pd.DataFrame({},index=['precision_yes','precision_no','precision_global','f1_yes','f1_no','f1_global']).T\\nprint(' ')\\nfor name_representation in dic_representations.keys():\\n    print('APPRENTISSAGE {} : '.format(name_representation))\\n    print(' ')\\n    resutls_representation=models_XGB_RF_SVM(name_representation,'models')\\n    results = pd.concat([results, resutls_representation])\\n    print(' ')\\n    print('#'*60)\\n    print('#'*60)\\n\\n### Export of results  \\nresults.to_csv('sample_data/Projet_NLP/results_classification_classique.csv')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ5bUrbioV5_",
        "colab_type": "code",
        "outputId": "9910a40e-9f47-4276-9aa3-29d90d10551d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "# Import of models and print results\n",
        "\n",
        "results = pd.read_csv('sample_data/Projet_NLP/results_classification_classique.csv',index_col=0)\n",
        "\n",
        "print(' ')\n",
        "print('Scores obtained with classic models :')\n",
        "display(results)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Scores obtained with classic models :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision_yes</th>\n",
              "      <th>precision_no</th>\n",
              "      <th>precision_global</th>\n",
              "      <th>f1_yes</th>\n",
              "      <th>f1_no</th>\n",
              "      <th>f1_global</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>RandomForest_tf_idf</th>\n",
              "      <td>0.782007</td>\n",
              "      <td>0.778351</td>\n",
              "      <td>0.780179</td>\n",
              "      <td>0.865624</td>\n",
              "      <td>0.417704</td>\n",
              "      <td>0.641664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_tf_idf</th>\n",
              "      <td>0.772260</td>\n",
              "      <td>0.738636</td>\n",
              "      <td>0.755448</td>\n",
              "      <td>0.858775</td>\n",
              "      <td>0.368794</td>\n",
              "      <td>0.613785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_tf_idf</th>\n",
              "      <td>0.738313</td>\n",
              "      <td>0.626866</td>\n",
              "      <td>0.682589</td>\n",
              "      <td>0.842945</td>\n",
              "      <td>0.140940</td>\n",
              "      <td>0.491942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_word2vec</th>\n",
              "      <td>0.795699</td>\n",
              "      <td>0.582133</td>\n",
              "      <td>0.688916</td>\n",
              "      <td>0.843164</td>\n",
              "      <td>0.463303</td>\n",
              "      <td>0.653233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_word2vec</th>\n",
              "      <td>0.804672</td>\n",
              "      <td>0.578811</td>\n",
              "      <td>0.691742</td>\n",
              "      <td>0.842391</td>\n",
              "      <td>0.491228</td>\n",
              "      <td>0.666810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_word2vec</th>\n",
              "      <td>0.765133</td>\n",
              "      <td>0.496377</td>\n",
              "      <td>0.630755</td>\n",
              "      <td>0.827496</td>\n",
              "      <td>0.342072</td>\n",
              "      <td>0.584784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_fast2vec_cluster</th>\n",
              "      <td>0.815163</td>\n",
              "      <td>0.644578</td>\n",
              "      <td>0.729871</td>\n",
              "      <td>0.863018</td>\n",
              "      <td>0.508918</td>\n",
              "      <td>0.685968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_fast2vec_cluster</th>\n",
              "      <td>0.795455</td>\n",
              "      <td>0.652344</td>\n",
              "      <td>0.723899</td>\n",
              "      <td>0.860563</td>\n",
              "      <td>0.436601</td>\n",
              "      <td>0.648582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_fast2vec_cluster</th>\n",
              "      <td>0.768908</td>\n",
              "      <td>0.473282</td>\n",
              "      <td>0.621095</td>\n",
              "      <td>0.830470</td>\n",
              "      <td>0.321660</td>\n",
              "      <td>0.576065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_fast2vec_mean</th>\n",
              "      <td>0.753326</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>0.763760</td>\n",
              "      <td>0.851771</td>\n",
              "      <td>0.288722</td>\n",
              "      <td>0.570246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_fast2vec_mean</th>\n",
              "      <td>0.787132</td>\n",
              "      <td>0.705660</td>\n",
              "      <td>0.746396</td>\n",
              "      <td>0.858361</td>\n",
              "      <td>0.464020</td>\n",
              "      <td>0.661190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_fast2vec_mean</th>\n",
              "      <td>0.751286</td>\n",
              "      <td>0.592179</td>\n",
              "      <td>0.671733</td>\n",
              "      <td>0.838010</td>\n",
              "      <td>0.294444</td>\n",
              "      <td>0.566227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_bert</th>\n",
              "      <td>0.790476</td>\n",
              "      <td>0.689516</td>\n",
              "      <td>0.739996</td>\n",
              "      <td>0.860940</td>\n",
              "      <td>0.443580</td>\n",
              "      <td>0.652260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_bert</th>\n",
              "      <td>0.814815</td>\n",
              "      <td>0.643646</td>\n",
              "      <td>0.729231</td>\n",
              "      <td>0.858970</td>\n",
              "      <td>0.526554</td>\n",
              "      <td>0.692762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_bert</th>\n",
              "      <td>0.757135</td>\n",
              "      <td>0.502370</td>\n",
              "      <td>0.629752</td>\n",
              "      <td>0.832799</td>\n",
              "      <td>0.288828</td>\n",
              "      <td>0.560814</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               precision_yes  precision_no  ...     f1_no  f1_global\n",
              "RandomForest_tf_idf                 0.782007      0.778351  ...  0.417704   0.641664\n",
              "XGBoost_tf_idf                      0.772260      0.738636  ...  0.368794   0.613785\n",
              "SVM_tf_idf                          0.738313      0.626866  ...  0.140940   0.491942\n",
              "RandomForest_word2vec               0.795699      0.582133  ...  0.463303   0.653233\n",
              "XGBoost_word2vec                    0.804672      0.578811  ...  0.491228   0.666810\n",
              "SVM_word2vec                        0.765133      0.496377  ...  0.342072   0.584784\n",
              "RandomForest_fast2vec_cluster       0.815163      0.644578  ...  0.508918   0.685968\n",
              "XGBoost_fast2vec_cluster            0.795455      0.652344  ...  0.436601   0.648582\n",
              "SVM_fast2vec_cluster                0.768908      0.473282  ...  0.321660   0.576065\n",
              "RandomForest_fast2vec_mean          0.753326      0.774194  ...  0.288722   0.570246\n",
              "XGBoost_fast2vec_mean               0.787132      0.705660  ...  0.464020   0.661190\n",
              "SVM_fast2vec_mean                   0.751286      0.592179  ...  0.294444   0.566227\n",
              "RandomForest_bert                   0.790476      0.689516  ...  0.443580   0.652260\n",
              "XGBoost_bert                        0.814815      0.643646  ...  0.526554   0.692762\n",
              "SVM_bert                            0.757135      0.502370  ...  0.288828   0.560814\n",
              "\n",
              "[15 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naNP07CCoV6D",
        "colab_type": "text"
      },
      "source": [
        "##### II.c Classification with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ4zsjOBoV6E",
        "colab_type": "code",
        "outputId": "0ad46929-9766-40fa-d996-81bb7749e021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Hyperparameters of the network  \n",
        "\"\"\"\n",
        "# Maximum words to use \n",
        "Max_nb_words= 50000\n",
        "# Maximum lenght for a tweet \n",
        "Max_sequence_length = 30 # an average of 12 words were retained per tweet\n",
        "Embedding_dim= 100\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=Max_nb_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "\n",
        "################################################################\n",
        "################################################################\n",
        "\n",
        "# Formating of databases in input\n",
        "\n",
        "# Tweets to convert\n",
        "corpus=import_clean_text()\n",
        "labels=list(df.Existence)\n",
        "corpus_new=[]\n",
        "for tweet in corpus: \n",
        "    tweet_sentence=''\n",
        "    for word in tweet:\n",
        "        tweet_sentence=tweet_sentence+' '+word\n",
        "    corpus_new.append(tweet_sentence)\n",
        "\n",
        "corpus=[]\n",
        "new_labels=[]\n",
        "for label_index in range(len(labels)):\n",
        "    if (labels[label_index]=='Yes')or(labels[label_index]=='No'):\n",
        "        corpus.append(corpus_new[label_index])\n",
        "        new_labels.append(labels[label_index])\n",
        "labels=new_labels\n",
        "labels=[(labels[tweet]=='Yes')*1 for tweet in range(len(labels))]\n",
        "\n",
        "    \n",
        "tokenizer.fit_on_texts(np.array(corpus))\n",
        "X = tokenizer.texts_to_sequences(np.array(corpus))\n",
        "X = sequence.pad_sequences(X, maxlen=Max_sequence_length) \n",
        "\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(X,labels, test_size=0.5)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Maximum words to use \\nMax_nb_words= 50000\\n# Maximum lenght for a tweet \\nMax_sequence_length = 30 # an average of 12 words were retained per tweet\\nEmbedding_dim= 100\\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=Max_nb_words, filters=\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\', lower=True)\\n\\n################################################################\\n################################################################\\n\\n# Formating of databases in input\\n\\n# Tweets to convert\\ncorpus=import_clean_text()\\nlabels=list(df.Existence)\\ncorpus_new=[]\\nfor tweet in corpus: \\n    tweet_sentence=\\'\\'\\n    for word in tweet:\\n        tweet_sentence=tweet_sentence+\\' \\'+word\\n    corpus_new.append(tweet_sentence)\\n\\ncorpus=[]\\nnew_labels=[]\\nfor label_index in range(len(labels)):\\n    if (labels[label_index]==\\'Yes\\')or(labels[label_index]==\\'No\\'):\\n        corpus.append(corpus_new[label_index])\\n        new_labels.append(labels[label_index])\\nlabels=new_labels\\nlabels=[(labels[tweet]==\\'Yes\\')*1 for tweet in range(len(labels))]\\n\\n    \\ntokenizer.fit_on_texts(np.array(corpus))\\nX = tokenizer.texts_to_sequences(np.array(corpus))\\nX = sequence.pad_sequences(X, maxlen=Max_sequence_length) \\n\\nX_train, X_test, Y_train, Y_test= train_test_split(X,labels, test_size=0.5)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1tJeGrsoV6I",
        "colab_type": "code",
        "outputId": "25ee2b26-229f-41e1-c6a5-5a40ffe6fb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Model training \n",
        "\"\"\"\n",
        "## train of the model \n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(Max_nb_words, Embedding_dim, input_length=Max_sequence_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, Y_train, epochs=10, batch_size=64)\n",
        "\n",
        "## predictions on the test database\n",
        "print(' ')\n",
        "print(' ')\n",
        "print('#'*60)\n",
        "print(' ')\n",
        "print('Prédiction LSTM sur le Test...')\n",
        "pred_LSTM=model.predict(X_test)\n",
        "pred_LSTM=[(float(pred_LSTM[pred])>=0.5)*1 for pred in range(len(pred_LSTM))]\n",
        "print('DONE')\n",
        "print(' ')\n",
        "\n",
        "## export of results\n",
        "df_results={}\n",
        "precision_yes=precision_score(Y_test, pred_LSTM,average=None)[1]\n",
        "precision_no=precision_score(Y_test, pred_LSTM,average=None)[1]\n",
        "precision_gen=precision_score(Y_test, pred_LSTM, average='macro')\n",
        "f1_yes=f1_score(Y_test, pred_LSTM,average=None)[1]\n",
        "f1_no=f1_score(Y_test, pred_LSTM,average=None)[0]\n",
        "f1_gen=f1_score(Y_test, pred_LSTM, average='macro')\n",
        "df_results['LSTM']=[precision_yes,precision_no,precision_gen,f1_yes,f1_no,f1_gen]\n",
        "df_results=pd.DataFrame(df_results,index=['precision_yes','precision_no','precision_global','f1_yes','f1_no','f1_global']).T\n",
        "df_results.to_csv('sample_data/Projet_NLP/results_classification_LSTM.csv')\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n## train of the model \\nembedding_vecor_length = 32\\nmodel = Sequential()\\nmodel.add(Embedding(Max_nb_words, Embedding_dim, input_length=Max_sequence_length))\\nmodel.add(LSTM(100))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\\nprint(model.summary())\\nmodel.fit(X_train, Y_train, epochs=10, batch_size=64)\\n\\n## predictions on the test database\\nprint(' ')\\nprint(' ')\\nprint('#'*60)\\nprint(' ')\\nprint('Prédiction LSTM sur le Test...')\\npred_LSTM=model.predict(X_test)\\npred_LSTM=[(float(pred_LSTM[pred])>=0.5)*1 for pred in range(len(pred_LSTM))]\\nprint('DONE')\\nprint(' ')\\n\\n## export of results\\ndf_results={}\\nprecision_yes=precision_score(Y_test, pred_LSTM,average=None)[1]\\nprecision_no=precision_score(Y_test, pred_LSTM,average=None)[1]\\nprecision_gen=precision_score(Y_test, pred_LSTM, average='macro')\\nf1_yes=f1_score(Y_test, pred_LSTM,average=None)[1]\\nf1_no=f1_score(Y_test, pred_LSTM,average=None)[0]\\nf1_gen=f1_score(Y_test, pred_LSTM, average='macro')\\ndf_results['LSTM']=[precision_yes,precision_no,precision_gen,f1_yes,f1_no,f1_gen]\\ndf_results=pd.DataFrame(df_results,index=['precision_yes','precision_no','precision_global','f1_yes','f1_no','f1_global']).T\\ndf_results.to_csv('sample_data/Projet_NLP/results_classification_LSTM.csv')\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tClfa-3VoV6K",
        "colab_type": "code",
        "outputId": "032bf4c9-5845-4ae4-d4a6-36c9d7623615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "# Import of results and print \n",
        "\n",
        "df_results = pd.read_csv('sample_data/Projet_NLP/results_classification_LSTM.csv',index_col=0)\n",
        "\n",
        "print(' ')\n",
        "print('Scores obtained with the LSTM model ')\n",
        "display(df_results)\n",
        "print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Scores obtained with the LSTM model \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision_yes</th>\n",
              "      <th>precision_no</th>\n",
              "      <th>precision_global</th>\n",
              "      <th>f1_yes</th>\n",
              "      <th>f1_no</th>\n",
              "      <th>f1_global</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LSTM</th>\n",
              "      <td>0.835072</td>\n",
              "      <td>0.835072</td>\n",
              "      <td>0.755099</td>\n",
              "      <td>0.87054</td>\n",
              "      <td>0.582694</td>\n",
              "      <td>0.726617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      precision_yes  precision_no  ...     f1_no  f1_global\n",
              "LSTM       0.835072      0.835072  ...  0.582694   0.726617\n",
              "\n",
              "[1 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yccJByqVoV6Z",
        "colab_type": "text"
      },
      "source": [
        "# III. Tweets analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c_v3sTXoV6b",
        "colab_type": "text"
      },
      "source": [
        "## III.a First descriptive analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIe3AeXQoV6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tweets databases  of Yes and No\n",
        "\n",
        "df_yes=df[df.Existence=='Yes'].reset_index()\n",
        "Tweet_clean_yes=import_clean_text(option='yes') \n",
        "Tweet_clean_yes=reduce(add, Tweet_clean_yes)\n",
        "\n",
        "df_no=df[df.Existence=='No'].reset_index()\n",
        "Tweet_clean_no=import_clean_text(option='no') \n",
        "Tweet_clean_no=reduce(add, Tweet_clean_no)\n",
        "\n",
        "Tweet_clean=import_clean_text() \n",
        "Tweet_clean=reduce(add, Tweet_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGDhbo9_oV6f",
        "colab_type": "code",
        "outputId": "8c7f3188-986b-4d24-cab6-c822d30bfe98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Most frequent words in the global database\n",
        "counter=collections.Counter(Tweet_clean)\n",
        "\n",
        "number_word=10\n",
        "print('The {} words that appear the most in the database are (in descending order) :'.format(number_word))\n",
        "print(' ')\n",
        "for word in counter.most_common(number_word):\n",
        "    print(word[0]+' ('+str(word[1])+') ') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 10 words that appear the most in the database are (in descending order) :\n",
            " \n",
            "climate_change (2936) \n",
            "global_warming (2709) \n",
            "the (2275) \n",
            "be (1801) \n",
            "to (1443) \n",
            "of (1328) \n",
            "a (1193) \n",
            "on (962) \n",
            "and (869) \n",
            "in (845) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YCT8OQVoV6i",
        "colab_type": "code",
        "outputId": "dfedeb84-aab2-4edf-f8b6-6369a5a501ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Most frequent words in the database No.\n",
        "counter=collections.Counter(Tweet_clean_no)\n",
        "\n",
        "number_word=10\n",
        "print('The {} words that appear the most in the database No are (in descending order) :'.format(number_word))\n",
        "print(' ')\n",
        "for word in counter.most_common(number_word):\n",
        "    print(word[0]+' ('+str(word[1])+') ') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 10 words that appear the most in the database No are (in descending order) :\n",
            " \n",
            "global_warming (812) \n",
            "be (547) \n",
            "the (454) \n",
            "a (311) \n",
            "climate_change (280) \n",
            "of (269) \n",
            "to (260) \n",
            "it (185) \n",
            "in (146) \n",
            "on (143) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPm_k5xFoV6p",
        "colab_type": "code",
        "outputId": "34b26fff-1677-41b3-a322-93c4d7c2e245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Mots les plus occurents dans la base Yes\n",
        "counter=collections.Counter(Tweet_clean_yes)\n",
        "\n",
        "number_word=10\n",
        "print('The {} words that appear the most in the database Yes are (in descending order) :'.format(number_word))\n",
        "print(' ')\n",
        "for word in counter.most_common(number_word):\n",
        "    print(word[0]+' ('+str(word[1])+') ') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 10 words that appear the most in the database Yes are (in descending order) :\n",
            " \n",
            "climate_change (1607) \n",
            "global_warming (1309) \n",
            "the (1138) \n",
            "be (902) \n",
            "to (840) \n",
            "of (634) \n",
            "a (577) \n",
            "on (479) \n",
            "and (468) \n",
            "in (455) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX84LYY3oV6r",
        "colab_type": "text"
      },
      "source": [
        "From this point of view, there is no real difference between the No and Yes base. So we look at the ones that are specific to Yes among the 200 most frequent words. Same for the No words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fsv9-IjoV6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determination of words specific to tweets Yes and tweets No\n",
        "\n",
        "counter=collections.Counter(Tweet_clean_yes)\n",
        "list_commun_yes=[word[0] for word in counter.most_common(200)]\n",
        "\n",
        "counter=collections.Counter(Tweet_clean_no)\n",
        "list_commun_no=[word[0] for word in counter.most_common(200)]\n",
        "\n",
        "list_spe_yes=[]\n",
        "for word in list_commun_yes:\n",
        "    if word not in list_commun_no:\n",
        "        list_spe_yes.append(word)\n",
        "        \n",
        "list_spe_no=[]\n",
        "for word in list_commun_no:\n",
        "    if word not in list_commun_yes:\n",
        "        list_spe_no.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWoNo2M7oV6x",
        "colab_type": "text"
      },
      "source": [
        "We are now looking in detail at the attendance statistics of these words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYto2k3poV6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculation of the statistics of the interesting words that have been retained\n",
        "\n",
        "## Interesting words\n",
        "mots_interessants_yes=['green','us','could','world','energy','earth','fight','worse','help','carbon']\n",
        "mots_interessants_no=['gore','hoax','scam','climategate','gop','teaparty','palin','collapse','fraud','believe']\n",
        "\n",
        "#################################################\n",
        "#################################################\n",
        "\n",
        "##Computing statistics \n",
        "\n",
        "counter_yes=collections.Counter(Tweet_clean_yes)\n",
        "counter_no=collections.Counter(Tweet_clean_no)\n",
        "\n",
        "### Frequency of these words in the bases\n",
        "frequence_yes_in_yes={}\n",
        "frequence_yes_in_no={}\n",
        "for word in mots_interessants_yes:\n",
        "    frequence_yes_in_yes[word]=counter_yes[word]/len(Tweet_clean_yes)*100\n",
        "    frequence_yes_in_no[word]=counter_no[word]/len(Tweet_clean_no)*100\n",
        "    \n",
        "    \n",
        "frequence_no_in_no={}\n",
        "frequence_no_in_yes={}\n",
        "for word in mots_interessants_no:\n",
        "    frequence_no_in_no[word]=(counter_no[word]/len(Tweet_clean_no))*100\n",
        "    frequence_no_in_yes[word]=(counter_yes[word]/len(Tweet_clean_yes))*100\n",
        "    \n",
        "### Number of tweets in which they appear \n",
        "def Number_apparition_tweets(list_tweet,word):\n",
        "    number=0\n",
        "    for tweet in list_tweet:\n",
        "        if word in tweet:\n",
        "            number=number+1\n",
        "    return(number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0He3q4NoV60",
        "colab_type": "code",
        "outputId": "28f6f503-969b-4046-bd47-c03bd056829b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Print results for climate change tweets\n",
        "\n",
        "print('Interesting words in favour of climate change:')\n",
        "print(' ')\n",
        "print('\\'frequency\\'=any words included in the Yes (or No) database')\n",
        "print(' ')\n",
        "print('Statistics on the tweets Yes :')\n",
        "print(' ')\n",
        "list_tweet_yes=import_clean_text(option='yes')\n",
        "for key in frequence_yes_in_yes.keys():\n",
        "    print(key+': frequency = '+str(round(frequence_yes_in_yes[key],2))+'%'+' , number of tweets containing it = '+str(Number_apparition_tweets(list_tweet_yes,key))+' (sur {})'.format(len(list_tweet_yes))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_yes,key)/len(list_tweet_yes),2)))                                   \n",
        "print(' ') \n",
        "print('#'*10)\n",
        "print(' ')\n",
        "print('Statistics on the tweets No :')\n",
        "print(' ')\n",
        "list_tweet_no=import_clean_text(option='no')\n",
        "for key in frequence_yes_in_no.keys():\n",
        "    print(key+': frequency  = '+str(round(frequence_yes_in_no[key],2))+'%'+' , number of tweets containing it = '+str(Number_apparition_tweets(list_tweet_no,key))+' (sur {})'.format(len(list_tweet_no))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_no,key)/len(list_tweet_no),2))) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interesting words in favour of climate change:\n",
            " \n",
            "'frequency'=any words included in the Yes (or No) database\n",
            " \n",
            "Statistics on the tweets Yes :\n",
            " \n",
            "green: frequency = 0.28% , number of tweets containing it = 91 (sur 2821)--> 3.23%\n",
            "us: frequency = 0.01% , number of tweets containing it = 3 (sur 2821)--> 0.11%\n",
            "could: frequency = 0.18% , number of tweets containing it = 62 (sur 2821)--> 2.2%\n",
            "world: frequency = 0.2% , number of tweets containing it = 67 (sur 2821)--> 2.38%\n",
            "energy: frequency = 0.18% , number of tweets containing it = 59 (sur 2821)--> 2.09%\n",
            "earth: frequency = 0.11% , number of tweets containing it = 38 (sur 2821)--> 1.35%\n",
            "fight: frequency = 0.21% , number of tweets containing it = 73 (sur 2821)--> 2.59%\n",
            "worse: frequency = 0.0% , number of tweets containing it = 0 (sur 2821)--> 0.0%\n",
            "help: frequency = 0.11% , number of tweets containing it = 38 (sur 2821)--> 1.35%\n",
            "carbon: frequency = 0.13% , number of tweets containing it = 44 (sur 2821)--> 1.56%\n",
            " \n",
            "##########\n",
            " \n",
            "Statistics on the tweets No :\n",
            " \n",
            "green: frequency  = 0.1% , number of tweets containing it = 14 (sur 1035)--> 1.35%\n",
            "us: frequency  = 0.01% , number of tweets containing it = 1 (sur 1035)--> 0.1%\n",
            "could: frequency  = 0.08% , number of tweets containing it = 11 (sur 1035)--> 1.06%\n",
            "world: frequency  = 0.09% , number of tweets containing it = 12 (sur 1035)--> 1.16%\n",
            "energy: frequency  = 0.06% , number of tweets containing it = 9 (sur 1035)--> 0.87%\n",
            "earth: frequency  = 0.06% , number of tweets containing it = 9 (sur 1035)--> 0.87%\n",
            "fight: frequency  = 0.04% , number of tweets containing it = 6 (sur 1035)--> 0.58%\n",
            "worse: frequency  = 0.0% , number of tweets containing it = 0 (sur 1035)--> 0.0%\n",
            "help: frequency  = 0.07% , number of tweets containing it = 10 (sur 1035)--> 0.97%\n",
            "carbon: frequency  = 0.04% , number of tweets containing it = 6 (sur 1035)--> 0.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osot9jAtoV64",
        "colab_type": "code",
        "outputId": "6e9297c1-a4c4-4200-9cd2-f98f0a1e916c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Print results for tweets skeptical of climate change\n",
        "\n",
        "print('Interesting words against climate change :')\n",
        "print(' ')\n",
        "print('\\'frequency\\'=any words included in the Yes (or No) database')\n",
        "print(' ')\n",
        "print('Statistics on the tweets Yes :')                                  \n",
        "print(' ')\n",
        "list_tweet_yes=import_clean_text(option='yes')\n",
        "for key in frequence_no_in_yes.keys():\n",
        "    print(key+': frequency  = '+str(round(frequence_no_in_yes[key],2))+'%'+' , number of tweets containing it = '+str(Number_apparition_tweets(list_tweet_yes,key))+' (sur {})'.format(len(list_tweet_yes))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_yes,key)/len(list_tweet_yes),2))) \n",
        "print(' ') \n",
        "print('#'*10)\n",
        "print(' ')\n",
        "print('Statistics on the tweets No :')\n",
        "print(' ')\n",
        "list_tweet_no=import_clean_text(option='no')\n",
        "for key in frequence_no_in_no.keys():\n",
        "    print(key+': frequency = '+str(round(frequence_no_in_no[key],2))+'%'+' , number of tweets containing it = '+str(Number_apparition_tweets(list_tweet_no,key))+' (sur {})'.format(len(list_tweet_no))+'--> {}%'.format(round(100*Number_apparition_tweets(list_tweet_no,key)/len(list_tweet_no),2))) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interesting words against climate change :\n",
            " \n",
            "'frequency'=any words included in the Yes (or No) database\n",
            " \n",
            "Statistics on the tweets Yes :\n",
            " \n",
            "gore: frequency  = 0.01% , number of tweets containing it = 2 (sur 2821)--> 0.07%\n",
            "hoax: frequency  = 0.02% , number of tweets containing it = 6 (sur 2821)--> 0.21%\n",
            "scam: frequency  = 0.0% , number of tweets containing it = 0 (sur 2821)--> 0.0%\n",
            "climategate: frequency  = 0.07% , number of tweets containing it = 23 (sur 2821)--> 0.82%\n",
            "gop: frequency  = 0.04% , number of tweets containing it = 13 (sur 2821)--> 0.46%\n",
            "teaparty: frequency  = 0.02% , number of tweets containing it = 8 (sur 2821)--> 0.28%\n",
            "palin: frequency  = 0.04% , number of tweets containing it = 12 (sur 2821)--> 0.43%\n",
            "collapse: frequency  = 0.03% , number of tweets containing it = 9 (sur 2821)--> 0.32%\n",
            "fraud: frequency  = 0.01% , number of tweets containing it = 4 (sur 2821)--> 0.14%\n",
            "believe: frequency  = 0.03% , number of tweets containing it = 11 (sur 2821)--> 0.39%\n",
            " \n",
            "##########\n",
            " \n",
            "Statistics on the tweets No :\n",
            " \n",
            "gore: frequency = 0.07% , number of tweets containing it = 10 (sur 1035)--> 0.97%\n",
            "hoax: frequency = 0.21% , number of tweets containing it = 28 (sur 1035)--> 2.71%\n",
            "scam: frequency = 0.16% , number of tweets containing it = 22 (sur 1035)--> 2.13%\n",
            "climategate: frequency = 0.19% , number of tweets containing it = 27 (sur 1035)--> 2.61%\n",
            "gop: frequency = 0.18% , number of tweets containing it = 26 (sur 1035)--> 2.51%\n",
            "teaparty: frequency = 0.1% , number of tweets containing it = 15 (sur 1035)--> 1.45%\n",
            "palin: frequency = 0.1% , number of tweets containing it = 13 (sur 1035)--> 1.26%\n",
            "collapse: frequency = 0.15% , number of tweets containing it = 22 (sur 1035)--> 2.13%\n",
            "fraud: frequency = 0.15% , number of tweets containing it = 21 (sur 1035)--> 2.03%\n",
            "believe: frequency = 0.08% , number of tweets containing it = 10 (sur 1035)--> 0.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDIv-YPVoV66",
        "colab_type": "text"
      },
      "source": [
        "$\\underline{Comments}$\n",
        "\n",
        "Looking at these first simply descriptive statistics, we can see that:\n",
        "\n",
        "The vocabulary of climate change skeptic tweets seems to consist mainly of two themes:\n",
        "- the vocabulary of the media and political scandal. For example, certain controversies seem to be particularly present with the names Al Gore and 'climategate' coming up in these tweets. Political personalities are also cited with the names of Sarah Palin and GOP appearing.\n",
        "- the vocabulary of lying and prankery. These include the words hoax, scam and teaparty.\n",
        "\n",
        "Concerning the tweets that believe in climate change, we can globally observe the theme related to the effort (could, energy, fight) of solidarity (us, help, world) and the environment (green, earth, carbon).\n",
        "\n",
        "\n",
        "At first glance, the sceptics seem more 'negative'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljg9oB7SoV67",
        "colab_type": "text"
      },
      "source": [
        "## III.b Clustering based on representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trgNcFk_oV67",
        "colab_type": "text"
      },
      "source": [
        "The idea of this clustering is to group tweets into groups of significant 'topics'. The objective is then to observe which are the dominant words in these topics.\n",
        "\n",
        "The selected representations are chosen according to their performance on the classification stage.\n",
        "The overall objective being the study of the arguments of climato-skeptics, we have privileged the representations having a good F1-score on the 'No' class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqnF22-DoV68",
        "colab_type": "code",
        "outputId": "95cf1684-b789-4241-9b1c-dd8ddbd07e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "#Scores ranking on the F1-No\n",
        "results = pd.read_csv('sample_data/Projet_NLP/results_classification_classique.csv',index_col=0)\n",
        "print(' ')\n",
        "print('Scores obtained with classic models :')\n",
        "display(results.sort_values(by=['f1_no'],ascending=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Scores obtained with classic models :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision_yes</th>\n",
              "      <th>precision_no</th>\n",
              "      <th>precision_global</th>\n",
              "      <th>f1_yes</th>\n",
              "      <th>f1_no</th>\n",
              "      <th>f1_global</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBoost_bert</th>\n",
              "      <td>0.814815</td>\n",
              "      <td>0.643646</td>\n",
              "      <td>0.729231</td>\n",
              "      <td>0.858970</td>\n",
              "      <td>0.526554</td>\n",
              "      <td>0.692762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_fast2vec_cluster</th>\n",
              "      <td>0.815163</td>\n",
              "      <td>0.644578</td>\n",
              "      <td>0.729871</td>\n",
              "      <td>0.863018</td>\n",
              "      <td>0.508918</td>\n",
              "      <td>0.685968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_word2vec</th>\n",
              "      <td>0.804672</td>\n",
              "      <td>0.578811</td>\n",
              "      <td>0.691742</td>\n",
              "      <td>0.842391</td>\n",
              "      <td>0.491228</td>\n",
              "      <td>0.666810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_fast2vec_mean</th>\n",
              "      <td>0.787132</td>\n",
              "      <td>0.705660</td>\n",
              "      <td>0.746396</td>\n",
              "      <td>0.858361</td>\n",
              "      <td>0.464020</td>\n",
              "      <td>0.661190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_word2vec</th>\n",
              "      <td>0.795699</td>\n",
              "      <td>0.582133</td>\n",
              "      <td>0.688916</td>\n",
              "      <td>0.843164</td>\n",
              "      <td>0.463303</td>\n",
              "      <td>0.653233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_bert</th>\n",
              "      <td>0.790476</td>\n",
              "      <td>0.689516</td>\n",
              "      <td>0.739996</td>\n",
              "      <td>0.860940</td>\n",
              "      <td>0.443580</td>\n",
              "      <td>0.652260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_fast2vec_cluster</th>\n",
              "      <td>0.795455</td>\n",
              "      <td>0.652344</td>\n",
              "      <td>0.723899</td>\n",
              "      <td>0.860563</td>\n",
              "      <td>0.436601</td>\n",
              "      <td>0.648582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_tf_idf</th>\n",
              "      <td>0.782007</td>\n",
              "      <td>0.778351</td>\n",
              "      <td>0.780179</td>\n",
              "      <td>0.865624</td>\n",
              "      <td>0.417704</td>\n",
              "      <td>0.641664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost_tf_idf</th>\n",
              "      <td>0.772260</td>\n",
              "      <td>0.738636</td>\n",
              "      <td>0.755448</td>\n",
              "      <td>0.858775</td>\n",
              "      <td>0.368794</td>\n",
              "      <td>0.613785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_word2vec</th>\n",
              "      <td>0.765133</td>\n",
              "      <td>0.496377</td>\n",
              "      <td>0.630755</td>\n",
              "      <td>0.827496</td>\n",
              "      <td>0.342072</td>\n",
              "      <td>0.584784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_fast2vec_cluster</th>\n",
              "      <td>0.768908</td>\n",
              "      <td>0.473282</td>\n",
              "      <td>0.621095</td>\n",
              "      <td>0.830470</td>\n",
              "      <td>0.321660</td>\n",
              "      <td>0.576065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_fast2vec_mean</th>\n",
              "      <td>0.751286</td>\n",
              "      <td>0.592179</td>\n",
              "      <td>0.671733</td>\n",
              "      <td>0.838010</td>\n",
              "      <td>0.294444</td>\n",
              "      <td>0.566227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_bert</th>\n",
              "      <td>0.757135</td>\n",
              "      <td>0.502370</td>\n",
              "      <td>0.629752</td>\n",
              "      <td>0.832799</td>\n",
              "      <td>0.288828</td>\n",
              "      <td>0.560814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForest_fast2vec_mean</th>\n",
              "      <td>0.753326</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>0.763760</td>\n",
              "      <td>0.851771</td>\n",
              "      <td>0.288722</td>\n",
              "      <td>0.570246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM_tf_idf</th>\n",
              "      <td>0.738313</td>\n",
              "      <td>0.626866</td>\n",
              "      <td>0.682589</td>\n",
              "      <td>0.842945</td>\n",
              "      <td>0.140940</td>\n",
              "      <td>0.491942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               precision_yes  precision_no  ...     f1_no  f1_global\n",
              "XGBoost_bert                        0.814815      0.643646  ...  0.526554   0.692762\n",
              "RandomForest_fast2vec_cluster       0.815163      0.644578  ...  0.508918   0.685968\n",
              "XGBoost_word2vec                    0.804672      0.578811  ...  0.491228   0.666810\n",
              "XGBoost_fast2vec_mean               0.787132      0.705660  ...  0.464020   0.661190\n",
              "RandomForest_word2vec               0.795699      0.582133  ...  0.463303   0.653233\n",
              "RandomForest_bert                   0.790476      0.689516  ...  0.443580   0.652260\n",
              "XGBoost_fast2vec_cluster            0.795455      0.652344  ...  0.436601   0.648582\n",
              "RandomForest_tf_idf                 0.782007      0.778351  ...  0.417704   0.641664\n",
              "XGBoost_tf_idf                      0.772260      0.738636  ...  0.368794   0.613785\n",
              "SVM_word2vec                        0.765133      0.496377  ...  0.342072   0.584784\n",
              "SVM_fast2vec_cluster                0.768908      0.473282  ...  0.321660   0.576065\n",
              "SVM_fast2vec_mean                   0.751286      0.592179  ...  0.294444   0.566227\n",
              "SVM_bert                            0.757135      0.502370  ...  0.288828   0.560814\n",
              "RandomForest_fast2vec_mean          0.753326      0.774194  ...  0.288722   0.570246\n",
              "SVM_tf_idf                          0.738313      0.626866  ...  0.140940   0.491942\n",
              "\n",
              "[15 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQpzelmoV7B",
        "colab_type": "text"
      },
      "source": [
        "###### Clustering based on the BERT representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDau4EkKoV7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Cut_words(option='all'):\n",
        "    \n",
        "    Tweet_clean=import_clean_text(option)\n",
        "\n",
        "    # You remove words of three letters or less and the stowords\n",
        "    Tweet_clean_lemmatize=[]\n",
        "    for tweet in Tweet_clean:\n",
        "        new_tweet=[]\n",
        "        for token in tweet:\n",
        "            if (len(token)>3)&(token not in stopWords):\n",
        "                new_tweet.append(token)\n",
        "        Tweet_clean_lemmatize.append(new_tweet)\n",
        "    Tweet_clean=Tweet_clean_lemmatize  \n",
        "    \n",
        "    return(Tweet_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LsHCLuJoV7E",
        "colab_type": "code",
        "outputId": "b5bbbc0f-1a7d-49b1-ab56-b5af27a01d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "#Clustering only on the basis of No ; and print results \n",
        "\n",
        "##Calling up the learning base and corresponding tweets\n",
        "\n",
        "df_bert_clustering=df_bert[df_bert.Labels=='No']\n",
        "data=Cut_words(option='no')\n",
        "df_bert_clustering['Words_tweet']=data\n",
        "\n",
        "##Clustering\n",
        "n_clusters=3\n",
        "kmeans = KMeans(n_clusters=n_clusters,random_state=2)\n",
        "\n",
        "##Learning\n",
        "kmeans.fit(df_bert_clustering[[str(x) for x in range(df_bert_clustering.shape[1]-2)]])\n",
        "\n",
        "##Prediction\n",
        "df_bert_clustering['cluster']=kmeans.predict(df_bert_clustering[[str(x) for x in range(df_bert_clustering.shape[1]-2)]]).tolist()\n",
        "\n",
        "df_bert_clustering=df_bert_clustering[['Words_tweet','cluster','Labels']]\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "########################################################################\n",
        "\n",
        "\n",
        "# Print words in each cluster\n",
        "\n",
        "nombre_total_no=len(df_bert_clustering)\n",
        "\n",
        "for number_cluster in range(n_clusters):\n",
        "    cluster=df_bert_clustering[df_bert_clustering.cluster==number_cluster]\n",
        "    list_cluster=reduce(add, list(cluster.Words_tweet))\n",
        "    counts = Counter(list_cluster)\n",
        "    counts = counts.most_common(10)\n",
        "    print('The 10 most frequent words in cluster {} are :'.format(str(number_cluster)))\n",
        "    print(' ')\n",
        "    print(counts)\n",
        "    print(' ')\n",
        "    number_no=round(100*len(cluster[cluster.Labels=='No'])/nombre_total_no,2) \n",
        "    print('In this cluster, the share of No relative to the total number of No is {} %'.format(str(number_no))) \n",
        "    print(' ')\n",
        "    print('#'*60)\n",
        "    print('#'*60)\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The 10 most frequent words in cluster 0 are :\n",
            " \n",
            "[('global_warming', 186), ('climate_change', 100), ('science', 27), ('the_great', 23), ('climate', 22), ('collapse', 19), ('climategate', 14), ('fraud', 11), ('hoax', 11), ('theory', 10)]\n",
            " \n",
            "In this cluster, the share of No relative to the total number of No is 25.02 %\n",
            " \n",
            "############################################################\n",
            "############################################################\n",
            " \n",
            "The 10 most frequent words in cluster 1 are :\n",
            " \n",
            "[('global_warming', 309), ('climate_change', 109), ('tcot', 50), ('al_gore', 34), ('snow', 25), ('obama', 19), ('science', 18), ('a_silly', 18), ('time', 18), ('scientist', 17)]\n",
            " \n",
            "In this cluster, the share of No relative to the total number of No is 39.9 %\n",
            " \n",
            "############################################################\n",
            "############################################################\n",
            " \n",
            "The 10 most frequent words in cluster 2 are :\n",
            " \n",
            "[('global_warming', 317), ('snow', 80), ('climate_change', 71), ('do_not', 31), ('cold', 28), ('tcot', 27), ('in_dc', 27), ('al_gore', 23), ('cause', 22), ('make', 21)]\n",
            " \n",
            "In this cluster, the share of No relative to the total number of No is 35.07 %\n",
            " \n",
            "############################################################\n",
            "############################################################\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaXpyWBaoV7J",
        "colab_type": "text"
      },
      "source": [
        "$\\underline{Comments}$\n",
        "\n",
        "We chose to make 3 clusters to avoid spreading ourselves too thin on the possible subjects and types of arguments. Clustering on climate-skeptic people brings out the main themes. \n",
        "\n",
        "The distribution of tweets in these clusters seems fair. The repartition of this population in clusters 0,1 and 2 are respectively $37$%,  $37$% and $26$%.\n",
        "\n",
        "Two clusters seem particularly interpretable to us.\n",
        "\n",
        "Cluster 2 clearly highlights the theme of lying, fraud and conspiracy. It is notably the only cluster where we find the terms 'climategate', 'fraud' and 'hoax'. In this context, the mote 'theory' reminds us of the argumentation of the conspiracy theory.\n",
        "\n",
        "Cluster 1 seems to refer to the political world. It includes the words 'Obama' and 'al gore', two political figures who are particularly committed to the fight against climate change. The words 'scam' and 'silly' are also included. One can therefore interpret this cluster as a criticism of the political class. \n",
        "\n",
        "Cluster 0 is more difficult to interpret. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR3qVK5toV7K",
        "colab_type": "code",
        "outputId": "4eae810f-3763-4206-ecb8-e7c0967afffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "#Clustering only on the basis of Yes and print results \n",
        "\n",
        "##Calling up the learning base and corresponding tweets\n",
        "\n",
        "df_bert_clustering=df_bert[df_bert.Labels=='Yes']\n",
        "data=Cut_words(option='yes')\n",
        "df_bert_clustering['Words_tweet']=data\n",
        "\n",
        "##Clustering\n",
        "n_clusters=3\n",
        "kmeans = KMeans(n_clusters=n_clusters,random_state=1)\n",
        "\n",
        "##Learning\n",
        "kmeans.fit(df_bert_clustering[[str(x) for x in range(df_bert_clustering.shape[1]-2)]])\n",
        "\n",
        "##Predictions\n",
        "df_bert_clustering['cluster']=kmeans.predict(df_bert_clustering[[str(x) for x in range(df_bert_clustering.shape[1]-2)]]).tolist()\n",
        "\n",
        "df_bert_clustering=df_bert_clustering[['Words_tweet','cluster','Labels']]\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "########################################################################\n",
        "\n",
        "\n",
        "# Print words in each cluster\n",
        "\n",
        "nombre_total_yes=len(df_bert_clustering)\n",
        "\n",
        "for number_cluster in range(n_clusters):\n",
        "    cluster=df_bert_clustering[df_bert_clustering.cluster==number_cluster]\n",
        "    list_cluster=reduce(add, list(cluster.Words_tweet))\n",
        "    counts = Counter(list_cluster)\n",
        "    counts = counts.most_common(10)\n",
        "    print('The 10 most frequent words in cluster {} are :'.format(str(number_cluster)))\n",
        "    print(' ')\n",
        "    print(counts)\n",
        "    print(' ')\n",
        "    number_yes=round(100*len(cluster[cluster.Labels=='Yes'])/nombre_total_yes,2) \n",
        "    print('In this cluster, the share of Yes relative to the total number of Yes is {} %'.format(str(number_yes))) \n",
        "    print(' ')\n",
        "    print('#'*60)\n",
        "    print('#'*60)\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The 10 most frequent words in cluster 0 are :\n",
            " \n",
            "[('climate_change', 688), ('global_warming', 555), ('climate', 98), ('news', 66), ('green', 52), ('science', 51), ('fight', 42), ('make', 38), ('tcot', 37), ('snow', 37)]\n",
            " \n",
            "In this cluster, the share of Yes relative to the total number of Yes is 43.64 %\n",
            " \n",
            "############################################################\n",
            "############################################################\n",
            " \n",
            "The 10 most frequent words in cluster 1 are :\n",
            " \n",
            "[('global_warming', 486), ('climate_change', 371), ('climate', 61), ('do_not', 49), ('scientist', 46), ('cause', 40), ('snow', 39), ('could', 37), ('change', 37), ('report', 35)]\n",
            " \n",
            "In this cluster, the share of Yes relative to the total number of Yes is 28.78 %\n",
            " \n",
            "############################################################\n",
            "############################################################\n",
            " \n",
            "The 10 most frequent words in cluster 2 are :\n",
            " \n",
            "[('climate_change', 548), ('global_warming', 268), ('climate', 52), ('global', 36), ('green', 28), ('energy', 28), ('study', 27), ('stop', 24), ('effect_of', 23), ('fight', 22)]\n",
            " \n",
            "In this cluster, the share of Yes relative to the total number of Yes is 27.58 %\n",
            " \n",
            "############################################################\n",
            "############################################################\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-NUIE93oV7M",
        "colab_type": "text"
      },
      "source": [
        "$\\underline{Comments}$ \n",
        "\n",
        "The distribution of the population is also equitable, with little over-representation of Cluster 0.\n",
        "\n",
        "Interpret these clusters is more difficult. More or less the same themes can be found within each cluster. They are mainly science and the effects of climate change ('science', 'scientist', 'report'); the fight against this change (particularly present in cluster 1 with the words 'fight', 'stop' and 'effect_of'); and the change 'climate_change', 'change', 'cause', 'effect-of'.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvzkTSqxoV7O",
        "colab_type": "text"
      },
      "source": [
        "## III.c LDA Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5eabFF3oV7P",
        "colab_type": "text"
      },
      "source": [
        "To use the LDA model, we chose to represent the words under a Bag-Of-Word representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT4cSAICoV7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag-Of-Word representation\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def Representation_BOW(Tweet_clean):\n",
        "    \n",
        "    bow=pd.DataFrame([Tweet_clean]).T\n",
        "    bow.columns=['headline_text']\n",
        "    bow=bow.headline_text\n",
        "    \n",
        "    dictionary = gensim.corpora.Dictionary(bow)\n",
        "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in bow]\n",
        "    \n",
        "    return(bow_corpus,dictionary)\n",
        "    \n",
        "#  BAW representations\n",
        "tweets_no=Cut_words(option='no')\n",
        "tweets_yes=Cut_words(option='yes')\n",
        "BOW_global , dictionary_global = Representation_BOW(tweets_no+tweets_yes)\n",
        "BOW_yes, dictionary_yes = Representation_BOW(tweets_yes)\n",
        "BOW_no, dictionary_no = Representation_BOW(tweets_no)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy81s2FMoV7R",
        "colab_type": "code",
        "outputId": "e879c66c-eeb7-4c13-ff23-f82881fc69cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#Bulding LDA models\n",
        "dic_models_lda={}\n",
        "\n",
        "number_topics=3\n",
        "print(' ')\n",
        "# Construction of the LDA model on the basis of Yes and No\n",
        "print('Construction of the model on the whole base..')\n",
        "t=time.time()\n",
        "lda_model_global = gensim.models.LdaMulticore(BOW_global, num_topics=number_topics, id2word=dictionary_global, passes=2, workers=2)\n",
        "print('DONE ({}s)'.format(str(round(time.time()-t,2))))\n",
        "print(' ')\n",
        "\n",
        "# Construction of the LDA model on the basis of the Yes\n",
        "print('Construction of the model on the basis of Yes..')\n",
        "t=time.time()\n",
        "lda_model_yes = gensim.models.LdaMulticore(BOW_yes, num_topics=number_topics, id2word=dictionary_yes, passes=2, workers=2)\n",
        "print('DONE ({}s)'.format(str(round(time.time()-t,2))))\n",
        "print(' ')\n",
        "\n",
        "# Construction of the LDA model on the basis of the No\n",
        "print('Construction of the model on the basis No..')\n",
        "t=time.time()\n",
        "lda_model_no = gensim.models.LdaMulticore(BOW_no, num_topics=number_topics, id2word=dictionary_no, passes=2, workers=2)\n",
        "print('DONE ({}s)'.format(str(round(time.time()-t,2))))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Construction of the model on the whole base..\n",
            "DONE (6.63s)\n",
            " \n",
            "Construction of the model on the basis of Yes..\n",
            "DONE (3.83s)\n",
            " \n",
            "Construction of the model on the basis No..\n",
            "DONE (1.77s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26THEuVzoV7T",
        "colab_type": "code",
        "outputId": "2b1b620d-9b40-415d-c931-a4aecb2ed807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "list_model={'global':lda_model_global, 'Yes':lda_model_yes, 'No':lda_model_no}\n",
        "\n",
        "# For each subject brought up by the model, \n",
        "# we look at which words appear and with what weight.\n",
        "print(' ')\n",
        "for model in list_model.keys():\n",
        "    print('Topics that appear in the database {} : '.format(model))\n",
        "    print(' ')\n",
        "    for idx, topic in list_model[model].print_topics(-1):\n",
        "        print('Topic number : {} \\nWords: {}'.format(idx, topic))\n",
        "        print(' ')\n",
        "    print('#'*30)\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Topics that appear in the database global : \n",
            " \n",
            "Topic number : 0 \n",
            "Words: 0.200*\"climate_change\" + 0.019*\"science\" + 0.018*\"scientist\" + 0.016*\"climate\" + 0.013*\"fight\" + 0.011*\"green\" + 0.011*\"world\" + 0.010*\"call\" + 0.010*\"need\" + 0.010*\"energy\"\n",
            " \n",
            "Topic number : 1 \n",
            "Words: 0.128*\"climate_change\" + 0.025*\"make\" + 0.021*\"snow\" + 0.016*\"global\" + 0.016*\"do_not\" + 0.013*\"volcano\" + 0.012*\"al_gore\" + 0.010*\"bill\" + 0.010*\"blizzard\" + 0.010*\"help\"\n",
            " \n",
            "Topic number : 2 \n",
            "Words: 0.080*\"climate_change\" + 0.046*\"climate\" + 0.024*\"tcot\" + 0.019*\"cause\" + 0.018*\"snow\" + 0.015*\"news\" + 0.015*\"study\" + 0.013*\"science\" + 0.012*\"report\" + 0.012*\"snowstorm\"\n",
            " \n",
            "##############################\n",
            " \n",
            "Topics that appear in the database Yes : \n",
            " \n",
            "Topic number : 0 \n",
            "Words: 0.225*\"global_warming\" + 0.032*\"climate\" + 0.018*\"fight\" + 0.017*\"make\" + 0.016*\"tcot\" + 0.015*\"change\" + 0.015*\"world\" + 0.013*\"snow\" + 0.013*\"global\" + 0.013*\"do_not\"\n",
            " \n",
            "Topic number : 1 \n",
            "Words: 0.093*\"global_warming\" + 0.024*\"climate\" + 0.022*\"volcano\" + 0.020*\"bill\" + 0.019*\"news\" + 0.019*\"energy\" + 0.017*\"do_not\" + 0.015*\"legislation\" + 0.014*\"global\" + 0.014*\"could\"\n",
            " \n",
            "Topic number : 2 \n",
            "Words: 0.159*\"global_warming\" + 0.029*\"green\" + 0.028*\"science\" + 0.024*\"climate\" + 0.022*\"report\" + 0.021*\"study\" + 0.018*\"cause\" + 0.016*\"scientist\" + 0.015*\"stop\" + 0.014*\"2010\"\n",
            " \n",
            "##############################\n",
            " \n",
            "Topics that appear in the database No : \n",
            " \n",
            "Topic number : 0 \n",
            "Words: 0.156*\"snow\" + 0.063*\"climate_change\" + 0.048*\"like\" + 0.044*\"climategate\" + 0.038*\"still\" + 0.038*\"weather\" + 0.035*\"a_silly\" + 0.034*\"call\" + 0.033*\"due_to\" + 0.033*\"think\"\n",
            " \n",
            "Topic number : 1 \n",
            "Words: 0.237*\"climate_change\" + 0.076*\"science\" + 0.055*\"al_gore\" + 0.048*\"climate\" + 0.046*\"tcot\" + 0.039*\"the_great\" + 0.038*\"make\" + 0.031*\"snow\" + 0.030*\"collapse\" + 0.028*\"in_dc\"\n",
            " \n",
            "Topic number : 2 \n",
            "Words: 0.109*\"climate_change\" + 0.056*\"tcot\" + 0.054*\"do_not\" + 0.046*\"cause\" + 0.045*\"cold\" + 0.037*\"winter\" + 0.035*\"time\" + 0.032*\"storm\" + 0.032*\"global\" + 0.030*\"obama\"\n",
            " \n",
            "##############################\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV5y7PvFoV7V",
        "colab_type": "text"
      },
      "source": [
        "$\\underline{Comments}$\n",
        "\n",
        "We have chosen to set the number of subjects at 3 so as not to spread ourselves too thin and capture the main elements. \n",
        "\n",
        "Concerning the topics present in the whole database, we can observe without great surprise that the terms 'climate_change', 'climate', 'green' or 'science' are the leading words of the topics. Topic number 1 is particularly interesting with the terms 'time', 'report', 'news' and 'change' which can make us think about the urgency of climate change.\n",
        "\n",
        "We were more interested in the topics in the Yes and No databases.\n",
        "\n",
        "Interestingly, the main term in the topics of the entire database is 'climate_change'. Those who believe in climate change have 'global_warming' as their main word. Thus, by using the term 'global warming', these people are using a much more accurate term. \n",
        "\n",
        "The first topic from the 'Yes' base seems to be (partly) about climate change news. Indeed, the association of words such as 'news', 'reuters' or 'carbon' implies the idea of information relayed by the media, for example.\n",
        "The second topic seems to be more about scientific and proven facts. The words 'science', 'report', 'due_to' or 'real' are used. \n",
        "The third topic can be related to the causes of global warming and in particular to inactive people. It includes the words 'do_not', 'tcot' or 'tackle'. Note that the term TCOT ('Top Conservatives On Twitter') refers to conservative people.\n",
        "\n",
        "The last topic (topic number 2) seems to be the most interesting for climate skeptics. Indeed, the word leader of the group is 'hoax'. We also find the word 'skeptic'. This topic refers very clearly to the theme of lying and prank. The term 'a_silly' only reinforces the idea of lying.\n",
        "Note also that the term 'tcot' appears in all subjects (major in subject number 0). Most of these people are therefore classified as conservative.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2CahGB_oV7W",
        "colab_type": "text"
      },
      "source": [
        "## III.d Study of the tonality of tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDoLhtyBoV7W",
        "colab_type": "text"
      },
      "source": [
        "For each tweet in the database, the SentiWordNet model is used to determine the feelings associated with it. We then average the scores associated with the words in a tweet. \n",
        "\n",
        "At the end of this process, we have a score associated with positivity, negativity and neutrality for each tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp76f9rIoV7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for scoring and averaging word scores for a tweet\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    \n",
        "    #Convert between the PennTreebank tags to simple Wordnet tags\n",
        "    \n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    return None\n",
        "\n",
        "def get_sentiment(word,tag):\n",
        "    #returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \n",
        "\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag not in (wordnet.NOUN, wordnet.ADJ, wordnet.ADV):\n",
        "        return []\n",
        "\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    synsets = wordnet.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "\n",
        "# For each tweet, computing the three scores \n",
        "def sentiments_score(Tweets,Initial_Tweet,labels):\n",
        "    \n",
        "    Tweet_sentiments=[]\n",
        "    for tweet_index in range(len(Tweets)):\n",
        "\n",
        "        list_tweet=[]\n",
        "\n",
        "        # Add the initial tweet \n",
        "        list_tweet.append(Initial_Tweet[tweet_index])\n",
        "\n",
        "        # Add the scores \n",
        "\n",
        "        ##Computing scores for each word in the tweet\n",
        "        pos_val = nltk.pos_tag(Tweets[tweet_index])\n",
        "        senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
        "\n",
        "        ## Average of these scores \n",
        "        pos_score=0\n",
        "        neg_score=0\n",
        "        neutre_score=0\n",
        "        size_senti=0\n",
        "        for score in senti_val:\n",
        "            if len(score)==3:\n",
        "                pos_score=pos_score+score[0]\n",
        "                neg_score=neg_score+score[1]\n",
        "                neutre_score=neutre_score+score[2]\n",
        "                size_senti=size_senti+1\n",
        "        if size_senti==0:\n",
        "            list_tweet.append('no words known by SWN')\n",
        "        else:\n",
        "            pos_score=pos_score/size_senti\n",
        "            neg_score=neg_score/size_senti\n",
        "            neutre_score=neutre_score/size_senti\n",
        "        \n",
        "            list_tweet.append([pos_score,neg_score,neutre_score])\n",
        "        \n",
        "        # Add the label  \n",
        "        list_tweet.append(labels[tweet_index])\n",
        "    \n",
        "        Tweet_sentiments.append(list_tweet)\n",
        "    \n",
        "    return(Tweet_sentiments)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFGthC-koV7Y",
        "colab_type": "code",
        "outputId": "1541b5d2-1c9b-4c91-f621-d530e64e3202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Computing sentiment scores \n",
        "\n",
        "Tweet_clean=import_clean_text()\n",
        "Labels=df.Existence\n",
        "\n",
        "print(' ')\n",
        "print('Computing of scores for each word and average scores for each tweet..')\n",
        "t=time.time()\n",
        "Base_SWN=sentiments_score(Tweet_clean,df.Tweet,Labels)\n",
        "print('DONE ({}s)'.format(str(round(time.time()-t,2))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Computing of scores for each word and average scores for each tweet..\n",
            "DONE (7.21s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qjO1uoBoV7c",
        "colab_type": "code",
        "outputId": "18725396-e60f-4264-858d-774ac71dbacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "# Study of the results \n",
        "Base_SWN=pd.DataFrame(Base_SWN,columns=['Tweet','Scores','Label'])\n",
        "\n",
        "#############################################\n",
        "#############################################\n",
        "\n",
        "# Scores obtained for the Yes\n",
        "Base_SWN_yes=Base_SWN[(Base_SWN.Label=='Yes')&(Base_SWN.Scores!='no words known by SWN')]\n",
        "\n",
        "print(' For tweets who believe in global warming : ')\n",
        "print(' ')\n",
        "\n",
        "# Print of the first 5 tweets of the Yes database\n",
        "for tweet_index in range(5):\n",
        "    print(list(Base_SWN_yes.Tweet)[tweet_index])\n",
        "    print('Scores (pos/neg/neutral) : '+str(list(Base_SWN_yes.Scores)[tweet_index]))\n",
        "    print(' ')\n",
        "#############################################\n",
        "#############################################\n",
        "\n",
        "\n",
        "print('#'*100)\n",
        "print('#'*100)\n",
        "print(' ')\n",
        "\n",
        "print(' For tweets who don\\'t believe in global warming : ')\n",
        "print(' ')\n",
        "# Scores obtenus pour les No\n",
        "Base_SWN_no=Base_SWN[(Base_SWN.Label=='No')&(Base_SWN.Scores!='no words known by SWN')]\n",
        "\n",
        "# Print of the first 5 tweets of the database No\n",
        "for tweet_index in range(5):\n",
        "    print(list(Base_SWN_no.Tweet)[tweet_index])\n",
        "    print('Scores (pos/neg/neutral) : '+str(list(Base_SWN_no.Scores)[tweet_index]))\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " For tweets who believe in global warming : \n",
            " \n",
            "Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. \n",
            "Scores (pos/neg/neutral) : [0.0, 0.0, 1.0]\n",
            " \n",
            "Fighting poverty and global warming in Africa \n",
            "Scores (pos/neg/neutral) : [0.0, 0.0, 1.0]\n",
            " \n",
            "RT @sejorg: RT @JaymiHeimbuch: Ocean Saltiness Shows Global Warming Is Intensifying Our Water Cycle \n",
            "Scores (pos/neg/neutral) : [0.0, 0.0, 1.0]\n",
            " \n",
            "Global warming evidence all around us|A message to global warming deniers and doubters: Just look around our .. \n",
            "Scores (pos/neg/neutral) : [0.020833333333333332, 0.0, 0.9791666666666666]\n",
            " \n",
            "Migratory Birds' New Climate Change Strategy: Stay Home \n",
            "Scores (pos/neg/neutral) : [0.125, 0.05, 0.825]\n",
            " \n",
            "####################################################################################################\n",
            "####################################################################################################\n",
            " \n",
            " For tweets who don't believe in global warming : \n",
            " \n",
            "Wait here's an idea: it's natural climate change, not human induced global warming. \n",
            "Scores (pos/neg/neutral) : [0.05357142857142857, 0.08928571428571429, 0.8571428571428571]\n",
            " \n",
            "@New_federalists  i have it on good auth tht global warming also causes toe fungus.  We R all fortunate tht thr IS no global warming! #tcot\n",
            "Scores (pos/neg/neutral) : [0.175, 0.05, 0.775]\n",
            " \n",
            "Illegal war and the myth of global warming|My main campaign platform for this election will be the illegal .. \n",
            "Scores (pos/neg/neutral) : [0.046875, 0.0, 0.953125]\n",
            " \n",
            "the scientific community was scamed by global green  gov warming scam.\n",
            "Scores (pos/neg/neutral) : [0.0, 0.125, 0.875]\n",
            " \n",
            "40 degrees in NYC. please urinate on next liberal global warming /climate change scum you see.\n",
            "Scores (pos/neg/neutral) : [0.175, 0.05, 0.775]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSBTlWnqoV7e",
        "colab_type": "text"
      },
      "source": [
        "From these representations, we can study how the positive scors are distributed in the Yes and No bases as well as in the whole base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eE-2GPjoV7e",
        "colab_type": "code",
        "outputId": "1d12dd5f-461a-47af-fdf1-cfad520f5512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def stati_scores(base):\n",
        "    base=pd.DataFrame(list(base.Scores),columns=['score_pos','score_neg','score_neutre'])\n",
        "    display(base.describe())\n",
        "    \n",
        "print(' ')\n",
        "print('Statistics of scores obtained on tweets from the entire database :')\n",
        "stati_scores(Base_SWN[Base_SWN.Scores!='no words known by SWN'])\n",
        "print(' ')\n",
        "print('#'*60)\n",
        "print(' ')\n",
        "print('Statistics of scores obtained on Yes\\' tweets :')\n",
        "stati_scores(Base_SWN_yes)\n",
        "print(' ')\n",
        "print('#'*60)\n",
        "print(' ')\n",
        "print('Statistics of scores obtained on No\\' tweets :')\n",
        "stati_scores(Base_SWN_no)\n",
        "print(' ')\n",
        "print('#'*60)\n",
        "print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Statistics of scores obtained on tweets from the entire database :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score_pos</th>\n",
              "      <th>score_neg</th>\n",
              "      <th>score_neutre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5435.000000</td>\n",
              "      <td>5435.000000</td>\n",
              "      <td>5435.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.038779</td>\n",
              "      <td>0.048963</td>\n",
              "      <td>0.912258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.061682</td>\n",
              "      <td>0.082293</td>\n",
              "      <td>0.106441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.944444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         score_pos    score_neg  score_neutre\n",
              "count  5435.000000  5435.000000   5435.000000\n",
              "mean      0.038779     0.048963      0.912258\n",
              "std       0.061682     0.082293      0.106441\n",
              "min       0.000000     0.000000      0.250000\n",
              "25%       0.000000     0.000000      0.862500\n",
              "50%       0.000000     0.000000      0.944444\n",
              "75%       0.062500     0.071429      1.000000\n",
              "max       0.625000     0.750000      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "############################################################\n",
            " \n",
            "Statistics of scores obtained on Yes' tweets :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score_pos</th>\n",
              "      <th>score_neg</th>\n",
              "      <th>score_neutre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2779.000000</td>\n",
              "      <td>2779.000000</td>\n",
              "      <td>2779.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.038137</td>\n",
              "      <td>0.049642</td>\n",
              "      <td>0.912221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.061173</td>\n",
              "      <td>0.086745</td>\n",
              "      <td>0.109528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.946429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         score_pos    score_neg  score_neutre\n",
              "count  2779.000000  2779.000000   2779.000000\n",
              "mean      0.038137     0.049642      0.912221\n",
              "std       0.061173     0.086745      0.109528\n",
              "min       0.000000     0.000000      0.250000\n",
              "25%       0.000000     0.000000      0.875000\n",
              "50%       0.000000     0.000000      0.946429\n",
              "75%       0.062500     0.071429      1.000000\n",
              "max       0.625000     0.750000      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "############################################################\n",
            " \n",
            "Statistics of scores obtained on No' tweets :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score_pos</th>\n",
              "      <th>score_neg</th>\n",
              "      <th>score_neutre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1014.000000</td>\n",
              "      <td>1014.000000</td>\n",
              "      <td>1014.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.047923</td>\n",
              "      <td>0.063173</td>\n",
              "      <td>0.888904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.067062</td>\n",
              "      <td>0.085786</td>\n",
              "      <td>0.111493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.906250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.986111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         score_pos    score_neg  score_neutre\n",
              "count  1014.000000  1014.000000   1014.000000\n",
              "mean      0.047923     0.063173      0.888904\n",
              "std       0.067062     0.085786      0.111493\n",
              "min       0.000000     0.000000      0.250000\n",
              "25%       0.000000     0.000000      0.833333\n",
              "50%       0.020833     0.031250      0.906250\n",
              "75%       0.075000     0.100000      0.986111\n",
              "max       0.625000     0.750000      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "############################################################\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxc6XUMgoV7j",
        "colab_type": "code",
        "outputId": "c2ad7b47-243f-47c9-b723-23be0ea3d8e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "# Boxplot des distributions des scores \n",
        "\n",
        "# Positive scores  No and Yes base scores  \n",
        "lis_pos_no=pd.DataFrame(list(Base_SWN_no.Scores),columns=['score_pos','score_neg','score_neutre'])['score_pos']\n",
        "lis_pos_yes=pd.DataFrame(list(Base_SWN_yes.Scores),columns=['score_pos','score_neg','score_neutre'])['score_pos']\n",
        "\n",
        "# Negative scores  No and Yes base scores   \n",
        "lis_neg_no=pd.DataFrame(list(Base_SWN_no.Scores),columns=['score_pos','score_neg','score_neutre'])['score_neg']\n",
        "lis_neg_yes=pd.DataFrame(list(Base_SWN_yes.Scores),columns=['score_pos','score_neg','score_neutre'])['score_neg']\n",
        "\n",
        "\n",
        "Base_pos=pd.DataFrame([[x,y] for (x,y) in zip(lis_pos_yes,lis_pos_no)],columns=['positive scores in the Yes database','positive scores in the No database'])\n",
        "Base_neg=pd.DataFrame([[x,y] for (x,y) in zip(lis_neg_yes,lis_neg_no)],columns=['negative scores in the Yes database','negative scores in the No database']) \n",
        "\n",
        "x=Base_pos.plot.box(grid='True',title='Distribution of positif scores')\n",
        "x=Base_neg.plot.box(grid='True',title='Distribution of negative scores')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLuAoA79oV7l",
        "colab_type": "text"
      },
      "source": [
        "$\\underline{Comments}$\n",
        "\n",
        "Concerning the positive scores, the distribution of these scores is approximately the same for the Yes and No databases.\n",
        "\n",
        "For negative scores, the distribution is different. It can be observed that tweets in the No database are more negatively scorned. For example, the 3rd quartile of negative scores is 0.10 for the No database; 0.075 for the Yes database. By observing the boxplots of these distributions, we can see that the difference is quite significant. \n",
        "\n",
        "The same remark can be made for positive scores.\n",
        "\n",
        "In addition to the different arguments between the No and Yes tweets (see part II.a, II.b), the tonality informs us of the pessimistic/optimistic character of the tweets. Skeptics seem to have a more incisive tone. This is interesting in the sense that the supposed 'pessimism' of the Yes tweets (about global warming, rising temperatures...) seems to be overtaken by the 'negativity' of the No tweets (about fraud, scandals, lies). This can be explained by the fact that No tweets are often accompanied by stronger words and, at times, insults. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEB8z5dLoV7m",
        "colab_type": "text"
      },
      "source": [
        "To summarize the three scores associated with a tweet, we can determine whether the positive score is higher than the negative score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1E5sw7yoV7n",
        "colab_type": "code",
        "outputId": "8c7a727b-a5c4-4014-8cc4-32367b8eea82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#A Tweet can be scored as positive if its negative score is greater than its positive score;\n",
        "# and vice versa. If the two scores are equal it is classified as neutral.\n",
        "\n",
        "def stati_scores_major(base):\n",
        "    base=pd.DataFrame(list(base.Scores),columns=['score_pos','score_neg','score_neutre'])\n",
        "    base['pos']=(base['score_pos']>base['score_neg'])*1\n",
        "    base['neg']=(base['score_neg']>base['score_pos'])*1\n",
        "    return([sum(base['pos']),sum(base['neg']),len(base)-(sum(base['neg'])+sum(base['pos']))])\n",
        "\n",
        "# On the entire database\n",
        "data=Base_SWN[Base_SWN.Scores!='no words known by SWN']\n",
        "results=stati_scores_major(data)\n",
        "print(' ')\n",
        "print('On the entire database ({} tweets) : '.format(len(data)))\n",
        "print(' ')\n",
        "print('The number of tweets judged positive is {} --> {}%'.format(str(results[0]),str(round(100*results[0]/len(data),2))))\n",
        "print('The number of tweets judged negative is {} --> {}%'.format(str(results[1]),str(round(100*results[1]/len(data),2))))\n",
        "print('The number of tweets judged neutral is{} --> {}%'.format(str(results[2]),str(round(100*results[2]/len(data),2))))\n",
        "print(' ')\n",
        "print('#'*30)\n",
        "print('#'*30)\n",
        "print(' ')\n",
        "\n",
        "# On the basis of the Yes  \n",
        "data=Base_SWN_yes\n",
        "results=stati_scores_major(data)\n",
        "print(' ')\n",
        "print('On a compound basis of the Yes ({} tweets) : '.format(len(data)))\n",
        "print(' ')\n",
        "print('The number of tweets judged positive is {} --> {}%'.format(str(results[0]),str(round(100*results[0]/len(data),2))))\n",
        "print('The number of tweets judged negative is {} --> {}%'.format(str(results[1]),str(round(100*results[1]/len(data),2))))\n",
        "print('The number of tweets judged neutral is {} --> {}%'.format(str(results[2]),str(round(100*results[2]/len(data),2))))\n",
        "print(' ')\n",
        "print('#'*30)\n",
        "print('#'*30)\n",
        "print(' ')\n",
        "\n",
        "# On the basis of the No \n",
        "data=Base_SWN_no\n",
        "results=stati_scores_major(data)\n",
        "print(' ')\n",
        "print('On a compound basis of the No ({} tweets) : '.format(len(data)))\n",
        "print(' ')\n",
        "print('The number of tweets judged positive is {} --> {}%'.format(str(results[0]),str(round(100*results[0]/len(data),2))))\n",
        "print('The number of tweets judged negtives is {} --> {}%'.format(str(results[1]),str(round(100*results[1]/len(data),2))))\n",
        "print('The number of tweets judged neutral is {} --> {}%'.format(str(results[2]),str(round(100*results[2]/len(data),2))))\n",
        "print(' ')\n",
        "print('#'*30)\n",
        "print('#'*30)\n",
        "print(' ')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "On the entire database (5435 tweets) : \n",
            " \n",
            "The number of tweets judged positive is 1452 --> 26.72%\n",
            "The number of tweets judged negative is 1837 --> 33.8%\n",
            "The number of tweets judged neutral is2146 --> 39.48%\n",
            " \n",
            "##############################\n",
            "##############################\n",
            " \n",
            " \n",
            "On a compound basis of the Yes (2779 tweets) : \n",
            " \n",
            "The number of tweets judged positive is 740 --> 26.63%\n",
            "The number of tweets judged negative is 939 --> 33.79%\n",
            "The number of tweets judged neutral is 1100 --> 39.58%\n",
            " \n",
            "##############################\n",
            "##############################\n",
            " \n",
            " \n",
            "On a compound basis of the No (1014 tweets) : \n",
            " \n",
            "The number of tweets judged positive is 297 --> 29.29%\n",
            "The number of tweets judged negtives is 406 --> 40.04%\n",
            "The number of tweets judged neutral is 311 --> 30.67%\n",
            " \n",
            "##############################\n",
            "##############################\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrNcUSgloV7p",
        "colab_type": "text"
      },
      "source": [
        "$\\underline{Comments}$\n",
        "\n",
        "We observe that the majority of tweets are negative in tone, whether they are Yes or No. Nevertheless, pessimism or optimism is more marked in the tweets of skeptics. For example, tweets with negative tones represent $40.04$% of the No base compared to $33.79$% for Yes tweets. The demonstrative character of the climate-skeptics can be found in the tweets of climate-skeptics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4myPe7hoV7q",
        "colab_type": "text"
      },
      "source": [
        "As an example, here are some tweets judged very 'negatively' in the No' database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KgHY0cGoV7q",
        "colab_type": "code",
        "outputId": "41be5ba6-51d8-454d-d917-6041b7468b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "def stati_scores_tweets(base):\n",
        "    base_1=base.copy()\n",
        "    base=pd.DataFrame(list(base.Scores),columns=['score_pos','score_neg','score_neutre'])\n",
        "    base['Tweet']=base_1.Tweet\n",
        "    return(base)\n",
        "\n",
        "# Print of tweets\n",
        "data=Base_SWN_no.reset_index()\n",
        "data=stati_scores_tweets(data)\n",
        "data=data.sort_values(by=['score_neg'],ascending=False).reset_index()\n",
        "print(' ')\n",
        "print(data.Tweet[4])\n",
        "print('score négatif : {}'.format(str(data.score_neg[4])))\n",
        "print(' ')\n",
        "print(data.Tweet[7])\n",
        "print('score négatif : {}'.format(str(data.score_neg[7])))\n",
        "print(' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "If Ensler & Behar represent liberal women, that's such a sad pathetic stmt. Global warming fanatics who yack about their vaginas. Pathetic!\n",
            "score négatif : 0.42857142857142855\n",
            " \n",
            "RT @gopevangelist: Global warming hysteria presumes 3 things: 1 it exists, 2 it is man-made, 3 it is bad. All 3 must be true. This winter may prove all 3 false\n",
            "score négatif : 0.3392857142857143\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}